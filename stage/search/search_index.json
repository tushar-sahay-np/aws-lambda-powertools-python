{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A suite of utilities for AWS Lambda functions to ease adopting best practices such as tracing, structured logging, custom metrics, and more. Tip: Looking for a quick read through how the core features are used? Check out this detailed blog post with a practical example. Tenets \u00b6 Core utilities such as Tracing, Logging, Metrics, and Event Handler will be available across all Lambda Powertools runtimes. Additional utilities are subjective to each language ecosystem and customer demand. AWS Lambda only . We optimise for AWS Lambda function environments and supported runtimes only. Utilities might work with web frameworks and non-Lambda environments, though they are not officially supported. Eases the adoption of best practices . The main priority of the utilities is to facilitate best practices adoption, as defined in the AWS Well-Architected Serverless Lens; all other functionality is optional. Keep it lean . Additional dependencies are carefully considered for security and ease of maintenance, and prevent negatively impacting startup time. We strive for backwards compatibility . New features and changes should keep backwards compatibility. If a breaking change cannot be avoided, the deprecation and migration process should be clearly defined. We work backwards from the community . We aim to strike a balance of what would work best for 80% of customers. Emerging practices are considered and discussed via Requests for Comment (RFCs) Progressive . Utilities are designed to be incrementally adoptable for customers at any stage of their Serverless journey. They follow language idioms and their community\u2019s common practices. Install \u00b6 Powertools is available in the following formats: Lambda Layer : arn:aws:lambda:{region}:017000801446:layer:AWSLambdaPowertoolsPython:7 PyPi : pip install aws-lambda-powertools Lambda Layer \u00b6 Lambda Layer is a .zip file archive that can contain additional code, pre-packaged dependencies, data, or configuration files. Layers promote code sharing and separation of responsibilities so that you can iterate faster on writing business logic. You can include Lambda Powertools Lambda Layer using AWS Lambda Console , or your preferred deployment framework. Note: Expand to copy any regional Lambda Layer ARN Region Layer ARN us-east-1 arn:aws:lambda:us-east-1:017000801446:layer:AWSLambdaPowertoolsPython:7 us-east-2 arn:aws:lambda:us-east-2:017000801446:layer:AWSLambdaPowertoolsPython:7 us-west-1 arn:aws:lambda:us-west-1:017000801446:layer:AWSLambdaPowertoolsPython:7 us-west-2 arn:aws:lambda:us-west-2:017000801446:layer:AWSLambdaPowertoolsPython:7 ap-south-1 arn:aws:lambda:ap-south-1:017000801446:layer:AWSLambdaPowertoolsPython:7 ap-northeast-1 arn:aws:lambda:ap-northeast-1:017000801446:layer:AWSLambdaPowertoolsPython:7 ap-northeast-2 arn:aws:lambda:ap-northeast-2:017000801446:layer:AWSLambdaPowertoolsPython:7 ap-northeast-3 arn:aws:lambda:ap-northeast-3:017000801446:layer:AWSLambdaPowertoolsPython:7 ap-southeast-1 arn:aws:lambda:ap-southeast-1:017000801446:layer:AWSLambdaPowertoolsPython:7 ap-southeast-2 arn:aws:lambda:ap-southeast-2:017000801446:layer:AWSLambdaPowertoolsPython:7 eu-central-1 arn:aws:lambda:eu-central-1:017000801446:layer:AWSLambdaPowertoolsPython:7 eu-west-1 arn:aws:lambda:eu-west-1:017000801446:layer:AWSLambdaPowertoolsPython:7 eu-west-2 arn:aws:lambda:eu-west-2:017000801446:layer:AWSLambdaPowertoolsPython:7 eu-west-3 arn:aws:lambda:eu-west-3:017000801446:layer:AWSLambdaPowertoolsPython:7 eu-north-1 arn:aws:lambda:eu-north-1:017000801446:layer:AWSLambdaPowertoolsPython:7 ca-central-1 arn:aws:lambda:ca-central-1:017000801446:layer:AWSLambdaPowertoolsPython:7 sa-east-1 arn:aws:lambda:sa-east-1:017000801446:layer:AWSLambdaPowertoolsPython:7 SAM 1 2 3 4 5 MyLambdaFunction : Type : AWS::Serverless::Function Properties : Layers : - !Sub arn:aws:lambda:${AWS::Region}:017000801446:layer:AWSLambdaPowertoolsPython:7 Serverless framework 1 2 3 4 5 functions : hello : handler : lambda_function.lambda_handler layers : - arn:aws:lambda:${aws:region}:017000801446:layer:AWSLambdaPowertoolsPython:7 CDK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_cdk import core , aws_lambda class SampleApp ( core . Construct ): def __init__ ( self , scope : core . Construct , id_ : str , env : core . Environment ) -> None : super () . __init__ ( scope , id_ ) powertools_layer = aws_lambda . LayerVersion . from_layer_version_arn ( self , id = \"lambda-powertools\" , layer_version_arn = f \"arn:aws:lambda: { env . region } :017000801446:layer:AWSLambdaPowertoolsPython:7\" ) aws_lambda . Function ( self , 'sample-app-lambda' , runtime = aws_lambda . Runtime . PYTHON_3_9 , layers = [ powertools_layer ] # other props... ) Terraform 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 terraform { required_version = \"~> 1.0.5\" required_providers { aws = \"~> 3.50.0\" } } provider \"aws\" { region = \"{region}\" } resource \"aws_iam_role\" \"iam_for_lambda\" { name = \"iam_for_lambda\" assume_role_policy = << EOF { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRole\" , \"Principal\" : { \"Service\" : \"lambda.amazonaws.com\" }, \"Effect\" : \"Allow\" } ] } EOF } resource \"aws_lambda_function\" \"test_lambda\" { filename = \"lambda_function_payload.zip\" function_name = \"lambda_function_name\" role = aws_iam_role.iam_for_lambda.arn handler = \"index.test\" runtime = \"python3.9\" layers = [ \"arn:aws:lambda:{region}:017000801446:layer:AWSLambdaPowertoolsPython:7\" ] source_code_hash = filebase64sha256 ( \"lambda_function_payload.zip\" ) } Amplify 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Create a new one with the layer \u276f amplify add function ? Select which capability you want to add: Lambda function ( serverless function ) ? Provide an AWS Lambda function name: <NAME-OF-FUNCTION> ? Choose the runtime that you want to use: Python ? Do you want to configure advanced settings? Yes ... ? Do you want to enable Lambda layers for this function ? Yes ? Enter up to 5 existing Lambda layer ARNs ( comma-separated ) : arn:aws:lambda:eu-central-1:017000801446:layer:AWSLambdaPowertoolsPython:7 \u276f amplify push -y # Updating an existing function and add the layer \u276f amplify update function ? Select the Lambda function you want to update test2 General information - Name: <NAME-OF-FUNCTION> ? Which setting do you want to update? Lambda layers configuration ? Do you want to enable Lambda layers for this function ? Yes ? Enter up to 5 existing Lambda layer ARNs ( comma-separated ) : arn:aws:lambda:eu-central-1:017000801446:layer:AWSLambdaPowertoolsPython:7 ? Do you want to edit the local lambda function now? No Get the Layer .zip contents Change {region} to your AWS region, e.g. eu-west-1 AWS CLI 1 aws lambda get-layer-version-by-arn --arn arn:aws:lambda: { region } :017000801446:layer:AWSLambdaPowertoolsPython:7 --region { region } The pre-signed URL to download this Lambda Layer will be within Location key. Warning: Limitations Container Image deployment (OCI) or inline Lambda functions do not support Lambda Layers. Lambda Powertools Lambda Layer do not include pydantic library - required dependency for the parser utility. See SAR option instead. SAR \u00b6 Serverless Application Repository (SAR) App deploys a CloudFormation stack with a copy of our Lambda Layer in your AWS account and region. Despite having more steps compared to the public Layer ARN option, the benefit is that you can specify a semantic version you want to use. App ARN Description aws-lambda-powertools-python-layer arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer Core dependencies only; sufficient for nearly all utilities. aws-lambda-powertools-python-layer-extras arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer-extras Core plus extra dependencies such as pydantic that is required by parser utility. Warning Layer-extras does not support Python 3.6 runtime. This layer also includes all extra dependencies: 22.4MB zipped , ~155MB unzipped . Tip You can create a shared Lambda Layers stack and make this along with other account level layers stack. If using SAM, you can include this SAR App as part of your shared Layers stack, and lock to a specific semantic version. Once deployed, it'll be available across the account this is deployed to. SAM 1 2 3 4 5 6 7 8 9 10 11 12 13 AwsLambdaPowertoolsPythonLayer : Type : AWS::Serverless::Application Properties : Location : ApplicationId : arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer SemanticVersion : 1.24.0 # change to latest semantic version available in SAR MyLambdaFunction : Type : AWS::Serverless::Function Properties : Layers : # fetch Layer ARN from SAR App stack output - !GetAtt AwsLambdaPowertoolsPythonLayer.Outputs.LayerVersionArn Serverless framework 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 functions : main : handler : lambda_function.lambda_handler layers : - !GetAtt AwsLambdaPowertoolsPythonLayer.Outputs.LayerVersionArn resources : Transform : AWS::Serverless-2016-10-31 Resources:**** AwsLambdaPowertoolsPythonLayer : Type : AWS::Serverless::Application Properties : Location : ApplicationId : arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer # Find latest from github.com/awslabs/aws-lambda-powertools-python/releases SemanticVersion : 1.24.0 CDK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from aws_cdk import core , aws_sam as sam , aws_lambda POWERTOOLS_BASE_NAME = 'AWSLambdaPowertools' # Find latest from github.com/awslabs/aws-lambda-powertools-python/releases POWERTOOLS_VER = '1.23.0' POWERTOOLS_ARN = 'arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer' class SampleApp ( core . Construct ): def __init__ ( self , scope : core . Construct , id_ : str ) -> None : super () . __init__ ( scope , id_ ) # Launches SAR App as CloudFormation nested stack and return Lambda Layer powertools_app = sam . CfnApplication ( self , f ' { POWERTOOLS_BASE_NAME } Application' , location = { 'applicationId' : POWERTOOLS_ARN , 'semanticVersion' : POWERTOOLS_VER }, ) powertools_layer_arn = powertools_app . get_att ( \"Outputs.LayerVersionArn\" ) . to_string () powertools_layer_version = aws_lambda . LayerVersion . from_layer_version_arn ( self , f ' { POWERTOOLS_BASE_NAME } ' , powertools_layer_arn ) aws_lambda . Function ( self , 'sample-app-lambda' , runtime = aws_lambda . Runtime . PYTHON_3_8 , function_name = 'sample-lambda' , code = aws_lambda . Code . asset ( './src' ), handler = 'app.handler' , layers : [ powertools_layer_version ] ) Terraform Credits to Dani Comnea for providing the Terraform equivalent. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 terraform { required_version = \"~> 0.13\" required_providers { aws = \"~> 3.50.0\" } } provider \"aws\" { region = \"us-east-1\" } resource \"aws_serverlessapplicationrepository_cloudformation_stack\" \"deploy_sar_stack\" { name = \"aws-lambda-powertools-python-layer\" application_id = data.aws_serverlessapplicationrepository_application.sar_app.application_id semantic_version = data.aws_serverlessapplicationrepository_application.sar_app.semantic_version capabilities = [ \"CAPABILITY_IAM\" , \"CAPABILITY_NAMED_IAM\" ] } data \"aws_serverlessapplicationrepository_application\" \"sar_app\" { application_id = \"arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer\" semantic_version = var.aws_powertools_version } variable \"aws_powertools_version\" { type = string default = \"1.20.2\" description = \"The AWS Powertools release version\" } output \"deployed_powertools_sar_version\" { value = data.aws_serverlessapplicationrepository_application.sar_app.semantic_version } # Fetch Lambda Powertools Layer ARN from deployed SAR App output \"aws_lambda_powertools_layer_arn\" { value = aws_serverlessapplicationrepository_cloudformation_stack.deploy_sar_stack.outputs.LayerVersionArn } Example: Least-privileged IAM permissions to deploy Layer Credits to mwarkentin for providing the scoped down IAM permissions. The region and the account id for CloudFormationTransform and GetCfnTemplate are fixed. template.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 AWSTemplateFormatVersion : \"2010-09-09\" Resources : PowertoolsLayerIamRole : Type : \"AWS::IAM::Role\" Properties : AssumeRolePolicyDocument : Version : \"2012-10-17\" Statement : - Effect : \"Allow\" Principal : Service : - \"cloudformation.amazonaws.com\" Action : - \"sts:AssumeRole\" Path : \"/\" PowertoolsLayerIamPolicy : Type : \"AWS::IAM::Policy\" Properties : PolicyName : PowertoolsLambdaLayerPolicy PolicyDocument : Version : \"2012-10-17\" Statement : - Sid : CloudFormationTransform Effect : Allow Action : cloudformation:CreateChangeSet Resource : - arn:aws:cloudformation:us-east-1:aws:transform/Serverless-2016-10-31 - Sid : GetCfnTemplate Effect : Allow Action : - serverlessrepo:CreateCloudFormationTemplate - serverlessrepo:GetCloudFormationTemplate Resource : # this is arn of the powertools SAR app - arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer - Sid : S3AccessLayer Effect : Allow Action : - s3:GetObject Resource : # AWS publishes to an external S3 bucket locked down to your account ID # The below example is us publishing lambda powertools # Bucket: awsserverlessrepo-changesets-plntc6bfnfj # Key: *****/arn:aws:serverlessrepo:eu-west-1:057560766410:applications-aws-lambda-powertools-python-layer-versions-1.10.2/aeeccf50-****-****-****-********* - arn:aws:s3:::awsserverlessrepo-changesets-*/* - Sid : GetLayerVersion Effect : Allow Action : - lambda:PublishLayerVersion - lambda:GetLayerVersion Resource : - !Sub arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:layer:aws-lambda-powertools-python-layer* Roles : - Ref : \"PowertoolsLayerIamRole\" You can fetch available versions via SAR ListApplicationVersions API: AWS CLI example 1 2 aws serverlessrepo list-application-versions \\ --application-id arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer Quick getting started \u00b6 Hello world example using SAM CLI 1 sam init --location https://github.com/aws-samples/cookiecutter-aws-sam-python Features \u00b6 Utility Description Tracing Decorators and utilities to trace Lambda function handlers, and both synchronous and asynchronous functions Logger Structured logging made easier, and decorator to enrich structured logging with key Lambda context details Metrics Custom Metrics created asynchronously via CloudWatch Embedded Metric Format (EMF) Event handler: AppSync AppSync event handler for Lambda Direct Resolver and Amplify GraphQL Transformer function Event handler: API Gateway and ALB Amazon API Gateway REST/HTTP API and ALB event handler for Lambda functions invoked using Proxy integration Middleware factory Decorator factory to create your own middleware to run logic before, and after each Lambda invocation Parameters Retrieve parameter values from AWS Systems Manager Parameter Store, AWS Secrets Manager, or Amazon DynamoDB, and cache them for a specific amount of time Batch processing Handle partial failures for AWS SQS batch processing Typing Static typing classes to speedup development in your IDE Validation JSON Schema validator for inbound events and responses Event source data classes Data classes describing the schema of common Lambda event triggers Parser Data parsing and deep validation using Pydantic Idempotency Idempotent Lambda handler Feature Flags A simple rule engine to evaluate when one or multiple features should be enabled depending on the input Environment variables \u00b6 Info Explicit parameters take precedence over environment variables Environment variable Description Utility Default POWERTOOLS_SERVICE_NAME Sets service name used for tracing namespace, metrics dimension and structured logging All \"service_undefined\" POWERTOOLS_METRICS_NAMESPACE Sets namespace used for metrics Metrics None POWERTOOLS_TRACE_DISABLED Explicitly disables tracing Tracing false POWERTOOLS_TRACER_CAPTURE_RESPONSE Captures Lambda or method return as metadata. Tracing true POWERTOOLS_TRACER_CAPTURE_ERROR Captures Lambda or method exception as metadata. Tracing true POWERTOOLS_TRACE_MIDDLEWARES Creates sub-segment for each custom middleware Middleware factory false POWERTOOLS_LOGGER_LOG_EVENT Logs incoming event Logging false POWERTOOLS_LOGGER_SAMPLE_RATE Debug log sampling Logging 0 POWERTOOLS_LOG_DEDUPLICATION_DISABLED Disables log deduplication filter protection to use Pytest Live Log feature Logging false POWERTOOLS_EVENT_HANDLER_DEBUG Enables debugging mode for event handler Event Handler false LOG_LEVEL Sets logging level Logging INFO Debug mode \u00b6 As a best practice, AWS Lambda Powertools module logging statements are suppressed. If necessary, you can enable debugging using set_package_logger for additional information on every internal operation: Powertools debug mode example 1 2 3 from aws_lambda_powertools.logging.logger import set_package_logger set_package_logger ()","title":"Homepage"},{"location":"#tenets","text":"Core utilities such as Tracing, Logging, Metrics, and Event Handler will be available across all Lambda Powertools runtimes. Additional utilities are subjective to each language ecosystem and customer demand. AWS Lambda only . We optimise for AWS Lambda function environments and supported runtimes only. Utilities might work with web frameworks and non-Lambda environments, though they are not officially supported. Eases the adoption of best practices . The main priority of the utilities is to facilitate best practices adoption, as defined in the AWS Well-Architected Serverless Lens; all other functionality is optional. Keep it lean . Additional dependencies are carefully considered for security and ease of maintenance, and prevent negatively impacting startup time. We strive for backwards compatibility . New features and changes should keep backwards compatibility. If a breaking change cannot be avoided, the deprecation and migration process should be clearly defined. We work backwards from the community . We aim to strike a balance of what would work best for 80% of customers. Emerging practices are considered and discussed via Requests for Comment (RFCs) Progressive . Utilities are designed to be incrementally adoptable for customers at any stage of their Serverless journey. They follow language idioms and their community\u2019s common practices.","title":"Tenets"},{"location":"#install","text":"Powertools is available in the following formats: Lambda Layer : arn:aws:lambda:{region}:017000801446:layer:AWSLambdaPowertoolsPython:7 PyPi : pip install aws-lambda-powertools","title":"Install"},{"location":"#lambda-layer","text":"Lambda Layer is a .zip file archive that can contain additional code, pre-packaged dependencies, data, or configuration files. Layers promote code sharing and separation of responsibilities so that you can iterate faster on writing business logic. You can include Lambda Powertools Lambda Layer using AWS Lambda Console , or your preferred deployment framework. Note: Expand to copy any regional Lambda Layer ARN Region Layer ARN us-east-1 arn:aws:lambda:us-east-1:017000801446:layer:AWSLambdaPowertoolsPython:7 us-east-2 arn:aws:lambda:us-east-2:017000801446:layer:AWSLambdaPowertoolsPython:7 us-west-1 arn:aws:lambda:us-west-1:017000801446:layer:AWSLambdaPowertoolsPython:7 us-west-2 arn:aws:lambda:us-west-2:017000801446:layer:AWSLambdaPowertoolsPython:7 ap-south-1 arn:aws:lambda:ap-south-1:017000801446:layer:AWSLambdaPowertoolsPython:7 ap-northeast-1 arn:aws:lambda:ap-northeast-1:017000801446:layer:AWSLambdaPowertoolsPython:7 ap-northeast-2 arn:aws:lambda:ap-northeast-2:017000801446:layer:AWSLambdaPowertoolsPython:7 ap-northeast-3 arn:aws:lambda:ap-northeast-3:017000801446:layer:AWSLambdaPowertoolsPython:7 ap-southeast-1 arn:aws:lambda:ap-southeast-1:017000801446:layer:AWSLambdaPowertoolsPython:7 ap-southeast-2 arn:aws:lambda:ap-southeast-2:017000801446:layer:AWSLambdaPowertoolsPython:7 eu-central-1 arn:aws:lambda:eu-central-1:017000801446:layer:AWSLambdaPowertoolsPython:7 eu-west-1 arn:aws:lambda:eu-west-1:017000801446:layer:AWSLambdaPowertoolsPython:7 eu-west-2 arn:aws:lambda:eu-west-2:017000801446:layer:AWSLambdaPowertoolsPython:7 eu-west-3 arn:aws:lambda:eu-west-3:017000801446:layer:AWSLambdaPowertoolsPython:7 eu-north-1 arn:aws:lambda:eu-north-1:017000801446:layer:AWSLambdaPowertoolsPython:7 ca-central-1 arn:aws:lambda:ca-central-1:017000801446:layer:AWSLambdaPowertoolsPython:7 sa-east-1 arn:aws:lambda:sa-east-1:017000801446:layer:AWSLambdaPowertoolsPython:7 SAM 1 2 3 4 5 MyLambdaFunction : Type : AWS::Serverless::Function Properties : Layers : - !Sub arn:aws:lambda:${AWS::Region}:017000801446:layer:AWSLambdaPowertoolsPython:7 Serverless framework 1 2 3 4 5 functions : hello : handler : lambda_function.lambda_handler layers : - arn:aws:lambda:${aws:region}:017000801446:layer:AWSLambdaPowertoolsPython:7 CDK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_cdk import core , aws_lambda class SampleApp ( core . Construct ): def __init__ ( self , scope : core . Construct , id_ : str , env : core . Environment ) -> None : super () . __init__ ( scope , id_ ) powertools_layer = aws_lambda . LayerVersion . from_layer_version_arn ( self , id = \"lambda-powertools\" , layer_version_arn = f \"arn:aws:lambda: { env . region } :017000801446:layer:AWSLambdaPowertoolsPython:7\" ) aws_lambda . Function ( self , 'sample-app-lambda' , runtime = aws_lambda . Runtime . PYTHON_3_9 , layers = [ powertools_layer ] # other props... ) Terraform 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 terraform { required_version = \"~> 1.0.5\" required_providers { aws = \"~> 3.50.0\" } } provider \"aws\" { region = \"{region}\" } resource \"aws_iam_role\" \"iam_for_lambda\" { name = \"iam_for_lambda\" assume_role_policy = << EOF { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRole\" , \"Principal\" : { \"Service\" : \"lambda.amazonaws.com\" }, \"Effect\" : \"Allow\" } ] } EOF } resource \"aws_lambda_function\" \"test_lambda\" { filename = \"lambda_function_payload.zip\" function_name = \"lambda_function_name\" role = aws_iam_role.iam_for_lambda.arn handler = \"index.test\" runtime = \"python3.9\" layers = [ \"arn:aws:lambda:{region}:017000801446:layer:AWSLambdaPowertoolsPython:7\" ] source_code_hash = filebase64sha256 ( \"lambda_function_payload.zip\" ) } Amplify 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Create a new one with the layer \u276f amplify add function ? Select which capability you want to add: Lambda function ( serverless function ) ? Provide an AWS Lambda function name: <NAME-OF-FUNCTION> ? Choose the runtime that you want to use: Python ? Do you want to configure advanced settings? Yes ... ? Do you want to enable Lambda layers for this function ? Yes ? Enter up to 5 existing Lambda layer ARNs ( comma-separated ) : arn:aws:lambda:eu-central-1:017000801446:layer:AWSLambdaPowertoolsPython:7 \u276f amplify push -y # Updating an existing function and add the layer \u276f amplify update function ? Select the Lambda function you want to update test2 General information - Name: <NAME-OF-FUNCTION> ? Which setting do you want to update? Lambda layers configuration ? Do you want to enable Lambda layers for this function ? Yes ? Enter up to 5 existing Lambda layer ARNs ( comma-separated ) : arn:aws:lambda:eu-central-1:017000801446:layer:AWSLambdaPowertoolsPython:7 ? Do you want to edit the local lambda function now? No Get the Layer .zip contents Change {region} to your AWS region, e.g. eu-west-1 AWS CLI 1 aws lambda get-layer-version-by-arn --arn arn:aws:lambda: { region } :017000801446:layer:AWSLambdaPowertoolsPython:7 --region { region } The pre-signed URL to download this Lambda Layer will be within Location key. Warning: Limitations Container Image deployment (OCI) or inline Lambda functions do not support Lambda Layers. Lambda Powertools Lambda Layer do not include pydantic library - required dependency for the parser utility. See SAR option instead.","title":"Lambda Layer"},{"location":"#sar","text":"Serverless Application Repository (SAR) App deploys a CloudFormation stack with a copy of our Lambda Layer in your AWS account and region. Despite having more steps compared to the public Layer ARN option, the benefit is that you can specify a semantic version you want to use. App ARN Description aws-lambda-powertools-python-layer arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer Core dependencies only; sufficient for nearly all utilities. aws-lambda-powertools-python-layer-extras arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer-extras Core plus extra dependencies such as pydantic that is required by parser utility. Warning Layer-extras does not support Python 3.6 runtime. This layer also includes all extra dependencies: 22.4MB zipped , ~155MB unzipped . Tip You can create a shared Lambda Layers stack and make this along with other account level layers stack. If using SAM, you can include this SAR App as part of your shared Layers stack, and lock to a specific semantic version. Once deployed, it'll be available across the account this is deployed to. SAM 1 2 3 4 5 6 7 8 9 10 11 12 13 AwsLambdaPowertoolsPythonLayer : Type : AWS::Serverless::Application Properties : Location : ApplicationId : arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer SemanticVersion : 1.24.0 # change to latest semantic version available in SAR MyLambdaFunction : Type : AWS::Serverless::Function Properties : Layers : # fetch Layer ARN from SAR App stack output - !GetAtt AwsLambdaPowertoolsPythonLayer.Outputs.LayerVersionArn Serverless framework 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 functions : main : handler : lambda_function.lambda_handler layers : - !GetAtt AwsLambdaPowertoolsPythonLayer.Outputs.LayerVersionArn resources : Transform : AWS::Serverless-2016-10-31 Resources:**** AwsLambdaPowertoolsPythonLayer : Type : AWS::Serverless::Application Properties : Location : ApplicationId : arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer # Find latest from github.com/awslabs/aws-lambda-powertools-python/releases SemanticVersion : 1.24.0 CDK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from aws_cdk import core , aws_sam as sam , aws_lambda POWERTOOLS_BASE_NAME = 'AWSLambdaPowertools' # Find latest from github.com/awslabs/aws-lambda-powertools-python/releases POWERTOOLS_VER = '1.23.0' POWERTOOLS_ARN = 'arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer' class SampleApp ( core . Construct ): def __init__ ( self , scope : core . Construct , id_ : str ) -> None : super () . __init__ ( scope , id_ ) # Launches SAR App as CloudFormation nested stack and return Lambda Layer powertools_app = sam . CfnApplication ( self , f ' { POWERTOOLS_BASE_NAME } Application' , location = { 'applicationId' : POWERTOOLS_ARN , 'semanticVersion' : POWERTOOLS_VER }, ) powertools_layer_arn = powertools_app . get_att ( \"Outputs.LayerVersionArn\" ) . to_string () powertools_layer_version = aws_lambda . LayerVersion . from_layer_version_arn ( self , f ' { POWERTOOLS_BASE_NAME } ' , powertools_layer_arn ) aws_lambda . Function ( self , 'sample-app-lambda' , runtime = aws_lambda . Runtime . PYTHON_3_8 , function_name = 'sample-lambda' , code = aws_lambda . Code . asset ( './src' ), handler = 'app.handler' , layers : [ powertools_layer_version ] ) Terraform Credits to Dani Comnea for providing the Terraform equivalent. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 terraform { required_version = \"~> 0.13\" required_providers { aws = \"~> 3.50.0\" } } provider \"aws\" { region = \"us-east-1\" } resource \"aws_serverlessapplicationrepository_cloudformation_stack\" \"deploy_sar_stack\" { name = \"aws-lambda-powertools-python-layer\" application_id = data.aws_serverlessapplicationrepository_application.sar_app.application_id semantic_version = data.aws_serverlessapplicationrepository_application.sar_app.semantic_version capabilities = [ \"CAPABILITY_IAM\" , \"CAPABILITY_NAMED_IAM\" ] } data \"aws_serverlessapplicationrepository_application\" \"sar_app\" { application_id = \"arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer\" semantic_version = var.aws_powertools_version } variable \"aws_powertools_version\" { type = string default = \"1.20.2\" description = \"The AWS Powertools release version\" } output \"deployed_powertools_sar_version\" { value = data.aws_serverlessapplicationrepository_application.sar_app.semantic_version } # Fetch Lambda Powertools Layer ARN from deployed SAR App output \"aws_lambda_powertools_layer_arn\" { value = aws_serverlessapplicationrepository_cloudformation_stack.deploy_sar_stack.outputs.LayerVersionArn } Example: Least-privileged IAM permissions to deploy Layer Credits to mwarkentin for providing the scoped down IAM permissions. The region and the account id for CloudFormationTransform and GetCfnTemplate are fixed. template.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 AWSTemplateFormatVersion : \"2010-09-09\" Resources : PowertoolsLayerIamRole : Type : \"AWS::IAM::Role\" Properties : AssumeRolePolicyDocument : Version : \"2012-10-17\" Statement : - Effect : \"Allow\" Principal : Service : - \"cloudformation.amazonaws.com\" Action : - \"sts:AssumeRole\" Path : \"/\" PowertoolsLayerIamPolicy : Type : \"AWS::IAM::Policy\" Properties : PolicyName : PowertoolsLambdaLayerPolicy PolicyDocument : Version : \"2012-10-17\" Statement : - Sid : CloudFormationTransform Effect : Allow Action : cloudformation:CreateChangeSet Resource : - arn:aws:cloudformation:us-east-1:aws:transform/Serverless-2016-10-31 - Sid : GetCfnTemplate Effect : Allow Action : - serverlessrepo:CreateCloudFormationTemplate - serverlessrepo:GetCloudFormationTemplate Resource : # this is arn of the powertools SAR app - arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer - Sid : S3AccessLayer Effect : Allow Action : - s3:GetObject Resource : # AWS publishes to an external S3 bucket locked down to your account ID # The below example is us publishing lambda powertools # Bucket: awsserverlessrepo-changesets-plntc6bfnfj # Key: *****/arn:aws:serverlessrepo:eu-west-1:057560766410:applications-aws-lambda-powertools-python-layer-versions-1.10.2/aeeccf50-****-****-****-********* - arn:aws:s3:::awsserverlessrepo-changesets-*/* - Sid : GetLayerVersion Effect : Allow Action : - lambda:PublishLayerVersion - lambda:GetLayerVersion Resource : - !Sub arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:layer:aws-lambda-powertools-python-layer* Roles : - Ref : \"PowertoolsLayerIamRole\" You can fetch available versions via SAR ListApplicationVersions API: AWS CLI example 1 2 aws serverlessrepo list-application-versions \\ --application-id arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer","title":"SAR"},{"location":"#quick-getting-started","text":"Hello world example using SAM CLI 1 sam init --location https://github.com/aws-samples/cookiecutter-aws-sam-python","title":"Quick getting started"},{"location":"#features","text":"Utility Description Tracing Decorators and utilities to trace Lambda function handlers, and both synchronous and asynchronous functions Logger Structured logging made easier, and decorator to enrich structured logging with key Lambda context details Metrics Custom Metrics created asynchronously via CloudWatch Embedded Metric Format (EMF) Event handler: AppSync AppSync event handler for Lambda Direct Resolver and Amplify GraphQL Transformer function Event handler: API Gateway and ALB Amazon API Gateway REST/HTTP API and ALB event handler for Lambda functions invoked using Proxy integration Middleware factory Decorator factory to create your own middleware to run logic before, and after each Lambda invocation Parameters Retrieve parameter values from AWS Systems Manager Parameter Store, AWS Secrets Manager, or Amazon DynamoDB, and cache them for a specific amount of time Batch processing Handle partial failures for AWS SQS batch processing Typing Static typing classes to speedup development in your IDE Validation JSON Schema validator for inbound events and responses Event source data classes Data classes describing the schema of common Lambda event triggers Parser Data parsing and deep validation using Pydantic Idempotency Idempotent Lambda handler Feature Flags A simple rule engine to evaluate when one or multiple features should be enabled depending on the input","title":"Features"},{"location":"#environment-variables","text":"Info Explicit parameters take precedence over environment variables Environment variable Description Utility Default POWERTOOLS_SERVICE_NAME Sets service name used for tracing namespace, metrics dimension and structured logging All \"service_undefined\" POWERTOOLS_METRICS_NAMESPACE Sets namespace used for metrics Metrics None POWERTOOLS_TRACE_DISABLED Explicitly disables tracing Tracing false POWERTOOLS_TRACER_CAPTURE_RESPONSE Captures Lambda or method return as metadata. Tracing true POWERTOOLS_TRACER_CAPTURE_ERROR Captures Lambda or method exception as metadata. Tracing true POWERTOOLS_TRACE_MIDDLEWARES Creates sub-segment for each custom middleware Middleware factory false POWERTOOLS_LOGGER_LOG_EVENT Logs incoming event Logging false POWERTOOLS_LOGGER_SAMPLE_RATE Debug log sampling Logging 0 POWERTOOLS_LOG_DEDUPLICATION_DISABLED Disables log deduplication filter protection to use Pytest Live Log feature Logging false POWERTOOLS_EVENT_HANDLER_DEBUG Enables debugging mode for event handler Event Handler false LOG_LEVEL Sets logging level Logging INFO","title":"Environment variables"},{"location":"#debug-mode","text":"As a best practice, AWS Lambda Powertools module logging statements are suppressed. If necessary, you can enable debugging using set_package_logger for additional information on every internal operation: Powertools debug mode example 1 2 3 from aws_lambda_powertools.logging.logger import set_package_logger set_package_logger ()","title":"Debug mode"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. This project follows Keep a Changelog format for changes and adheres to Semantic Versioning . 1.24.0 - 2021-12-31 \u00b6 Bug Fixes \u00b6 apigateway: support @app .not_found() syntax & housekeeping ( #926 ) event-sources: handle dynamodb null type as none, not bool ( #929 ) warning: future distutils deprecation in 3.12 ( #921 ) Documentation \u00b6 general : consistency round admonitions and snippets ( #919 ) homepage : Added GraphQL Sample API to Examples section of README.md ( #930 ) batch: remove leftover from legacy layer: bump Lambda Layer to version 6 tracer: new ignore_endpoint feature ( #931 ) Features \u00b6 event-sources: cache parsed json in data class ( #909 ) feature_flags: support beyond boolean values (JSON values) ( #804 ) idempotency: support dataclasses & pydantic models payloads ( #908 ) logger: support use_datetime_directive for timestamps ( #920 ) tracer: ignore tracing for certain hostname(s) or url(s) ( #910 ) Maintenance \u00b6 deps-dev: bump mypy from 0.920 to 0.930 ( #925 ) 1.23.0 - 2021-12-20 \u00b6 Bug Fixes \u00b6 apigateway: allow list of HTTP methods in route method ( #838 ) event-sources: pass authorizer data to APIGatewayEventAuthorizer ( #897 ) event-sources: handle claimsOverrideDetails set to null ( #878 ) idempotency: include decorated fn name in hash ( #869 ) metrics: explicit type to single_metric ctx manager ( #865 ) parameters: mypy appconfig transform and return types ( #877 ) parser: mypy overload parse when using envelope ( #885 ) parser: kinesis sequence number is str, not int ( #907 ) parser: mypy support for payload type override as models ( #883 ) tracer: add warm start annotation (ColdStart=False) ( #851 ) Documentation \u00b6 nav : reference cloudformation custom resource helper (CRD) ( #914 ) add new public Slack invite disable search blur in non-prod env update Lambda Layers version apigateway: add new not_found feature ( #915 ) apigateway: fix sample layout provided ( #864 ) appsync: fix users.py typo to locations #830 lambda_layer: fix CDK layer syntax Features \u00b6 apigateway: add exception_handler support ( #898 ) apigateway: access parent api resolver from router ( #842 ) batch: new BatchProcessor for SQS, DynamoDB, Kinesis ( #886 ) logger: allow handler with custom kwargs signature ( #913 ) tracer: add service annotation when service is set ( #861 ) Maintenance \u00b6 minor housekeeping before release ( #912 ) correct pr label order ci: split latest docs workflow deps: bump fastjsonschema from 2.15.1 to 2.15.2 ( #891 ) deps: bump actions/setup-python from 2.2.2 to 2.3.0 ( #831 ) deps: support arm64 when developing locally ( #862 ) deps: bump actions/setup-python from 2.3.0 to 2.3.1 ( #852 ) deps: bump aws-xray-sdk from 2.8.0 to 2.9.0 ( #876 ) deps-dev: bump mypy from 0.910 to 0.920 ( #903 ) deps-dev: bump flake8 from 3.9.2 to 4.0.1 ( #789 ) deps-dev: bump black from 21.10b0 to 21.11b1 ( #839 ) deps-dev: bump black from 21.11b1 to 21.12b0 ( #872 ) 1.22.0 - 2021-11-17 \u00b6 Tenet update! We've updated Idiomatic tenet to Progressive to reflect the new Router feature in Event Handler, and more importantly the new wave of customers coming from SRE, Data Analysis, and Data Science background. BEFORE: Idiomatic . Utilities follow programming language idioms and language-specific best practices. AFTER: Progressive . Utilities are designed to be incrementally adoptable for customers at any stage of their Serverless journey. They follow language idioms and their community\u2019s common practices. Bug Fixes \u00b6 ci: change supported python version from 3.6.1 to 3.6.2, bump black ( #807 ) ci: skip sync master on docs hotfix parser: body and query strings can be null or omitted in ApiGatewayProxyEventModel and ApiGatewayProxyEventV2Model ( #820 ) Code Refactoring \u00b6 apigateway: Add BaseRouter and duplicate route check ( #757 ) Documentation \u00b6 docs: updated Lambda Layers definition & limitations. ( #775 ) docs: Idiomatic tenet updated to Progressive docs: use higher contrast font to improve accessibility ( #822 ) docs: fix indentation of SAM snippets in install section ( #778 ) docs: improve public lambda layer wording, add clipboard buttons to improve UX ( #762 ) docs: add amplify-cli instructions for public layer ( #754 ) api-gateway: add new router feature to allow route splitting in API Gateway and ALB ( #767 ) apigateway: re-add sample layout, add considerations ( #826 ) appsync: add new router feature to allow GraphQL Resolver composition ( #821 ) idempotency: add support for DynamoDB composite keys ( #808 ) tenets: update Idiomatic tenet to Progressive ( #823 ) docs: remove Lambda Layer version tag Features \u00b6 apigateway: add Router to allow large routing composition ( #645 ) appsync: add Router to allow large resolver composition ( #776 ) data-classes: ActiveMQ and RabbitMQ support ( #770 ) logger: add ALB correlation ID support ( #816 ) Maintenance \u00b6 deps: bump boto3 from 1.19.6 to 1.20.3 ( #809 ) deps: bump boto3 from 1.18.58 to 1.18.59 ( #760 ) deps: bump urllib3 from 1.26.4 to 1.26.5 ( #787 ) deps: bump boto3 from 1.18.61 to 1.19.6 ( #783 ) deps: bump boto3 from 1.18.56 to 1.18.58 ( #755 ) deps: bump boto3 from 1.18.59 to 1.18.61 ( #766 ) deps: bump boto3 from 1.20.3 to 1.20.5 ( #817 ) deps-dev: bump coverage from 6.0.1 to 6.0.2 ( #764 ) deps-dev: bump pytest-asyncio from 0.15.1 to 0.16.0 ( #782 ) deps-dev: bump flake8-eradicate from 1.1.0 to 1.2.0 ( #784 ) deps-dev: bump flake8-comprehensions from 3.6.1 to 3.7.0 ( #759 ) deps-dev: bump flake8-isort from 4.0.0 to 4.1.1 ( #785 ) deps-dev: bump coverage from 6.0 to 6.0.1 ( #751 ) deps-dev: bump mkdocs-material from 7.3.3 to 7.3.5 ( #781 ) deps-dev: bump mkdocs-material from 7.3.5 to 7.3.6 ( #791 ) deps-dev: bump mkdocs-material from 7.3.2 to 7.3.3 ( #758 ) 1.21.1 - 2021-10-07 \u00b6 Regression \u00b6 metrics: typing regression on log_metrics callable ( #744 ) Documentation \u00b6 add new public layer ARNs ( #746 ) Maintenance \u00b6 ignore constants in test cov ( #745 ) github-actions: add support for publishing fallback deps: bump boto3 from 1.18.54 to 1.18.56 ( #742 ) deps-dev: bump mkdocs-material from 7.3.1 to 7.3.2 ( #741 ) 1.21.0 - 2021-10-05 \u00b6 Bug Fixes \u00b6 data-classes: use correct asdict funciton ( #666 ) feature-flags: rules should evaluate with an AND op ( #724 ) idempotency: sorting keys before hashing ( #722 ) idempotency: sorting keys before hashing logger: push extra keys to the end ( #722 ) mypy: a few return types, type signatures, and untyped areas ( #718 ) Code Refactoring \u00b6 data-classes: clean up internal logic for APIGatewayAuthorizerResponse ( #643 ) Documentation \u00b6 Terraform reference for SAR Lambda Layer ( #716 ) event-handler: document catch-all routes ( #705 ) idempotency: fix misleading idempotent examples ( #661 ) jmespath: clarify envelope terminology parser: fix incorrect import in root_validator example ( #735 ) Features \u00b6 expose jmespath powertools functions ( #736 ) boto3 sessions in batch, parameters & idempotency ( #717 ) feature-flags : add get_raw_configuration property in store; expose store ( #720 ) feature-flags: Bring your own logger for debug ( #709 ) feature-flags: improve \"IN/NOT_IN\"; new rule actions ( #710 ) feature-flags: get_raw_configuration property in Store ( #720 ) feature_flags: Added inequality conditions ( #721 ) idempotency: makes customers unit testing easier ( #719 ) validator: include missing data elements from a validation error ( #686 ) Maintenance \u00b6 add python 3.9 support deps: bump boto3 from 1.18.51 to 1.18.54 ( #733 ) deps: bump boto3 from 1.18.32 to 1.18.38 ( #671 ) deps: bump boto3 from 1.18.38 to 1.18.41 ( #677 ) deps: bump boto3 from 1.18.49 to 1.18.51 ( #713 ) deps: bump boto3 from 1.18.41 to 1.18.49 ( #703 ) deps: bump codecov/codecov-action from 2.0.2 to 2.1.0 ( #675 ) deps-dev: bump coverage from 5.5 to 6.0 ( #732 ) deps-dev: bump mkdocs-material from 7.2.8 to 7.3.0 ( #695 ) deps-dev: bump mkdocs-material from 7.2.6 to 7.2.8 ( #682 ) deps-dev: bump flake8-bugbear from 21.4.3 to 21.9.1 ( #676 ) deps-dev: bump flake8-bugbear from 21.9.1 to 21.9.2 ( #712 ) deps-dev: bump radon from 4.5.2 to 5.1.0 ( #673 ) deps-dev: bump mkdocs-material from 7.3.0 to 7.3.1 ( #731 ) deps-dev: bump xenon from 0.7.3 to 0.8.0 ( #669 ) Bug Fixes \u00b6 event-handler: fix issue with strip_prefixes and root level resolvers ( #646 ) Maintenance \u00b6 deps: bump boto3 from 1.18.26 to 1.18.32 ( #663 ) deps-dev: bump mkdocs-material from 7.2.4 to 7.2.6 ( #665 ) deps-dev: bump pytest from 6.2.4 to 6.2.5 ( #662 ) deps-dev: bump mike from 0.6.0 to 1.0.1 ( #453 ) license: add third party license to pyproject.toml ( #641 ) 1.20.2 - 2021-09-02 \u00b6 Bug Fixes \u00b6 event-handler: fix issue with strip_prefixes and root level resolvers ( #646 ) Maintenance \u00b6 deps: bump boto3 from 1.18.26 to 1.18.32 ( #663 ) deps-dev: bump mkdocs-material from 7.2.4 to 7.2.6 ( #665 ) deps-dev: bump pytest from 6.2.4 to 6.2.5 ( #662 ) deps-dev: bump mike from 0.6.0 to 1.0.1 ( #453 ) license: add third party license to pyproject.toml ( #641 ) 1.20.1 - 2021-08-22 \u00b6 Bug Fixes \u00b6 idempotency: sorting keys before hashing ( #639 ) Maintenance \u00b6 markdown linter fixes ( #636 ) setup codespaces ( #637 ) license: add third party license ( #635 ) 1.20.0 - 2021-08-21 \u00b6 Bug Fixes \u00b6 api-gateway: HTTP API strip stage name from request path ( #622 ) Code Refactoring \u00b6 event-handler: match to match_results; 3.10 new keyword ( #616 ) Documentation \u00b6 data-classes: make authorizer concise; use enum ( #630 ) feature-flags: correct link and json examples ( #605 ) data-class: fix invalid syntax in new AppSync Authorizer api-gateway: add new API mapping support Features \u00b6 data-classes: authorizer for API Gateway HTTP and REST API ( #620 ) data-classes: new data_as_bytes property in KinesisStreamRecordPayload ( #628 ) data-classes: AppSync Lambda authorizer event ( #610 ) event-handler: prefixes to strip for custom domain mapping paths ( #579 ) general: support for Python 3.9 ( #626 ) idempotency: support for any synchronous function ( #625 ) Maintenance \u00b6 actions: include new labels api-docs: enable allow_reuse to fix the docs ( #612 ) deps: bump boto3 from 1.18.25 to 1.18.26 ( #627 ) deps: bump boto3 from 1.18.24 to 1.18.25 ( #623 ) deps: bump boto3 from 1.18.22 to 1.18.24 ( #619 ) deps: bump boto3 from 1.18.17 to 1.18.21 ( #608 ) deps: bump boto3 from 1.18.21 to 1.18.22 ( #614 ) deps-dev: bump flake8-comprehensions from 3.5.0 to 3.6.0 ( #609 ) deps-dev: bump flake8-comprehensions from 3.6.0 to 3.6.1 ( #615 ) deps-dev: bump mkdocs-material from 7.2.3 to 7.2.4 ( #607 ) docs: correct markdown based on markdown lint ( #603 ) shared: fix cyclic import & refactor data extraction fn ( #613 ) 1.19.0 - 2021-08-11 \u00b6 Bug Fixes \u00b6 deps: bump poetry to latest ( #592 ) feature-flags: bug handling multiple conditions ( #599 ) parser: API Gateway WebSocket validation under check_message_id; plus some housekeeping ( #553 ) feature-toggles: correct cdk example ( #601 ) Code Refactoring \u00b6 feature-flags: add debug statements for all feature evaluations ( #590 ) feature-flags: optimize UX and maintenance ( #563 ) Documentation \u00b6 event-handler: new custom serializer option feature-flags: create concrete documentation ( #594 ) feature-flags: correct docs and typing ( #588 ) parameters: auto-transforming values based on suffix ( #573 ) readme: add code coverage badge ( #577 ) tracer: update wording that it auto-disables on non-Lambda env feature-flags: fix SAM infra, convert CDK to Python feature-flags: fix sample feature name in evaluate method feature-flags: add guidance when to use vs env vars vs parameters Features \u00b6 api-gateway: add support for custom serializer ( #568 ) data-classes: decode json_body if based64 encoded ( #560 ) feature-flags: Add not_in action and rename contains to in ( #589 ) params: expose params max_age , raise_on_transform_error to high level functions ( #567 ) tracer: auto-disable tracer for non-Lambda environments to ease testing ( #598 ) Maintenance \u00b6 deps: bump boto3 from 1.18.1 to 1.18.15 ( #591 ) deps: bump codecov/codecov-action from 2.0.1 to 2.0.2 ( #558 ) deps: bump boto3 from 1.18.15 to 1.18.17 ( #597 ) deps-dev: bump mkdocs-material from 7.2.2 to 7.2.3 ( #596 ) deps-dev: bump mkdocs-material from 7.2.1 to 7.2.2 ( #582 ) deps-dev: bump pdoc3 from 0.9.2 to 0.10.0 ( #584 ) deps-dev: bump isort from 5.9.2 to 5.9.3 ( #574 ) deps-dev: bump mkdocs-material from 7.2.0 to 7.2.1 ( #566 ) deps-dev: bump mkdocs-material from 7.1.11 to 7.2.0 ( #551 ) deps-dev: bump flake8-black from 0.2.1 to 0.2.3 ( #541 ) 1.18.1 - 2021-07-23 \u00b6 Bug Fixes \u00b6 api-gateway: route regression for non-word and unsafe URI chars ( #556 ) 1.18.0 - 2021-07-20 \u00b6 Bug Fixes \u00b6 api-gateway: non-greedy route pattern regex which incorrectly mapped certain route params to function params ( #533 ) api-gateway: incorrect plain text mimetype constant #506 data-classes: include milliseconds in scalar types to correctly align with AppSync scalars ( #504 ) mypy: addresses lack of optional types ( #521 ) parser: make ApiGateway version, authorizer fields optional ( #532 ) tracer: mypy generic to preserve decorated method signature ( #529 ) Code Refactoring \u00b6 feature-toggles: code coverage and housekeeping ( #530 ) Documentation \u00b6 api-gateway: new HTTP service error exceptions ( #546 ) logger: new get_correlation_id method ( #545 ) Features \u00b6 api-gateway: add debug mode ( #507 ) api-gateway: add common HTTP service errors ( #506 ) event-handler: Support AppSyncResolverEvent subclassing ( #526 ) feat-toggle: New simple feature toggles rule engine (WIP) ( #494 ) logger: add get_correlation_id method ( #516 ) Maintenance \u00b6 mypy: add mypy support to makefile ( #508 ) deps: bump codecov/codecov-action from 1 to 2.0.1 ( #539 ) deps: bump boto3 from 1.18.0 to 1.18.1 ( #528 ) deps: bump boto3 from 1.17.110 to 1.18.0 ( #527 ) deps: bump boto3 from 1.17.102 to 1.17.110 ( #523 ) deps-dev: bump mkdocs-material from 7.1.10 to 7.1.11 ( #542 ) deps-dev: bump mkdocs-material from 7.1.9 to 7.1.10 ( #522 ) deps-dev: bump isort from 5.9.1 to 5.9.2 ( #514 ) event-handler: adjusts API Gateway/ALB service errors exception docstrings to not confuse AppSync customers 1.17.1 - 2021-07-02 \u00b6 Bug Fixes \u00b6 Validator: Handle built-in custom formats like date-time when type is string ( #498 ) Documentation \u00b6 Layers: Add Layers example for Serverless framework & CDK ( #500 ) Misc.: Enable dark mode switch ( #471 ) Tracer: Additional scenario when to disable auto-capture for responses larger than 64K ( #499 ) Maintenance \u00b6 deps: bump boto3 from 1.17.101 to 1.17.102 ( #493 ) deps: bump boto3 from 1.17.91 to 1.17.101 ( #490 ) deps: bump email-validator from 1.1.2 to 1.1.3 ( #478 ) deps: bump boto3 from 1.17.89 to 1.17.91 ( #473 ) deps-dev: bump flake8-eradicate from 1.0.0 to 1.1.0 ( #492 ) deps-dev: bump isort from 5.8.0 to 5.9.1 ( #487 ) deps-dev: bump mkdocs-material from 7.1.7 to 7.1.9 ( #491 ) 1.17.0 - 2021-06-08 \u00b6 Added \u00b6 Documentation : Include new public roadmap ( #452 ) Documentation : Remove old todo in idempotency docs Data classes: New AttributeValueType to get type and value from data in DynamoDBStreamEvent ( #462 ) Data classes: New decorator event_source to instantiate data_classes ( #442 ) Logger: New clear_state parameter to clear previously added custom keys upon invocation ( #467 ) Parser: Support for API Gateway HTTP API #434 ( #441 ) Maintenance \u00b6 deps : bump xenon from 0.7.1 to 0.7.3 ( #446 ) assited changelog pre-generation, auto-label PR ( #443 ) enable dependabot for dep upgrades ( #444 ) enable mergify ( #450 ) deps : bump mkdocs-material from 7.1.5 to 7.1.6 ( #451 ) deps : bump boto3 from 1.17.78 to 1.17.84 ( #449 ) update mergify to require approval on dependabot ( #456 ) deps : bump actions/setup-python from 1 to 2.2.2 ( #445 ) deps: bump boto3 from 1.17.87 to 1.17.88 ( #463 ) deps: bump boto3 from 1.17.88 to 1.17.89 ( #466 ) deps: bump boto3 from 1.17.84 to 1.17.85 ( #455 ) deps: bump boto3 from 1.17.85 to 1.17.86 ( #458 ) deps: bump boto3 from 1.17.86 to 1.17.87 ( #459 ) deps-dev: bump mkdocs-material from 7.1.6 to 7.1.7 ( #464 ) deps-dev: bump pytest-cov from 2.12.0 to 2.12.1 ( #454 ) mergify: disable check for matrix jobs mergify: use job name to match GH Actions 1.16.1 - 2021-05-23 \u00b6 Fixed \u00b6 Parser : Upgrade Pydantic to 1.8.2 due to CVE-2021-29510 1.16.0 - 2021-05-17 \u00b6 Features \u00b6 data-classes(API Gateway, ALB): New method to decode base64 encoded body ( #425 ) data-classes(CodePipeline): Support for CodePipeline job event and methods to handle artifacts more easily ( #416 ) 1.15.1 - 2021-05-13 \u00b6 Fixed \u00b6 Logger : Fix a regression with the %s operator 1.15.0 - 2021-05-06 \u00b6 Added \u00b6 Event handlers : New API Gateway and ALB utility to reduce routing boilerplate and more Documentation : Logger enhancements such as bring your own formatter, handler, UTC support, and testing for Python 3.6 Parser : Support for API Gateway REST Proxy event and envelope Logger : Support for bringing custom formatter, custom handler, custom JSON serializer and deserializer, UTC support, expose LambdaPowertoolsFormatter Metrics : Support for persisting default dimensions that should always be added Fixed \u00b6 Documentation : Fix highlights, Parser types Validator : Fix event type annotations for validate standalone function Parser : Improve and fix types Internal : Remove X-Ray SDK version pinning as serialization regression has been fixed in 2.8.0 Internal : Latest documentation correctly includes a copy of API docs reference 1.14.0 - 2021-04-09 \u00b6 Added \u00b6 Event handlers : New core utility to easily handle incoming requests tightly integrated with Data Classes; AppSync being the first as we gauge from the community what additional ones would be helpful Documentation : Enabled versioning to access docs on a per release basis or staging docs ( develop branch) Documentation : Links now open in a new tab and improved snippet line highlights Documentation(validation) : JSON Schema snippets and more complete examples Documentation(idempotency) : Table with expected configuration values for hash key and TTL attribute name when using the default behaviour Documentation(logger) : New example on how to set logging record timestamps in UTC Parser(S3) : Support for the new S3 Object Lambda Event model ( S3ObjectLambdaEvent ) Parameters : Support for DynamoDB Local via endpoint_url parameter, including docs Internal : Include make pr in pre-commit hooks when contributing to shorten feedback loop on pre-commit specific linting Fixed \u00b6 Parser : S3Model now supports keys with 0 length Tracer : Lock X-Ray SDK to 2.6.0 as there's been a regression upstream in 2.7.0 on serializing & capturing exceptions Data Classes(API Gateway) : Add missing property operationName within request context Misc. : Numerous typing fixes to better to support MyPy across all utilities Internal : Downgraded poetry to 1.1.4 as there's been a regression with importlib-metadata in 1.1.5 not yet fixed 1.13.0 - 2021-03-23 \u00b6 Added \u00b6 Data Classes : New S3 Object Lambda event Fixed \u00b6 Docs : Lambda Layer SAM template reference example 1.12.0 - 2021-03-17 \u00b6 Added \u00b6 Parameters : New force_fetch param to always fetch the latest and bypass cache, if available Data Classes : New AppSync Lambda Resolver event covering both Direct Lambda Resolver and Amplify GraphQL Transformer Resolver @function Data Classes : New AppSync scalar utilities to easily compose Lambda Resolvers with date utils, uuid, etc. Logger : Support for Correlation ID both in inject_lambda_context decorator and set_correlation_id method Logger : Include new exception_name key to help customers easily enumerate exceptions across all functions Fixed \u00b6 Tracer : Type hint on return instance that made PyCharm no longer recognize autocompletion Idempotency : Error handling for missing idempotency key and save_in_progress errors 1.11.0 - 2021-03-05 \u00b6 Fixed \u00b6 Tracer : Lazy loads X-Ray SDK to increase perf by 75% for those not instantiating Tracer Metrics : Optimize validation and serialization to increase perf by nearly 50% for large operations (<1ms) Added \u00b6 Dataclass : Add new Amazon Connect contact flow event Idempotency : New Idempotency utility Docs : Add example on how to integrate Batch utility with Sentry.io Internal : Added performance SLA tests for high level imports and Metrics validation/serialization 1.10.5 - 2021-02-17 \u00b6 No changes. Bumped version to trigger new pipeline build for layer publishing. 1.10.4 - 2021-02-17 \u00b6 Fixed \u00b6 Docs : Fix anchor tags to be lower case Docs : Correct the docs location for the labeller 1.10.3 - 2021-02-04 \u00b6 Added \u00b6 Docs : Migrated from Gatsby to MKdocs documentation system Docs : Included Getting started and Advanced sections in Core utilities, including additional examples Fixed \u00b6 Tracer : Disabled batching segments as X-Ray SDK does not flush traces upon reaching limits Parser : Model type is now compliant with mypy 1.10.2 - 2021-02-04 \u00b6 Fixed \u00b6 Utilities : Correctly handle and list multiple exceptions in SQS batch processing utility. * Docs :: Fix typos on AppConfig docstring import, and SnsModel typo in parser. Utilities : typing_extensions package is now only installed in Python < 3.8 1.10.1 - 2021-01-19 \u00b6 Fixed \u00b6 Utilities : Added SnsSqsEnvelope in parser to dynamically adjust model mismatch when customers use SNS + SQS instead of SNS + Lambda, since we've discovered three payload keys are slightly different. 1.10.0 - 2021-01-18 \u00b6 Added \u00b6 Utilities : Added support for AppConfig in Parameters utility Logger : Added support for extra parameter to add additional root fields when logging messages Logger : Added support to Pytest Live Log feat. via feature toggle POWERTOOLS_LOG_DEDUPLICATION_DISABLED Tracer : Added support to disable auto-capturing response and exception as metadata Utilities : Added support to handle custom string/integer formats in JSON Schema in Validator utility Install : Added new Lambda Layer with all extra dependencies installed, available in Serverless Application Repository (SAR) Fixed \u00b6 Docs : Added missing SNS parser model Docs : Added new environment variables for toggling features in Logger and Tracer: POWERTOOLS_LOG_DEDUPLICATION_DISABLED , POWERTOOLS_TRACER_CAPTURE_RESPONSE , POWERTOOLS_TRACER_CAPTURE_ERROR Docs : Fixed incorrect import for Cognito data classes in Event Sources utility 1.9.1 - 2020-12-21 \u00b6 Fixed \u00b6 Logger : Bugfix to prevent parent loggers with the same name being configured more than once Added \u00b6 Docs : Add clarification to Tracer docs for how capture_method decorator can cause function responses to be read and serialized. Utilities : Added equality to ease testing Event source data classes Package : Added py.typed for initial work needed for PEP 561 compliance 1.9.0 - 2020-12-04 \u00b6 Added \u00b6 Utilities : Added Kinesis, S3, CloudWatch Logs, Application Load Balancer, and SES support in Parser Docs : Sidebar menu are now always expanded Fixed \u00b6 Docs : Broken link to GitHub to homepage 1.8.0 - 2020-11-20 \u00b6 Added \u00b6 Utilities : Added support for new EventBridge Replay field in Parser and Event source data classes Utilities : Added SNS support in Parser Utilities : Added API Gateway HTTP API data class support for new IAM and Lambda authorizer in Event source data classes Docs : Add new FAQ section for Logger on how to enable debug logging for boto3 Docs : Add explicit minimal set of permissions required to use Layers provided by Serverless Application Repository (SAR) Fixed \u00b6 Docs : Fix typo in Dataclasses example for SES when fetching common email headers 1.7.0 - 2020-10-26 \u00b6 Added \u00b6 Utilities : Add new Parser utility to provide parsing and deep data validation using Pydantic Models Utilities : Add case insensitive header lookup, and Cognito custom auth triggers to Event source data classes Fixed \u00b6 Logger : keeps Lambda root logger handler, and add log filter instead to prevent child log records duplication Docs : Improve wording on adding log keys conditionally 1.6.1 - 2020-09-23 \u00b6 Fixed \u00b6 Utilities : Fix issue with boolean values in DynamoDB stream event data class. 1.6.0 - 2020-09-22 \u00b6 Added \u00b6 Metrics : Support adding multiple metric values to a single metric name Utilities : Add new Validator utility to validate inbound events and responses using JSON Schema Utilities : Add new Event source data classes utility to easily describe event schema of popular event sources Docs : Add new Testing your code section to both Logger and Metrics page, and content width is now wider Tracer : Support for automatically disable Tracer when running a Chalice app Fixed \u00b6 Docs : Improve wording on log sampling feature in Logger, and removed duplicate content on main page Utilities : Remove DeleteMessageBatch API call when there are no messages to delete 1.5.0 - 2020-09-04 \u00b6 Added \u00b6 Logger : Add xray_trace_id to log output to improve integration with CloudWatch Service Lens Logger : Allow reordering of logged output Utilities : Add new SQS batch processing utility to handle partial failures in processing message batches Utilities : Add typing utility providing static type for lambda context object Utilities : Add transform=auto in parameters utility to deserialize parameter values based on the key name Fixed \u00b6 Logger : The value of json_default formatter is no longer written to logs 1.4.0 - 2020-08-25 \u00b6 Added \u00b6 All : Official Lambda Layer via Serverless Application Repository Tracer : capture_method and capture_lambda_handler now support capture_response=False parameter to prevent Tracer to capture response as metadata to allow customers running Tracer with sensitive workloads Fixed \u00b6 Metrics : Cold start metric is now completely separate from application metrics dimensions, making it easier and cheaper to visualize. This is a breaking change if you were graphing/alerting on both application metrics with the same name to compensate this previous malfunctioning Marked as bugfix as this is the intended behaviour since the beginning, as you shouldn't have the same application metric with different dimensions Utilities : SSMProvider within Parameters utility now have decrypt and recursive parameters correctly defined to support autocompletion Added \u00b6 Tracer : capture_lambda_handler and capture_method decorators now support capture_response parameter to not include function's response as part of tracing metadata 1.3.1 - 2020-08-22 \u00b6 Fixed \u00b6 Tracer : capture_method decorator did not properly handle nested context managers 1.3.0 - 2020-08-21 \u00b6 Added \u00b6 Utilities : Add new parameters utility to retrieve a single or multiple parameters from SSM Parameter Store, Secrets Manager, DynamoDB, or your very own 1.2.0 - 2020-08-20 \u00b6 Added \u00b6 Tracer : capture_method decorator now supports generator functions (including context managers) 1.1.3 - 2020-08-18 \u00b6 Fixed \u00b6 Logger : Logs emitted twice, structured and unstructured, due to Lambda configuring the root handler 1.1.2 - 2020-08-16 \u00b6 Fixed \u00b6 Docs : Clarify confusion on Tracer reuse and auto_patch=False statement Logger : Autocomplete for log statements in PyCharm 1.1.1 - 2020-08-14 \u00b6 Fixed \u00b6 Logger : Regression on Logger level not accepting int i.e. Logger(level=logging.INFO) 1.1.0 - 2020-08-14 \u00b6 Added \u00b6 Logger : Support for logger inheritance with child parameter Fixed \u00b6 Logger : Log level is now case insensitive via params and env var 1.0.2 - 2020-07-16 \u00b6 Fixed \u00b6 Tracer : Correct AWS X-Ray SDK dependency to support 2.5.0 and higher 1.0.1 - 2020-07-06 \u00b6 Fixed \u00b6 Logger : Fix a bug with inject_lambda_context causing existing Logger keys to be overridden if structure_logs was called before 1.0.0 - 2020-06-18 \u00b6 Added \u00b6 Metrics : add_metadata method to add any metric metadata you'd like to ease finding metric related data via CloudWatch Logs Set status as General Availability 0.11.0 - 2020-06-08 \u00b6 Added \u00b6 Imports can now be made from top level of module, e.g.: from aws_lambda_powertools import Logger, Metrics, Tracer Fixed \u00b6 Metrics : Fix a bug with Metrics causing an exception to be thrown when logging metrics if dimensions were not explicitly added. Changed \u00b6 Metrics : No longer throws exception by default in case no metrics are emitted when using the log_metrics decorator. 0.10.0 - 2020-06-08 \u00b6 Added \u00b6 Metrics : capture_cold_start_metric parameter added to log_metrics decorator Metrics : Optional namespace and service parameters added to Metrics constructor to more closely resemble other core utils Changed \u00b6 Metrics : Default dimension is now created based on service parameter or POWERTOOLS_SERVICE_NAME env var Deprecated \u00b6 Metrics : add_namespace method deprecated in favor of using namespace parameter to Metrics constructor or POWERTOOLS_METRICS_NAMESPACE env var 0.9.5 - 2020-06-02 \u00b6 Fixed \u00b6 Metrics : Coerce non-string dimension values to string Logger : Correct cold_start , function_memory_size values from string to bool and int respectively 0.9.4 - 2020-05-29 \u00b6 Fixed \u00b6 Metrics : Fix issue where metrics were not correctly flushed, and cleared on every invocation 0.9.3 - 2020-05-16 \u00b6 Fixed \u00b6 Tracer : Fix Runtime Error for nested sync due to incorrect loop usage 0.9.2 - 2020-05-14 \u00b6 Fixed \u00b6 Tracer : Import aiohttp lazily so it's not a hard dependency 0.9.0 - 2020-05-12 \u00b6 Added \u00b6 Tracer : Support for async functions in Tracer via capture_method decorator Tracer : Support for aiohttp via aiohttp_trace_config trace config Tracer : Support for patching specific modules via patch_modules param Tracer : Document escape hatch mechanisms via tracer.provider 0.8.1 - 2020-05-1 \u00b6 Fixed \u00b6 Metrics : Fix metric unit casting logic if one passes plain string (value or key) Metrics: : Fix MetricUnit enum values for BytesPerSecond KilobytesPerSecond MegabytesPerSecond GigabytesPerSecond TerabytesPerSecond BitsPerSecond KilobitsPerSecond MegabitsPerSecond GigabitsPerSecond TerabitsPerSecond CountPerSecond 0.8.0 - 2020-04-24 \u00b6 Added \u00b6 Logger : Introduced Logger class for structured logging as a replacement for logger_setup Logger : Introduced Logger.inject_lambda_context decorator as a replacement for logger_inject_lambda_context Removed \u00b6 Logger : Raise DeprecationWarning exception for both logger_setup , logger_inject_lambda_context 0.7.0 - 2020-04-20 \u00b6 Added \u00b6 Middleware factory : Introduced Middleware Factory to build your own middleware via lambda_handler_decorator Fixed \u00b6 Metrics : Fixed metrics dimensions not being included correctly in EMF 0.6.3 - 2020-04-09 \u00b6 Fixed \u00b6 Logger : Fix log_metrics decorator logic not calling the decorated function, and exception handling 0.6.1 - 2020-04-08 \u00b6 Added \u00b6 Metrics : Introduces Metrics middleware to utilise CloudWatch Embedded Metric Format Deprecated \u00b6 Metrics : Added deprecation warning for log_metrics 0.5.0 - 2020-02-20 \u00b6 Added \u00b6 Logger : Introduced log sampling for debug - Thanks to Danilo's contribution 0.1.0 - 2019-11-15 \u00b6 Added \u00b6 Public beta release","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. This project follows Keep a Changelog format for changes and adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#1240-2021-12-31","text":"","title":"1.24.0 - 2021-12-31"},{"location":"changelog/#bug-fixes","text":"apigateway: support @app .not_found() syntax & housekeeping ( #926 ) event-sources: handle dynamodb null type as none, not bool ( #929 ) warning: future distutils deprecation in 3.12 ( #921 )","title":"Bug Fixes"},{"location":"changelog/#documentation","text":"general : consistency round admonitions and snippets ( #919 ) homepage : Added GraphQL Sample API to Examples section of README.md ( #930 ) batch: remove leftover from legacy layer: bump Lambda Layer to version 6 tracer: new ignore_endpoint feature ( #931 )","title":"Documentation"},{"location":"changelog/#features","text":"event-sources: cache parsed json in data class ( #909 ) feature_flags: support beyond boolean values (JSON values) ( #804 ) idempotency: support dataclasses & pydantic models payloads ( #908 ) logger: support use_datetime_directive for timestamps ( #920 ) tracer: ignore tracing for certain hostname(s) or url(s) ( #910 )","title":"Features"},{"location":"changelog/#maintenance","text":"deps-dev: bump mypy from 0.920 to 0.930 ( #925 )","title":"Maintenance"},{"location":"changelog/#1230-2021-12-20","text":"","title":"1.23.0 - 2021-12-20"},{"location":"changelog/#bug-fixes_1","text":"apigateway: allow list of HTTP methods in route method ( #838 ) event-sources: pass authorizer data to APIGatewayEventAuthorizer ( #897 ) event-sources: handle claimsOverrideDetails set to null ( #878 ) idempotency: include decorated fn name in hash ( #869 ) metrics: explicit type to single_metric ctx manager ( #865 ) parameters: mypy appconfig transform and return types ( #877 ) parser: mypy overload parse when using envelope ( #885 ) parser: kinesis sequence number is str, not int ( #907 ) parser: mypy support for payload type override as models ( #883 ) tracer: add warm start annotation (ColdStart=False) ( #851 )","title":"Bug Fixes"},{"location":"changelog/#documentation_1","text":"nav : reference cloudformation custom resource helper (CRD) ( #914 ) add new public Slack invite disable search blur in non-prod env update Lambda Layers version apigateway: add new not_found feature ( #915 ) apigateway: fix sample layout provided ( #864 ) appsync: fix users.py typo to locations #830 lambda_layer: fix CDK layer syntax","title":"Documentation"},{"location":"changelog/#features_1","text":"apigateway: add exception_handler support ( #898 ) apigateway: access parent api resolver from router ( #842 ) batch: new BatchProcessor for SQS, DynamoDB, Kinesis ( #886 ) logger: allow handler with custom kwargs signature ( #913 ) tracer: add service annotation when service is set ( #861 )","title":"Features"},{"location":"changelog/#maintenance_1","text":"minor housekeeping before release ( #912 ) correct pr label order ci: split latest docs workflow deps: bump fastjsonschema from 2.15.1 to 2.15.2 ( #891 ) deps: bump actions/setup-python from 2.2.2 to 2.3.0 ( #831 ) deps: support arm64 when developing locally ( #862 ) deps: bump actions/setup-python from 2.3.0 to 2.3.1 ( #852 ) deps: bump aws-xray-sdk from 2.8.0 to 2.9.0 ( #876 ) deps-dev: bump mypy from 0.910 to 0.920 ( #903 ) deps-dev: bump flake8 from 3.9.2 to 4.0.1 ( #789 ) deps-dev: bump black from 21.10b0 to 21.11b1 ( #839 ) deps-dev: bump black from 21.11b1 to 21.12b0 ( #872 )","title":"Maintenance"},{"location":"changelog/#1220-2021-11-17","text":"Tenet update! We've updated Idiomatic tenet to Progressive to reflect the new Router feature in Event Handler, and more importantly the new wave of customers coming from SRE, Data Analysis, and Data Science background. BEFORE: Idiomatic . Utilities follow programming language idioms and language-specific best practices. AFTER: Progressive . Utilities are designed to be incrementally adoptable for customers at any stage of their Serverless journey. They follow language idioms and their community\u2019s common practices.","title":"1.22.0 - 2021-11-17"},{"location":"changelog/#bug-fixes_2","text":"ci: change supported python version from 3.6.1 to 3.6.2, bump black ( #807 ) ci: skip sync master on docs hotfix parser: body and query strings can be null or omitted in ApiGatewayProxyEventModel and ApiGatewayProxyEventV2Model ( #820 )","title":"Bug Fixes"},{"location":"changelog/#code-refactoring","text":"apigateway: Add BaseRouter and duplicate route check ( #757 )","title":"Code Refactoring"},{"location":"changelog/#documentation_2","text":"docs: updated Lambda Layers definition & limitations. ( #775 ) docs: Idiomatic tenet updated to Progressive docs: use higher contrast font to improve accessibility ( #822 ) docs: fix indentation of SAM snippets in install section ( #778 ) docs: improve public lambda layer wording, add clipboard buttons to improve UX ( #762 ) docs: add amplify-cli instructions for public layer ( #754 ) api-gateway: add new router feature to allow route splitting in API Gateway and ALB ( #767 ) apigateway: re-add sample layout, add considerations ( #826 ) appsync: add new router feature to allow GraphQL Resolver composition ( #821 ) idempotency: add support for DynamoDB composite keys ( #808 ) tenets: update Idiomatic tenet to Progressive ( #823 ) docs: remove Lambda Layer version tag","title":"Documentation"},{"location":"changelog/#features_2","text":"apigateway: add Router to allow large routing composition ( #645 ) appsync: add Router to allow large resolver composition ( #776 ) data-classes: ActiveMQ and RabbitMQ support ( #770 ) logger: add ALB correlation ID support ( #816 )","title":"Features"},{"location":"changelog/#maintenance_2","text":"deps: bump boto3 from 1.19.6 to 1.20.3 ( #809 ) deps: bump boto3 from 1.18.58 to 1.18.59 ( #760 ) deps: bump urllib3 from 1.26.4 to 1.26.5 ( #787 ) deps: bump boto3 from 1.18.61 to 1.19.6 ( #783 ) deps: bump boto3 from 1.18.56 to 1.18.58 ( #755 ) deps: bump boto3 from 1.18.59 to 1.18.61 ( #766 ) deps: bump boto3 from 1.20.3 to 1.20.5 ( #817 ) deps-dev: bump coverage from 6.0.1 to 6.0.2 ( #764 ) deps-dev: bump pytest-asyncio from 0.15.1 to 0.16.0 ( #782 ) deps-dev: bump flake8-eradicate from 1.1.0 to 1.2.0 ( #784 ) deps-dev: bump flake8-comprehensions from 3.6.1 to 3.7.0 ( #759 ) deps-dev: bump flake8-isort from 4.0.0 to 4.1.1 ( #785 ) deps-dev: bump coverage from 6.0 to 6.0.1 ( #751 ) deps-dev: bump mkdocs-material from 7.3.3 to 7.3.5 ( #781 ) deps-dev: bump mkdocs-material from 7.3.5 to 7.3.6 ( #791 ) deps-dev: bump mkdocs-material from 7.3.2 to 7.3.3 ( #758 )","title":"Maintenance"},{"location":"changelog/#1211-2021-10-07","text":"","title":"1.21.1 - 2021-10-07"},{"location":"changelog/#regression","text":"metrics: typing regression on log_metrics callable ( #744 )","title":"Regression"},{"location":"changelog/#documentation_3","text":"add new public layer ARNs ( #746 )","title":"Documentation"},{"location":"changelog/#maintenance_3","text":"ignore constants in test cov ( #745 ) github-actions: add support for publishing fallback deps: bump boto3 from 1.18.54 to 1.18.56 ( #742 ) deps-dev: bump mkdocs-material from 7.3.1 to 7.3.2 ( #741 )","title":"Maintenance"},{"location":"changelog/#1210-2021-10-05","text":"","title":"1.21.0 - 2021-10-05"},{"location":"changelog/#bug-fixes_3","text":"data-classes: use correct asdict funciton ( #666 ) feature-flags: rules should evaluate with an AND op ( #724 ) idempotency: sorting keys before hashing ( #722 ) idempotency: sorting keys before hashing logger: push extra keys to the end ( #722 ) mypy: a few return types, type signatures, and untyped areas ( #718 )","title":"Bug Fixes"},{"location":"changelog/#code-refactoring_1","text":"data-classes: clean up internal logic for APIGatewayAuthorizerResponse ( #643 )","title":"Code Refactoring"},{"location":"changelog/#documentation_4","text":"Terraform reference for SAR Lambda Layer ( #716 ) event-handler: document catch-all routes ( #705 ) idempotency: fix misleading idempotent examples ( #661 ) jmespath: clarify envelope terminology parser: fix incorrect import in root_validator example ( #735 )","title":"Documentation"},{"location":"changelog/#features_3","text":"expose jmespath powertools functions ( #736 ) boto3 sessions in batch, parameters & idempotency ( #717 ) feature-flags : add get_raw_configuration property in store; expose store ( #720 ) feature-flags: Bring your own logger for debug ( #709 ) feature-flags: improve \"IN/NOT_IN\"; new rule actions ( #710 ) feature-flags: get_raw_configuration property in Store ( #720 ) feature_flags: Added inequality conditions ( #721 ) idempotency: makes customers unit testing easier ( #719 ) validator: include missing data elements from a validation error ( #686 )","title":"Features"},{"location":"changelog/#maintenance_4","text":"add python 3.9 support deps: bump boto3 from 1.18.51 to 1.18.54 ( #733 ) deps: bump boto3 from 1.18.32 to 1.18.38 ( #671 ) deps: bump boto3 from 1.18.38 to 1.18.41 ( #677 ) deps: bump boto3 from 1.18.49 to 1.18.51 ( #713 ) deps: bump boto3 from 1.18.41 to 1.18.49 ( #703 ) deps: bump codecov/codecov-action from 2.0.2 to 2.1.0 ( #675 ) deps-dev: bump coverage from 5.5 to 6.0 ( #732 ) deps-dev: bump mkdocs-material from 7.2.8 to 7.3.0 ( #695 ) deps-dev: bump mkdocs-material from 7.2.6 to 7.2.8 ( #682 ) deps-dev: bump flake8-bugbear from 21.4.3 to 21.9.1 ( #676 ) deps-dev: bump flake8-bugbear from 21.9.1 to 21.9.2 ( #712 ) deps-dev: bump radon from 4.5.2 to 5.1.0 ( #673 ) deps-dev: bump mkdocs-material from 7.3.0 to 7.3.1 ( #731 ) deps-dev: bump xenon from 0.7.3 to 0.8.0 ( #669 )","title":"Maintenance"},{"location":"changelog/#bug-fixes_4","text":"event-handler: fix issue with strip_prefixes and root level resolvers ( #646 )","title":"Bug Fixes"},{"location":"changelog/#maintenance_5","text":"deps: bump boto3 from 1.18.26 to 1.18.32 ( #663 ) deps-dev: bump mkdocs-material from 7.2.4 to 7.2.6 ( #665 ) deps-dev: bump pytest from 6.2.4 to 6.2.5 ( #662 ) deps-dev: bump mike from 0.6.0 to 1.0.1 ( #453 ) license: add third party license to pyproject.toml ( #641 )","title":"Maintenance"},{"location":"changelog/#1202-2021-09-02","text":"","title":"1.20.2 - 2021-09-02"},{"location":"changelog/#bug-fixes_5","text":"event-handler: fix issue with strip_prefixes and root level resolvers ( #646 )","title":"Bug Fixes"},{"location":"changelog/#maintenance_6","text":"deps: bump boto3 from 1.18.26 to 1.18.32 ( #663 ) deps-dev: bump mkdocs-material from 7.2.4 to 7.2.6 ( #665 ) deps-dev: bump pytest from 6.2.4 to 6.2.5 ( #662 ) deps-dev: bump mike from 0.6.0 to 1.0.1 ( #453 ) license: add third party license to pyproject.toml ( #641 )","title":"Maintenance"},{"location":"changelog/#1201-2021-08-22","text":"","title":"1.20.1 - 2021-08-22"},{"location":"changelog/#bug-fixes_6","text":"idempotency: sorting keys before hashing ( #639 )","title":"Bug Fixes"},{"location":"changelog/#maintenance_7","text":"markdown linter fixes ( #636 ) setup codespaces ( #637 ) license: add third party license ( #635 )","title":"Maintenance"},{"location":"changelog/#1200-2021-08-21","text":"","title":"1.20.0 - 2021-08-21"},{"location":"changelog/#bug-fixes_7","text":"api-gateway: HTTP API strip stage name from request path ( #622 )","title":"Bug Fixes"},{"location":"changelog/#code-refactoring_2","text":"event-handler: match to match_results; 3.10 new keyword ( #616 )","title":"Code Refactoring"},{"location":"changelog/#documentation_5","text":"data-classes: make authorizer concise; use enum ( #630 ) feature-flags: correct link and json examples ( #605 ) data-class: fix invalid syntax in new AppSync Authorizer api-gateway: add new API mapping support","title":"Documentation"},{"location":"changelog/#features_4","text":"data-classes: authorizer for API Gateway HTTP and REST API ( #620 ) data-classes: new data_as_bytes property in KinesisStreamRecordPayload ( #628 ) data-classes: AppSync Lambda authorizer event ( #610 ) event-handler: prefixes to strip for custom domain mapping paths ( #579 ) general: support for Python 3.9 ( #626 ) idempotency: support for any synchronous function ( #625 )","title":"Features"},{"location":"changelog/#maintenance_8","text":"actions: include new labels api-docs: enable allow_reuse to fix the docs ( #612 ) deps: bump boto3 from 1.18.25 to 1.18.26 ( #627 ) deps: bump boto3 from 1.18.24 to 1.18.25 ( #623 ) deps: bump boto3 from 1.18.22 to 1.18.24 ( #619 ) deps: bump boto3 from 1.18.17 to 1.18.21 ( #608 ) deps: bump boto3 from 1.18.21 to 1.18.22 ( #614 ) deps-dev: bump flake8-comprehensions from 3.5.0 to 3.6.0 ( #609 ) deps-dev: bump flake8-comprehensions from 3.6.0 to 3.6.1 ( #615 ) deps-dev: bump mkdocs-material from 7.2.3 to 7.2.4 ( #607 ) docs: correct markdown based on markdown lint ( #603 ) shared: fix cyclic import & refactor data extraction fn ( #613 )","title":"Maintenance"},{"location":"changelog/#1190-2021-08-11","text":"","title":"1.19.0 - 2021-08-11"},{"location":"changelog/#bug-fixes_8","text":"deps: bump poetry to latest ( #592 ) feature-flags: bug handling multiple conditions ( #599 ) parser: API Gateway WebSocket validation under check_message_id; plus some housekeeping ( #553 ) feature-toggles: correct cdk example ( #601 )","title":"Bug Fixes"},{"location":"changelog/#code-refactoring_3","text":"feature-flags: add debug statements for all feature evaluations ( #590 ) feature-flags: optimize UX and maintenance ( #563 )","title":"Code Refactoring"},{"location":"changelog/#documentation_6","text":"event-handler: new custom serializer option feature-flags: create concrete documentation ( #594 ) feature-flags: correct docs and typing ( #588 ) parameters: auto-transforming values based on suffix ( #573 ) readme: add code coverage badge ( #577 ) tracer: update wording that it auto-disables on non-Lambda env feature-flags: fix SAM infra, convert CDK to Python feature-flags: fix sample feature name in evaluate method feature-flags: add guidance when to use vs env vars vs parameters","title":"Documentation"},{"location":"changelog/#features_5","text":"api-gateway: add support for custom serializer ( #568 ) data-classes: decode json_body if based64 encoded ( #560 ) feature-flags: Add not_in action and rename contains to in ( #589 ) params: expose params max_age , raise_on_transform_error to high level functions ( #567 ) tracer: auto-disable tracer for non-Lambda environments to ease testing ( #598 )","title":"Features"},{"location":"changelog/#maintenance_9","text":"deps: bump boto3 from 1.18.1 to 1.18.15 ( #591 ) deps: bump codecov/codecov-action from 2.0.1 to 2.0.2 ( #558 ) deps: bump boto3 from 1.18.15 to 1.18.17 ( #597 ) deps-dev: bump mkdocs-material from 7.2.2 to 7.2.3 ( #596 ) deps-dev: bump mkdocs-material from 7.2.1 to 7.2.2 ( #582 ) deps-dev: bump pdoc3 from 0.9.2 to 0.10.0 ( #584 ) deps-dev: bump isort from 5.9.2 to 5.9.3 ( #574 ) deps-dev: bump mkdocs-material from 7.2.0 to 7.2.1 ( #566 ) deps-dev: bump mkdocs-material from 7.1.11 to 7.2.0 ( #551 ) deps-dev: bump flake8-black from 0.2.1 to 0.2.3 ( #541 )","title":"Maintenance"},{"location":"changelog/#1181-2021-07-23","text":"","title":"1.18.1 - 2021-07-23"},{"location":"changelog/#bug-fixes_9","text":"api-gateway: route regression for non-word and unsafe URI chars ( #556 )","title":"Bug Fixes"},{"location":"changelog/#1180-2021-07-20","text":"","title":"1.18.0 - 2021-07-20"},{"location":"changelog/#bug-fixes_10","text":"api-gateway: non-greedy route pattern regex which incorrectly mapped certain route params to function params ( #533 ) api-gateway: incorrect plain text mimetype constant #506 data-classes: include milliseconds in scalar types to correctly align with AppSync scalars ( #504 ) mypy: addresses lack of optional types ( #521 ) parser: make ApiGateway version, authorizer fields optional ( #532 ) tracer: mypy generic to preserve decorated method signature ( #529 )","title":"Bug Fixes"},{"location":"changelog/#code-refactoring_4","text":"feature-toggles: code coverage and housekeeping ( #530 )","title":"Code Refactoring"},{"location":"changelog/#documentation_7","text":"api-gateway: new HTTP service error exceptions ( #546 ) logger: new get_correlation_id method ( #545 )","title":"Documentation"},{"location":"changelog/#features_6","text":"api-gateway: add debug mode ( #507 ) api-gateway: add common HTTP service errors ( #506 ) event-handler: Support AppSyncResolverEvent subclassing ( #526 ) feat-toggle: New simple feature toggles rule engine (WIP) ( #494 ) logger: add get_correlation_id method ( #516 )","title":"Features"},{"location":"changelog/#maintenance_10","text":"mypy: add mypy support to makefile ( #508 ) deps: bump codecov/codecov-action from 1 to 2.0.1 ( #539 ) deps: bump boto3 from 1.18.0 to 1.18.1 ( #528 ) deps: bump boto3 from 1.17.110 to 1.18.0 ( #527 ) deps: bump boto3 from 1.17.102 to 1.17.110 ( #523 ) deps-dev: bump mkdocs-material from 7.1.10 to 7.1.11 ( #542 ) deps-dev: bump mkdocs-material from 7.1.9 to 7.1.10 ( #522 ) deps-dev: bump isort from 5.9.1 to 5.9.2 ( #514 ) event-handler: adjusts API Gateway/ALB service errors exception docstrings to not confuse AppSync customers","title":"Maintenance"},{"location":"changelog/#1171-2021-07-02","text":"","title":"1.17.1 - 2021-07-02"},{"location":"changelog/#bug-fixes_11","text":"Validator: Handle built-in custom formats like date-time when type is string ( #498 )","title":"Bug Fixes"},{"location":"changelog/#documentation_8","text":"Layers: Add Layers example for Serverless framework & CDK ( #500 ) Misc.: Enable dark mode switch ( #471 ) Tracer: Additional scenario when to disable auto-capture for responses larger than 64K ( #499 )","title":"Documentation"},{"location":"changelog/#maintenance_11","text":"deps: bump boto3 from 1.17.101 to 1.17.102 ( #493 ) deps: bump boto3 from 1.17.91 to 1.17.101 ( #490 ) deps: bump email-validator from 1.1.2 to 1.1.3 ( #478 ) deps: bump boto3 from 1.17.89 to 1.17.91 ( #473 ) deps-dev: bump flake8-eradicate from 1.0.0 to 1.1.0 ( #492 ) deps-dev: bump isort from 5.8.0 to 5.9.1 ( #487 ) deps-dev: bump mkdocs-material from 7.1.7 to 7.1.9 ( #491 )","title":"Maintenance"},{"location":"changelog/#1170-2021-06-08","text":"","title":"1.17.0 - 2021-06-08"},{"location":"changelog/#added","text":"Documentation : Include new public roadmap ( #452 ) Documentation : Remove old todo in idempotency docs Data classes: New AttributeValueType to get type and value from data in DynamoDBStreamEvent ( #462 ) Data classes: New decorator event_source to instantiate data_classes ( #442 ) Logger: New clear_state parameter to clear previously added custom keys upon invocation ( #467 ) Parser: Support for API Gateway HTTP API #434 ( #441 )","title":"Added"},{"location":"changelog/#maintenance_12","text":"deps : bump xenon from 0.7.1 to 0.7.3 ( #446 ) assited changelog pre-generation, auto-label PR ( #443 ) enable dependabot for dep upgrades ( #444 ) enable mergify ( #450 ) deps : bump mkdocs-material from 7.1.5 to 7.1.6 ( #451 ) deps : bump boto3 from 1.17.78 to 1.17.84 ( #449 ) update mergify to require approval on dependabot ( #456 ) deps : bump actions/setup-python from 1 to 2.2.2 ( #445 ) deps: bump boto3 from 1.17.87 to 1.17.88 ( #463 ) deps: bump boto3 from 1.17.88 to 1.17.89 ( #466 ) deps: bump boto3 from 1.17.84 to 1.17.85 ( #455 ) deps: bump boto3 from 1.17.85 to 1.17.86 ( #458 ) deps: bump boto3 from 1.17.86 to 1.17.87 ( #459 ) deps-dev: bump mkdocs-material from 7.1.6 to 7.1.7 ( #464 ) deps-dev: bump pytest-cov from 2.12.0 to 2.12.1 ( #454 ) mergify: disable check for matrix jobs mergify: use job name to match GH Actions","title":"Maintenance"},{"location":"changelog/#1161-2021-05-23","text":"","title":"1.16.1 - 2021-05-23"},{"location":"changelog/#fixed","text":"Parser : Upgrade Pydantic to 1.8.2 due to CVE-2021-29510","title":"Fixed"},{"location":"changelog/#1160-2021-05-17","text":"","title":"1.16.0 - 2021-05-17"},{"location":"changelog/#features_7","text":"data-classes(API Gateway, ALB): New method to decode base64 encoded body ( #425 ) data-classes(CodePipeline): Support for CodePipeline job event and methods to handle artifacts more easily ( #416 )","title":"Features"},{"location":"changelog/#1151-2021-05-13","text":"","title":"1.15.1 - 2021-05-13"},{"location":"changelog/#fixed_1","text":"Logger : Fix a regression with the %s operator","title":"Fixed"},{"location":"changelog/#1150-2021-05-06","text":"","title":"1.15.0 - 2021-05-06"},{"location":"changelog/#added_1","text":"Event handlers : New API Gateway and ALB utility to reduce routing boilerplate and more Documentation : Logger enhancements such as bring your own formatter, handler, UTC support, and testing for Python 3.6 Parser : Support for API Gateway REST Proxy event and envelope Logger : Support for bringing custom formatter, custom handler, custom JSON serializer and deserializer, UTC support, expose LambdaPowertoolsFormatter Metrics : Support for persisting default dimensions that should always be added","title":"Added"},{"location":"changelog/#fixed_2","text":"Documentation : Fix highlights, Parser types Validator : Fix event type annotations for validate standalone function Parser : Improve and fix types Internal : Remove X-Ray SDK version pinning as serialization regression has been fixed in 2.8.0 Internal : Latest documentation correctly includes a copy of API docs reference","title":"Fixed"},{"location":"changelog/#1140-2021-04-09","text":"","title":"1.14.0 - 2021-04-09"},{"location":"changelog/#added_2","text":"Event handlers : New core utility to easily handle incoming requests tightly integrated with Data Classes; AppSync being the first as we gauge from the community what additional ones would be helpful Documentation : Enabled versioning to access docs on a per release basis or staging docs ( develop branch) Documentation : Links now open in a new tab and improved snippet line highlights Documentation(validation) : JSON Schema snippets and more complete examples Documentation(idempotency) : Table with expected configuration values for hash key and TTL attribute name when using the default behaviour Documentation(logger) : New example on how to set logging record timestamps in UTC Parser(S3) : Support for the new S3 Object Lambda Event model ( S3ObjectLambdaEvent ) Parameters : Support for DynamoDB Local via endpoint_url parameter, including docs Internal : Include make pr in pre-commit hooks when contributing to shorten feedback loop on pre-commit specific linting","title":"Added"},{"location":"changelog/#fixed_3","text":"Parser : S3Model now supports keys with 0 length Tracer : Lock X-Ray SDK to 2.6.0 as there's been a regression upstream in 2.7.0 on serializing & capturing exceptions Data Classes(API Gateway) : Add missing property operationName within request context Misc. : Numerous typing fixes to better to support MyPy across all utilities Internal : Downgraded poetry to 1.1.4 as there's been a regression with importlib-metadata in 1.1.5 not yet fixed","title":"Fixed"},{"location":"changelog/#1130-2021-03-23","text":"","title":"1.13.0 - 2021-03-23"},{"location":"changelog/#added_3","text":"Data Classes : New S3 Object Lambda event","title":"Added"},{"location":"changelog/#fixed_4","text":"Docs : Lambda Layer SAM template reference example","title":"Fixed"},{"location":"changelog/#1120-2021-03-17","text":"","title":"1.12.0 - 2021-03-17"},{"location":"changelog/#added_4","text":"Parameters : New force_fetch param to always fetch the latest and bypass cache, if available Data Classes : New AppSync Lambda Resolver event covering both Direct Lambda Resolver and Amplify GraphQL Transformer Resolver @function Data Classes : New AppSync scalar utilities to easily compose Lambda Resolvers with date utils, uuid, etc. Logger : Support for Correlation ID both in inject_lambda_context decorator and set_correlation_id method Logger : Include new exception_name key to help customers easily enumerate exceptions across all functions","title":"Added"},{"location":"changelog/#fixed_5","text":"Tracer : Type hint on return instance that made PyCharm no longer recognize autocompletion Idempotency : Error handling for missing idempotency key and save_in_progress errors","title":"Fixed"},{"location":"changelog/#1110-2021-03-05","text":"","title":"1.11.0 - 2021-03-05"},{"location":"changelog/#fixed_6","text":"Tracer : Lazy loads X-Ray SDK to increase perf by 75% for those not instantiating Tracer Metrics : Optimize validation and serialization to increase perf by nearly 50% for large operations (<1ms)","title":"Fixed"},{"location":"changelog/#added_5","text":"Dataclass : Add new Amazon Connect contact flow event Idempotency : New Idempotency utility Docs : Add example on how to integrate Batch utility with Sentry.io Internal : Added performance SLA tests for high level imports and Metrics validation/serialization","title":"Added"},{"location":"changelog/#1105-2021-02-17","text":"No changes. Bumped version to trigger new pipeline build for layer publishing.","title":"1.10.5 - 2021-02-17"},{"location":"changelog/#1104-2021-02-17","text":"","title":"1.10.4 - 2021-02-17"},{"location":"changelog/#fixed_7","text":"Docs : Fix anchor tags to be lower case Docs : Correct the docs location for the labeller","title":"Fixed"},{"location":"changelog/#1103-2021-02-04","text":"","title":"1.10.3 - 2021-02-04"},{"location":"changelog/#added_6","text":"Docs : Migrated from Gatsby to MKdocs documentation system Docs : Included Getting started and Advanced sections in Core utilities, including additional examples","title":"Added"},{"location":"changelog/#fixed_8","text":"Tracer : Disabled batching segments as X-Ray SDK does not flush traces upon reaching limits Parser : Model type is now compliant with mypy","title":"Fixed"},{"location":"changelog/#1102-2021-02-04","text":"","title":"1.10.2 - 2021-02-04"},{"location":"changelog/#fixed_9","text":"Utilities : Correctly handle and list multiple exceptions in SQS batch processing utility. * Docs :: Fix typos on AppConfig docstring import, and SnsModel typo in parser. Utilities : typing_extensions package is now only installed in Python < 3.8","title":"Fixed"},{"location":"changelog/#1101-2021-01-19","text":"","title":"1.10.1 - 2021-01-19"},{"location":"changelog/#fixed_10","text":"Utilities : Added SnsSqsEnvelope in parser to dynamically adjust model mismatch when customers use SNS + SQS instead of SNS + Lambda, since we've discovered three payload keys are slightly different.","title":"Fixed"},{"location":"changelog/#1100-2021-01-18","text":"","title":"1.10.0 - 2021-01-18"},{"location":"changelog/#added_7","text":"Utilities : Added support for AppConfig in Parameters utility Logger : Added support for extra parameter to add additional root fields when logging messages Logger : Added support to Pytest Live Log feat. via feature toggle POWERTOOLS_LOG_DEDUPLICATION_DISABLED Tracer : Added support to disable auto-capturing response and exception as metadata Utilities : Added support to handle custom string/integer formats in JSON Schema in Validator utility Install : Added new Lambda Layer with all extra dependencies installed, available in Serverless Application Repository (SAR)","title":"Added"},{"location":"changelog/#fixed_11","text":"Docs : Added missing SNS parser model Docs : Added new environment variables for toggling features in Logger and Tracer: POWERTOOLS_LOG_DEDUPLICATION_DISABLED , POWERTOOLS_TRACER_CAPTURE_RESPONSE , POWERTOOLS_TRACER_CAPTURE_ERROR Docs : Fixed incorrect import for Cognito data classes in Event Sources utility","title":"Fixed"},{"location":"changelog/#191-2020-12-21","text":"","title":"1.9.1 - 2020-12-21"},{"location":"changelog/#fixed_12","text":"Logger : Bugfix to prevent parent loggers with the same name being configured more than once","title":"Fixed"},{"location":"changelog/#added_8","text":"Docs : Add clarification to Tracer docs for how capture_method decorator can cause function responses to be read and serialized. Utilities : Added equality to ease testing Event source data classes Package : Added py.typed for initial work needed for PEP 561 compliance","title":"Added"},{"location":"changelog/#190-2020-12-04","text":"","title":"1.9.0 - 2020-12-04"},{"location":"changelog/#added_9","text":"Utilities : Added Kinesis, S3, CloudWatch Logs, Application Load Balancer, and SES support in Parser Docs : Sidebar menu are now always expanded","title":"Added"},{"location":"changelog/#fixed_13","text":"Docs : Broken link to GitHub to homepage","title":"Fixed"},{"location":"changelog/#180-2020-11-20","text":"","title":"1.8.0 - 2020-11-20"},{"location":"changelog/#added_10","text":"Utilities : Added support for new EventBridge Replay field in Parser and Event source data classes Utilities : Added SNS support in Parser Utilities : Added API Gateway HTTP API data class support for new IAM and Lambda authorizer in Event source data classes Docs : Add new FAQ section for Logger on how to enable debug logging for boto3 Docs : Add explicit minimal set of permissions required to use Layers provided by Serverless Application Repository (SAR)","title":"Added"},{"location":"changelog/#fixed_14","text":"Docs : Fix typo in Dataclasses example for SES when fetching common email headers","title":"Fixed"},{"location":"changelog/#170-2020-10-26","text":"","title":"1.7.0 - 2020-10-26"},{"location":"changelog/#added_11","text":"Utilities : Add new Parser utility to provide parsing and deep data validation using Pydantic Models Utilities : Add case insensitive header lookup, and Cognito custom auth triggers to Event source data classes","title":"Added"},{"location":"changelog/#fixed_15","text":"Logger : keeps Lambda root logger handler, and add log filter instead to prevent child log records duplication Docs : Improve wording on adding log keys conditionally","title":"Fixed"},{"location":"changelog/#161-2020-09-23","text":"","title":"1.6.1 - 2020-09-23"},{"location":"changelog/#fixed_16","text":"Utilities : Fix issue with boolean values in DynamoDB stream event data class.","title":"Fixed"},{"location":"changelog/#160-2020-09-22","text":"","title":"1.6.0 - 2020-09-22"},{"location":"changelog/#added_12","text":"Metrics : Support adding multiple metric values to a single metric name Utilities : Add new Validator utility to validate inbound events and responses using JSON Schema Utilities : Add new Event source data classes utility to easily describe event schema of popular event sources Docs : Add new Testing your code section to both Logger and Metrics page, and content width is now wider Tracer : Support for automatically disable Tracer when running a Chalice app","title":"Added"},{"location":"changelog/#fixed_17","text":"Docs : Improve wording on log sampling feature in Logger, and removed duplicate content on main page Utilities : Remove DeleteMessageBatch API call when there are no messages to delete","title":"Fixed"},{"location":"changelog/#150-2020-09-04","text":"","title":"1.5.0 - 2020-09-04"},{"location":"changelog/#added_13","text":"Logger : Add xray_trace_id to log output to improve integration with CloudWatch Service Lens Logger : Allow reordering of logged output Utilities : Add new SQS batch processing utility to handle partial failures in processing message batches Utilities : Add typing utility providing static type for lambda context object Utilities : Add transform=auto in parameters utility to deserialize parameter values based on the key name","title":"Added"},{"location":"changelog/#fixed_18","text":"Logger : The value of json_default formatter is no longer written to logs","title":"Fixed"},{"location":"changelog/#140-2020-08-25","text":"","title":"1.4.0 - 2020-08-25"},{"location":"changelog/#added_14","text":"All : Official Lambda Layer via Serverless Application Repository Tracer : capture_method and capture_lambda_handler now support capture_response=False parameter to prevent Tracer to capture response as metadata to allow customers running Tracer with sensitive workloads","title":"Added"},{"location":"changelog/#fixed_19","text":"Metrics : Cold start metric is now completely separate from application metrics dimensions, making it easier and cheaper to visualize. This is a breaking change if you were graphing/alerting on both application metrics with the same name to compensate this previous malfunctioning Marked as bugfix as this is the intended behaviour since the beginning, as you shouldn't have the same application metric with different dimensions Utilities : SSMProvider within Parameters utility now have decrypt and recursive parameters correctly defined to support autocompletion","title":"Fixed"},{"location":"changelog/#added_15","text":"Tracer : capture_lambda_handler and capture_method decorators now support capture_response parameter to not include function's response as part of tracing metadata","title":"Added"},{"location":"changelog/#131-2020-08-22","text":"","title":"1.3.1 - 2020-08-22"},{"location":"changelog/#fixed_20","text":"Tracer : capture_method decorator did not properly handle nested context managers","title":"Fixed"},{"location":"changelog/#130-2020-08-21","text":"","title":"1.3.0 - 2020-08-21"},{"location":"changelog/#added_16","text":"Utilities : Add new parameters utility to retrieve a single or multiple parameters from SSM Parameter Store, Secrets Manager, DynamoDB, or your very own","title":"Added"},{"location":"changelog/#120-2020-08-20","text":"","title":"1.2.0 - 2020-08-20"},{"location":"changelog/#added_17","text":"Tracer : capture_method decorator now supports generator functions (including context managers)","title":"Added"},{"location":"changelog/#113-2020-08-18","text":"","title":"1.1.3 - 2020-08-18"},{"location":"changelog/#fixed_21","text":"Logger : Logs emitted twice, structured and unstructured, due to Lambda configuring the root handler","title":"Fixed"},{"location":"changelog/#112-2020-08-16","text":"","title":"1.1.2 - 2020-08-16"},{"location":"changelog/#fixed_22","text":"Docs : Clarify confusion on Tracer reuse and auto_patch=False statement Logger : Autocomplete for log statements in PyCharm","title":"Fixed"},{"location":"changelog/#111-2020-08-14","text":"","title":"1.1.1 - 2020-08-14"},{"location":"changelog/#fixed_23","text":"Logger : Regression on Logger level not accepting int i.e. Logger(level=logging.INFO)","title":"Fixed"},{"location":"changelog/#110-2020-08-14","text":"","title":"1.1.0 - 2020-08-14"},{"location":"changelog/#added_18","text":"Logger : Support for logger inheritance with child parameter","title":"Added"},{"location":"changelog/#fixed_24","text":"Logger : Log level is now case insensitive via params and env var","title":"Fixed"},{"location":"changelog/#102-2020-07-16","text":"","title":"1.0.2 - 2020-07-16"},{"location":"changelog/#fixed_25","text":"Tracer : Correct AWS X-Ray SDK dependency to support 2.5.0 and higher","title":"Fixed"},{"location":"changelog/#101-2020-07-06","text":"","title":"1.0.1 - 2020-07-06"},{"location":"changelog/#fixed_26","text":"Logger : Fix a bug with inject_lambda_context causing existing Logger keys to be overridden if structure_logs was called before","title":"Fixed"},{"location":"changelog/#100-2020-06-18","text":"","title":"1.0.0 - 2020-06-18"},{"location":"changelog/#added_19","text":"Metrics : add_metadata method to add any metric metadata you'd like to ease finding metric related data via CloudWatch Logs Set status as General Availability","title":"Added"},{"location":"changelog/#0110-2020-06-08","text":"","title":"0.11.0 - 2020-06-08"},{"location":"changelog/#added_20","text":"Imports can now be made from top level of module, e.g.: from aws_lambda_powertools import Logger, Metrics, Tracer","title":"Added"},{"location":"changelog/#fixed_27","text":"Metrics : Fix a bug with Metrics causing an exception to be thrown when logging metrics if dimensions were not explicitly added.","title":"Fixed"},{"location":"changelog/#changed","text":"Metrics : No longer throws exception by default in case no metrics are emitted when using the log_metrics decorator.","title":"Changed"},{"location":"changelog/#0100-2020-06-08","text":"","title":"0.10.0 - 2020-06-08"},{"location":"changelog/#added_21","text":"Metrics : capture_cold_start_metric parameter added to log_metrics decorator Metrics : Optional namespace and service parameters added to Metrics constructor to more closely resemble other core utils","title":"Added"},{"location":"changelog/#changed_1","text":"Metrics : Default dimension is now created based on service parameter or POWERTOOLS_SERVICE_NAME env var","title":"Changed"},{"location":"changelog/#deprecated","text":"Metrics : add_namespace method deprecated in favor of using namespace parameter to Metrics constructor or POWERTOOLS_METRICS_NAMESPACE env var","title":"Deprecated"},{"location":"changelog/#095-2020-06-02","text":"","title":"0.9.5 - 2020-06-02"},{"location":"changelog/#fixed_28","text":"Metrics : Coerce non-string dimension values to string Logger : Correct cold_start , function_memory_size values from string to bool and int respectively","title":"Fixed"},{"location":"changelog/#094-2020-05-29","text":"","title":"0.9.4 - 2020-05-29"},{"location":"changelog/#fixed_29","text":"Metrics : Fix issue where metrics were not correctly flushed, and cleared on every invocation","title":"Fixed"},{"location":"changelog/#093-2020-05-16","text":"","title":"0.9.3 - 2020-05-16"},{"location":"changelog/#fixed_30","text":"Tracer : Fix Runtime Error for nested sync due to incorrect loop usage","title":"Fixed"},{"location":"changelog/#092-2020-05-14","text":"","title":"0.9.2 - 2020-05-14"},{"location":"changelog/#fixed_31","text":"Tracer : Import aiohttp lazily so it's not a hard dependency","title":"Fixed"},{"location":"changelog/#090-2020-05-12","text":"","title":"0.9.0 - 2020-05-12"},{"location":"changelog/#added_22","text":"Tracer : Support for async functions in Tracer via capture_method decorator Tracer : Support for aiohttp via aiohttp_trace_config trace config Tracer : Support for patching specific modules via patch_modules param Tracer : Document escape hatch mechanisms via tracer.provider","title":"Added"},{"location":"changelog/#081-2020-05-1","text":"","title":"0.8.1 - 2020-05-1"},{"location":"changelog/#fixed_32","text":"Metrics : Fix metric unit casting logic if one passes plain string (value or key) Metrics: : Fix MetricUnit enum values for BytesPerSecond KilobytesPerSecond MegabytesPerSecond GigabytesPerSecond TerabytesPerSecond BitsPerSecond KilobitsPerSecond MegabitsPerSecond GigabitsPerSecond TerabitsPerSecond CountPerSecond","title":"Fixed"},{"location":"changelog/#080-2020-04-24","text":"","title":"0.8.0 - 2020-04-24"},{"location":"changelog/#added_23","text":"Logger : Introduced Logger class for structured logging as a replacement for logger_setup Logger : Introduced Logger.inject_lambda_context decorator as a replacement for logger_inject_lambda_context","title":"Added"},{"location":"changelog/#removed","text":"Logger : Raise DeprecationWarning exception for both logger_setup , logger_inject_lambda_context","title":"Removed"},{"location":"changelog/#070-2020-04-20","text":"","title":"0.7.0 - 2020-04-20"},{"location":"changelog/#added_24","text":"Middleware factory : Introduced Middleware Factory to build your own middleware via lambda_handler_decorator","title":"Added"},{"location":"changelog/#fixed_33","text":"Metrics : Fixed metrics dimensions not being included correctly in EMF","title":"Fixed"},{"location":"changelog/#063-2020-04-09","text":"","title":"0.6.3 - 2020-04-09"},{"location":"changelog/#fixed_34","text":"Logger : Fix log_metrics decorator logic not calling the decorated function, and exception handling","title":"Fixed"},{"location":"changelog/#061-2020-04-08","text":"","title":"0.6.1 - 2020-04-08"},{"location":"changelog/#added_25","text":"Metrics : Introduces Metrics middleware to utilise CloudWatch Embedded Metric Format","title":"Added"},{"location":"changelog/#deprecated_1","text":"Metrics : Added deprecation warning for log_metrics","title":"Deprecated"},{"location":"changelog/#050-2020-02-20","text":"","title":"0.5.0 - 2020-02-20"},{"location":"changelog/#added_26","text":"Logger : Introduced log sampling for debug - Thanks to Danilo's contribution","title":"Added"},{"location":"changelog/#010-2019-11-15","text":"","title":"0.1.0 - 2019-11-15"},{"location":"changelog/#added_27","text":"Public beta release","title":"Added"},{"location":"core/logger/","text":"Logger provides an opinionated logger with output structured as JSON. Key features \u00b6 Capture key fields from Lambda context, cold start and structures logging output as JSON Log Lambda event when instructed (disabled by default) Log sampling enables DEBUG log level for a percentage of requests (disabled by default) Append additional keys to structured log at any point in time Getting started \u00b6 Logger requires two settings: Setting Description Environment variable Constructor parameter Logging level Sets how verbose Logger should be (INFO, by default) LOG_LEVEL level Service Sets service key that will be present across all log statements POWERTOOLS_SERVICE_NAME service Example AWS Serverless Application Model (SAM) template.yaml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_SERVICE_NAME : example app.py 1 2 3 from aws_lambda_powertools import Logger logger = Logger () # Sets service via env var # OR logger = Logger(service=\"example\") Standard structured keys \u00b6 Your Logger will include the following keys to your structured logging: Key Example Note level : str INFO Logging level location : str collect.handler:1 Source code location where statement was executed message : Any Collecting payment Unserializable JSON values are casted as str timestamp : str 2021-05-03 10:20:19,650+0200 Timestamp with milliseconds, by default uses local timezone service : str payment Service name defined, by default service_undefined xray_trace_id : str 1-5759e988-bd862e3fe1be46a994272793 When tracing is enabled , it shows X-Ray Trace ID sampling_rate : float 0.1 When enabled, it shows sampling rate in percentage e.g. 10% exception_name : str ValueError When logger.exception is used and there is an exception exception : str Traceback (most recent call last).. When logger.exception is used and there is an exception Capturing Lambda context info \u00b6 You can enrich your structured logs with key Lambda context information via inject_lambda_context . collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context def handler ( event , context ): logger . info ( \"Collecting payment\" ) # You can log entire objects too logger . info ({ \"operation\" : \"collect_payment\" , \"charge_id\" : event [ 'charge_id' ] }) ... Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : { \"operation\" : \"collect_payment\" , \"charge_id\" : \"ch_AZFlk2345C0\" }, \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } When used, this will include the following keys: Key Example cold_start : bool false function_name str example-powertools-HelloWorldFunction-1P1Z6B39FLU73 function_memory_size : int 128 function_arn : str arn:aws:lambda:eu-west-1:012345678910:function:example-powertools-HelloWorldFunction-1P1Z6B39FLU73 function_request_id : str 899856cb-83d1-40d7-8611-9e78f15f32f4 Logging incoming event \u00b6 When debugging in non-production environments, you can instruct Logger to log the incoming event with log_event param or via POWERTOOLS_LOGGER_LOG_EVENT env var. Warning This is disabled by default to prevent sensitive info being logged Logging incoming event 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( log_event = True ) def handler ( event , context ): ... Setting a Correlation ID \u00b6 You can set a Correlation ID using correlation_id_path param by passing a JMESPath expression . Tip You can retrieve correlation IDs via get_correlation_id method collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( correlation_id_path = \"headers.my_request_id_header\" ) def handler ( event , context ): logger . debug ( f \"Correlation ID => { logger . get_correlation_id () } \" ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"headers\" : { \"my_request_id_header\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"correlation_id\" : \"correlation_id_value\" } We provide built-in JMESPath expressions for known event sources, where either a request ID or X-Ray Trace ID are present. collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging import correlation_paths logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) def handler ( event , context ): logger . debug ( f \"Correlation ID => { logger . get_correlation_id () } \" ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:8\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"correlation_id\" : \"correlation_id_value\" } Appending additional keys \u00b6 Info: Custom keys are persisted across warm invocations Always set additional keys as part of your handler to ensure they have the latest value, or explicitly clear them with clear_state=True . You can append additional keys using either mechanism: Persist new keys across all future log messages via append_keys method Add additional keys on a per log message basis via extra parameter append_keys method \u00b6 Note append_keys replaces structure_logs(append=True, **kwargs) method. structure_logs will be removed in v2. You can append your own keys to your existing Logger via append_keys(**additional_key_values) method. collect.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): order_id = event . get ( \"order_id\" ) # this will ensure order_id key always has the latest value before logging logger . append_keys ( order_id = order_id ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:11\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"order_id\" : \"order_id_value\" } Tip: Logger will automatically reject any key with a None value If you conditionally add keys depending on the payload, you can follow the example above. This example will add order_id if its value is not empty, and in subsequent invocations where order_id might not be present it'll remove it from the Logger. extra parameter \u00b6 Extra parameter is available for all log levels' methods, as implemented in the standard logging library - e.g. logger.info, logger.warning . It accepts any dictionary, and all keyword arguments will be added as part of the root structure of the logs for that log statement. Info Any keyword argument added using extra will not be persisted for subsequent messages. extra_parameter.py 1 2 3 4 5 6 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) fields = { \"request_id\" : \"1123\" } logger . info ( \"Collecting payment\" , extra = fields ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:6\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"request_id\" : \"1123\" } set_correlation_id method \u00b6 You can set a correlation_id to your existing Logger via set_correlation_id(value) method by passing any string value. collect.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . set_correlation_id ( event [ \"requestContext\" ][ \"requestId\" ]) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"correlation_id\" : \"correlation_id_value\" } Alternatively, you can combine Data Classes utility with Logger to use dot notation object: collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent logger = Logger ( service = \"payment\" ) def handler ( event , context ): event = APIGatewayProxyEvent ( event ) logger . set_correlation_id ( event . request_context . request_id ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:9\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"correlation_id\" : \"correlation_id_value\" , \"message\" : \"Collecting payment\" } Removing additional keys \u00b6 You can remove any additional key from Logger state using remove_keys . collect.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . append_keys ( sample_key = \"value\" ) logger . info ( \"Collecting payment\" ) logger . remove_keys ([ \"sample_key\" ]) logger . info ( \"Collecting payment without sample key\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"sample_key\" : \"value\" }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment without sample key\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" } Clearing all state \u00b6 Logger is commonly initialized in the global scope. Due to Lambda Execution Context reuse , this means that custom keys can be persisted across invocations. If you want all custom keys to be deleted, you can use clear_state=True param in inject_lambda_context decorator. Tip: When is this useful? It is useful when you add multiple custom keys conditionally, instead of setting a default None value if not present. Any key with None value is automatically removed by Logger. Danger: This can have unintended side effects if you use Layers Lambda Layers code is imported before the Lambda handler. This means that clear_state=True will instruct Logger to remove any keys previously added before Lambda handler execution proceeds. You can either avoid running any code as part of Lambda Layers global scope, or override keys with their latest value as part of handler's execution. collect.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( clear_state = True ) def handler ( event , context ): if event . get ( \"special_key\" ): # Should only be available in the first request log # as the second request doesn't contain `special_key` logger . append_keys ( debugging_key = \"value\" ) logger . info ( \"Collecting payment\" ) #1 request 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"special_key\" : \"debug_key\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } #2 request 1 2 3 4 5 6 7 8 9 10 11 12 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : false , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } Logging exceptions \u00b6 Use logger.exception method to log contextual information about exceptions. Logger will include exception_name and exception keys to aid troubleshooting and error enumeration. Tip You can use your preferred Log Analytics tool to enumerate and visualize exceptions across all your services using exception_name key. collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) try : raise ValueError ( \"something went wrong\" ) except Exception : logger . exception ( \"Received an exception\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"level\" : \"ERROR\" , \"location\" : \"collect.handler:5\" , \"message\" : \"Received an exception\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"exception_name\" : \"ValueError\" , \"exception\" : \"Traceback (most recent call last):\\n File \\\"<input>\\\", line 2, in <module>\\nValueError: something went wrong\" } Advanced \u00b6 Built-in Correlation ID expressions \u00b6 You can use any of the following built-in JMESPath expressions as part of inject_lambda_context decorator . Note: Any object key named with - must be escaped For example, request.headers.\"x-amzn-trace-id\" . Name Expression Description API_GATEWAY_REST \"requestContext.requestId\" API Gateway REST API request ID API_GATEWAY_HTTP \"requestContext.requestId\" API Gateway HTTP API request ID APPSYNC_RESOLVER 'request.headers.\"x-amzn-trace-id\"' AppSync X-Ray Trace ID APPLICATION_LOAD_BALANCER 'headers.\"x-amzn-trace-id\"' ALB X-Ray Trace ID EVENT_BRIDGE \"id\" EventBridge Event ID Reusing Logger across your code \u00b6 Logger supports inheritance via child parameter. This allows you to create multiple Loggers across your code base, and propagate changes such as new keys to all Loggers. collect.py 1 2 3 4 5 6 7 8 import shared # Creates a child logger named \"payment.shared\" from aws_lambda_powertools import Logger logger = Logger () # POWERTOOLS_SERVICE_NAME: \"payment\" def handler ( event , context ): shared . inject_payment_id ( event ) ... shared.py 1 2 3 4 5 6 from aws_lambda_powertools import Logger logger = Logger ( child = True ) # POWERTOOLS_SERVICE_NAME: \"payment\" def inject_payment_id ( event ): logger . structure_logs ( append = True , payment_id = event . get ( \"payment_id\" )) In this example, Logger will create a parent logger named payment and a child logger named payment.shared . Changes in either parent or child logger will be propagated bi-directionally. Info: Child loggers will be named after the following convention {service}.{filename} If you forget to use child param but the service name is the same of the parent, we will return the existing parent Logger instead. Sampling debug logs \u00b6 Use sampling when you want to dynamically change your log level to DEBUG based on a percentage of your concurrent/cold start invocations . You can use values ranging from 0.0 to 1 (100%) when setting POWERTOOLS_LOGGER_SAMPLE_RATE env var or sample_rate parameter in Logger. Tip: When is this useful? Let's imagine a sudden spike increase in concurrency triggered a transient issue downstream. When looking into the logs you might not have enough information, and while you can adjust log levels it might not happen again. This feature takes into account transient issues where additional debugging information can be useful. Sampling decision happens at the Logger initialization. This means sampling may happen significantly more or less than depending on your traffic patterns, for example a steady low number of invocations and thus few cold starts. Note Open a feature request if you want Logger to calculate sampling for every invocation collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger # Sample 10% of debug logs e.g. 0.1 logger = Logger ( service = \"payment\" , sample_rate = 0.1 ) def handler ( event , context ): logger . debug ( \"Verifying whether order_id is present\" ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \"level\" : \"DEBUG\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Verifying whether order_id is present\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"sampling_rate\" : 0.1 }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"sampling_rate\" : 0.1 } LambdaPowertoolsFormatter \u00b6 Logger propagates a few formatting configurations to the built-in LambdaPowertoolsFormatter logging formatter. If you prefer configuring it separately, or you'd want to bring this JSON Formatter to another application, these are the supported settings: Parameter Description Default json_serializer function to serialize obj to a JSON formatted str json.dumps json_deserializer function to deserialize str , bytes , bytearray containing a JSON document to a Python obj json.loads json_default function to coerce unserializable values, when no custom serializer/deserializer is set str datefmt string directives (strftime) to format log timestamp %Y-%m-%d %H:%M:%S,%F%z , where %F is a custom ms directive use_datetime_directive format the datefmt timestamps using datetime , not time (also supports the custom %F directive for milliseconds) False utc set logging timestamp to UTC False log_record_order set order of log keys when logging [\"level\", \"location\", \"message\", \"timestamp\"] kwargs key-value to be included in log messages None Pre-configuring Lambda Powertools Formatter 1 2 3 4 5 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import LambdaPowertoolsFormatter formatter = LambdaPowertoolsFormatter ( utc = True , log_record_order = [ \"message\" ]) logger = Logger ( service = \"example\" , logger_formatter = formatter ) Migrating from other Loggers \u00b6 If you're migrating from other Loggers, there are few key points to be aware of: Service parameter , Inheriting Loggers , Overriding Log records , and Logging exceptions . The service parameter \u00b6 Service is what defines the Logger name, including what the Lambda function is responsible for, or part of (e.g payment service). For Logger, the service is the logging key customers can use to search log operations for one or more functions - For example, search for all errors, or messages like X, where service is payment . Inheriting Loggers \u00b6 Python Logging hierarchy happens via the dot notation: service , service.child , service.child_2 For inheritance, Logger uses a child=True parameter along with service being the same value across Loggers. For child Loggers, we introspect the name of your module where Logger(child=True, service=\"name\") is called, and we name your Logger as {service}.{filename} . Danger A common issue when migrating from other Loggers is that service might be defined in the parent Logger (no child param), and not defined in the child Logger: incorrect_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( child = True ) correct_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" , child = True ) In this case, Logger will register a Logger named payment , and a Logger named service_undefined . The latter isn't inheriting from the parent, and will have no handler, resulting in no message being logged to standard output. Tip This can be fixed by either ensuring both has the service value as payment , or simply use the environment variable POWERTOOLS_SERVICE_NAME to ensure service value will be the same across all Loggers when not explicitly set. Overriding Log records \u00b6 Tip Use datefmt for custom date formats - We honour standard logging library string formats . Prefer using datetime string formats ? Set use_datetime_directive at Logger constructor or at Lambda Powertools Formatter . You might want to continue to use the same date formatting style, or override location to display the package.function_name:line_number as you previously had. Logger allows you to either change the format or suppress the following keys altogether at the initialization: location , timestamp , level , xray_trace_id . lambda_handler.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger date_format = \"%m/ %d /%Y %I:%M:%S %p\" location_format = \"[ %(funcName)s ] %(module)s \" # override location and timestamp format logger = Logger ( service = \"payment\" , location = location_format , datefmt = date_format ) # suppress the location key with a None value logger_two = Logger ( service = \"payment\" , location = None ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 { \"level\" : \"INFO\" , \"location\" : \"[<module>] lambda_handler\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"02/09/2021 09:25:17 AM\" , \"service\" : \"payment\" } Reordering log keys position \u00b6 You can change the order of standard Logger keys or any keys that will be appended later at runtime via the log_record_order parameter. lambda_handler.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger # make message as the first key logger = Logger ( service = \"payment\" , log_record_order = [ \"message\" ]) # make request_id that will be added later as the first key # Logger(service=\"payment\", log_record_order=[\"request_id\"]) # Default key sorting order when omit # Logger(service=\"payment\", log_record_order=[\"level\",\"location\",\"message\",\"timestamp\"]) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"message\" : \"hello world\" , \"level\" : \"INFO\" , \"location\" : \"[<module>]:6\" , \"timestamp\" : \"2021-02-09 09:36:12,280\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 } Setting timestamp to UTC \u00b6 By default, this Logger and standard logging library emits records using local time timestamp. You can override this behaviour via utc parameter: Setting UTC timestamp by default 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) logger . info ( \"Local time\" ) logger_in_utc = Logger ( service = \"payment\" , utc = True ) logger_in_utc . info ( \"GMT time zone\" ) Custom function for unserializable values \u00b6 By default, Logger uses str to handle values non-serializable by JSON. You can override this behaviour via json_default parameter by passing a Callable: collect.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger def custom_json_default ( value ): return f \"<non-serializable: { type ( value ) . __name__ } >\" class Unserializable : pass logger = Logger ( service = \"payment\" , json_default = custom_json_default ) def handler ( event , context ): logger . info ( Unserializable ()) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:8\" , \"message\" : \"\" < n o n - serializable : U nser ializable> \"\" , \"timestamp\" : \"2021-05-03 15:17:23,632+0200\" , \"service\" : \"payment\" } Bring your own handler \u00b6 By default, Logger uses StreamHandler and logs to standard output. You can override this behaviour via logger_handler parameter: Configure Logger to output to a file 1 2 3 4 5 6 7 8 9 10 import logging from pathlib import Path from aws_lambda_powertools import Logger log_file = Path ( \"/tmp/log.json\" ) log_file_handler = logging . FileHandler ( filename = log_file ) logger = Logger ( service = \"payment\" , logger_handler = log_file_handler ) logger . info ( \"Collecting payment\" ) Bring your own formatter \u00b6 By default, Logger uses LambdaPowertoolsFormatter that persists its custom structure between non-cold start invocations. There could be scenarios where the existing feature set isn't sufficient to your formatting needs. For minor changes like remapping keys after all log record processing has completed, you can override serialize method from LambdaPowertoolsFormatter : custom_formatter.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import LambdaPowertoolsFormatter from typing import Dict class CustomFormatter ( LambdaPowertoolsFormatter ): def serialize ( self , log : Dict ) -> str : \"\"\"Serialize final structured log dict to JSON str\"\"\" log [ \"event\" ] = log . pop ( \"message\" ) # rename message key to event return self . json_serializer ( log ) # use configured json serializer logger = Logger ( service = \"example\" , logger_formatter = CustomFormatter ()) logger . info ( \"hello\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 { \"level\" : \"INFO\" , \"location\" : \"<module>:16\" , \"timestamp\" : \"2021-12-30 13:41:53,413+0100\" , \"event\" : \"hello\" } For replacing the formatter entirely , you can subclass BasePowertoolsFormatter , implement append_keys method, and override format standard logging method. This ensures the current feature set of Logger like injecting Lambda context and sampling will continue to work. Info You might need to implement remove_keys method if you make use of the feature too. collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import BasePowertoolsFormatter class CustomFormatter ( BasePowertoolsFormatter ): custom_format = {} # arbitrary dict to hold our structured keys def append_keys ( self , ** additional_keys ): # also used by `inject_lambda_context` decorator self . custom_format . update ( additional_keys ) # Optional unless you make use of this Logger feature def remove_keys ( self , keys : Iterable [ str ]): for key in keys : self . custom_format . pop ( key , None ) def format ( self , record : logging . LogRecord ) -> str : # noqa: A003 \"\"\"Format logging record as structured JSON str\"\"\" return json . dumps ( { \"event\" : super () . format ( record ), \"timestamp\" : self . formatTime ( record ), \"my_default_key\" : \"test\" , ** self . custom_format , } ) logger = Logger ( service = \"payment\" , logger_formatter = CustomFormatter ()) @logger . inject_lambda_context def handler ( event , context ): logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 { \"event\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494\" , \"my_default_key\" : \"test\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } Bring your own JSON serializer \u00b6 By default, Logger uses json.dumps and json.loads as serializer and deserializer respectively. There could be scenarios where you are making use of alternative JSON libraries like orjson . As parameters don't always translate well between them, you can pass any callable that receives a Dict and return a str : Using Rust orjson library as serializer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import orjson from aws_lambda_powertools import Logger custom_serializer = orjson . dumps custom_deserializer = orjson . loads logger = Logger ( service = \"payment\" , json_serializer = custom_serializer , json_deserializer = custom_deserializer ) # when using parameters, you can pass a partial # custom_serializer=functools.partial(orjson.dumps, option=orjson.OPT_SERIALIZE_NUMPY) Testing your code \u00b6 Inject Lambda Context \u00b6 When unit testing your code that makes use of inject_lambda_context decorator, you need to pass a dummy Lambda Context, or else Logger will fail. This is a Pytest sample that provides the minimum information necessary for Logger to succeed: fake_lambda_context_for_logger.py Note that dataclasses are available in Python 3.7+ only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from dataclasses import dataclass import pytest @pytest . fixture def lambda_context (): @dataclass class LambdaContext : function_name : str = \"test\" memory_limit_in_mb : int = 128 invoked_function_arn : str = \"arn:aws:lambda:eu-west-1:809313241:function:test\" aws_request_id : str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\" return LambdaContext () def test_lambda_handler ( lambda_context ): test_event = { 'test' : 'event' } your_lambda_handler ( test_event , lambda_context ) # this will now have a Context object populated fake_lambda_context_for_logger_py36.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from collections import namedtuple import pytest @pytest . fixture def lambda_context (): lambda_context = { \"function_name\" : \"test\" , \"memory_limit_in_mb\" : 128 , \"invoked_function_arn\" : \"arn:aws:lambda:eu-west-1:809313241:function:test\" , \"aws_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , } return namedtuple ( \"LambdaContext\" , lambda_context . keys ())( * lambda_context . values ()) def test_lambda_handler ( lambda_context ): test_event = { 'test' : 'event' } # this will now have a Context object populated your_lambda_handler ( test_event , lambda_context ) Tip Check out the built-in Pytest caplog fixture to assert plain log messages Pytest live log feature \u00b6 Pytest Live Log feature duplicates emitted log messages in order to style log statements according to their levels, for this to work use POWERTOOLS_LOG_DEDUPLICATION_DISABLED env var. Disabling log deduplication to use Pytest live log 1 POWERTOOLS_LOG_DEDUPLICATION_DISABLED = \"1\" pytest -o log_cli = 1 Warning This feature should be used with care, as it explicitly disables our ability to filter propagated messages to the root logger (if configured). FAQ \u00b6 How can I enable boto3 and botocore library logging? You can enable the botocore and boto3 logs by using the set_stream_logger method, this method will add a stream handler for the given name and level to the logging module. By default, this logs all boto3 messages to stdout. Enabling AWS SDK logging 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from typing import Dict , List from aws_lambda_powertools.utilities.typing import LambdaContext from aws_lambda_powertools import Logger import boto3 boto3 . set_stream_logger () boto3 . set_stream_logger ( 'botocore' ) logger = Logger () client = boto3 . client ( 's3' ) def handler ( event : Dict , context : LambdaContext ) -> List : response = client . list_buckets () return response . get ( \"Buckets\" , []) How can I enable powertools logging for imported libraries? You can copy the Logger setup to all or sub-sets of registered external loggers. Use the copy_config_to_registered_logger method to do this. By default all registered loggers will be modified. You can change this behaviour by providing include and exclude attributes. You can also provide optional log_level attribute external loggers will be configured with. Cloning Logger config to all other registered standard loggers 1 2 3 4 5 6 7 8 9 10 11 import logging from aws_lambda_powertools import Logger from aws_lambda_powertools.logging import utils logger = Logger () external_logger = logging . logger () utils . copy_config_to_registered_loggers ( source_logger = logger ) external_logger . info ( \"test message\" ) What's the difference between append_keys and extra ? Keys added with append_keys will persist across multiple log messages while keys added via extra will only be available in a given log message operation. Here's an example where we persist payment_id not request_id . Note that payment_id remains in both log messages while booking_id is only available in the first message. lambda_handler.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . append_keys ( payment_id = \"123456789\" ) try : booking_id = book_flight () logger . info ( \"Flight booked successfully\" , extra = { \"booking_id\" : booking_id }) except BookingReservationError : ... logger . info ( \"goodbye\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"level\" : \"INFO\" , \"location\" : \"<module>:10\" , \"message\" : \"Flight booked successfully\" , \"timestamp\" : \"2021-01-12 14:09:10,859\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" , \"booking_id\" : \"75edbad0-0857-4fc9-b547-6180e2f7959b\" }, { \"level\" : \"INFO\" , \"location\" : \"<module>:14\" , \"message\" : \"goodbye\" , \"timestamp\" : \"2021-01-12 14:09:10,860\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" } How do I aggregate and search Powertools logs across accounts? As of now, ElasticSearch (ELK) or 3rd party solutions are best suited to this task. Please refer to this discussion for more details","title":"Logger"},{"location":"core/logger/#key-features","text":"Capture key fields from Lambda context, cold start and structures logging output as JSON Log Lambda event when instructed (disabled by default) Log sampling enables DEBUG log level for a percentage of requests (disabled by default) Append additional keys to structured log at any point in time","title":"Key features"},{"location":"core/logger/#getting-started","text":"Logger requires two settings: Setting Description Environment variable Constructor parameter Logging level Sets how verbose Logger should be (INFO, by default) LOG_LEVEL level Service Sets service key that will be present across all log statements POWERTOOLS_SERVICE_NAME service Example AWS Serverless Application Model (SAM) template.yaml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_SERVICE_NAME : example app.py 1 2 3 from aws_lambda_powertools import Logger logger = Logger () # Sets service via env var # OR logger = Logger(service=\"example\")","title":"Getting started"},{"location":"core/logger/#standard-structured-keys","text":"Your Logger will include the following keys to your structured logging: Key Example Note level : str INFO Logging level location : str collect.handler:1 Source code location where statement was executed message : Any Collecting payment Unserializable JSON values are casted as str timestamp : str 2021-05-03 10:20:19,650+0200 Timestamp with milliseconds, by default uses local timezone service : str payment Service name defined, by default service_undefined xray_trace_id : str 1-5759e988-bd862e3fe1be46a994272793 When tracing is enabled , it shows X-Ray Trace ID sampling_rate : float 0.1 When enabled, it shows sampling rate in percentage e.g. 10% exception_name : str ValueError When logger.exception is used and there is an exception exception : str Traceback (most recent call last).. When logger.exception is used and there is an exception","title":"Standard structured keys"},{"location":"core/logger/#capturing-lambda-context-info","text":"You can enrich your structured logs with key Lambda context information via inject_lambda_context . collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context def handler ( event , context ): logger . info ( \"Collecting payment\" ) # You can log entire objects too logger . info ({ \"operation\" : \"collect_payment\" , \"charge_id\" : event [ 'charge_id' ] }) ... Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : { \"operation\" : \"collect_payment\" , \"charge_id\" : \"ch_AZFlk2345C0\" }, \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } When used, this will include the following keys: Key Example cold_start : bool false function_name str example-powertools-HelloWorldFunction-1P1Z6B39FLU73 function_memory_size : int 128 function_arn : str arn:aws:lambda:eu-west-1:012345678910:function:example-powertools-HelloWorldFunction-1P1Z6B39FLU73 function_request_id : str 899856cb-83d1-40d7-8611-9e78f15f32f4","title":"Capturing Lambda context info"},{"location":"core/logger/#logging-incoming-event","text":"When debugging in non-production environments, you can instruct Logger to log the incoming event with log_event param or via POWERTOOLS_LOGGER_LOG_EVENT env var. Warning This is disabled by default to prevent sensitive info being logged Logging incoming event 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( log_event = True ) def handler ( event , context ): ...","title":"Logging incoming event"},{"location":"core/logger/#setting-a-correlation-id","text":"You can set a Correlation ID using correlation_id_path param by passing a JMESPath expression . Tip You can retrieve correlation IDs via get_correlation_id method collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( correlation_id_path = \"headers.my_request_id_header\" ) def handler ( event , context ): logger . debug ( f \"Correlation ID => { logger . get_correlation_id () } \" ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"headers\" : { \"my_request_id_header\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"correlation_id\" : \"correlation_id_value\" } We provide built-in JMESPath expressions for known event sources, where either a request ID or X-Ray Trace ID are present. collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging import correlation_paths logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) def handler ( event , context ): logger . debug ( f \"Correlation ID => { logger . get_correlation_id () } \" ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:8\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"correlation_id\" : \"correlation_id_value\" }","title":"Setting a Correlation ID"},{"location":"core/logger/#appending-additional-keys","text":"Info: Custom keys are persisted across warm invocations Always set additional keys as part of your handler to ensure they have the latest value, or explicitly clear them with clear_state=True . You can append additional keys using either mechanism: Persist new keys across all future log messages via append_keys method Add additional keys on a per log message basis via extra parameter","title":"Appending additional keys"},{"location":"core/logger/#append_keys-method","text":"Note append_keys replaces structure_logs(append=True, **kwargs) method. structure_logs will be removed in v2. You can append your own keys to your existing Logger via append_keys(**additional_key_values) method. collect.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): order_id = event . get ( \"order_id\" ) # this will ensure order_id key always has the latest value before logging logger . append_keys ( order_id = order_id ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:11\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"order_id\" : \"order_id_value\" } Tip: Logger will automatically reject any key with a None value If you conditionally add keys depending on the payload, you can follow the example above. This example will add order_id if its value is not empty, and in subsequent invocations where order_id might not be present it'll remove it from the Logger.","title":"append_keys method"},{"location":"core/logger/#extra-parameter","text":"Extra parameter is available for all log levels' methods, as implemented in the standard logging library - e.g. logger.info, logger.warning . It accepts any dictionary, and all keyword arguments will be added as part of the root structure of the logs for that log statement. Info Any keyword argument added using extra will not be persisted for subsequent messages. extra_parameter.py 1 2 3 4 5 6 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) fields = { \"request_id\" : \"1123\" } logger . info ( \"Collecting payment\" , extra = fields ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:6\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"request_id\" : \"1123\" }","title":"extra parameter"},{"location":"core/logger/#set_correlation_id-method","text":"You can set a correlation_id to your existing Logger via set_correlation_id(value) method by passing any string value. collect.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . set_correlation_id ( event [ \"requestContext\" ][ \"requestId\" ]) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"correlation_id\" : \"correlation_id_value\" } Alternatively, you can combine Data Classes utility with Logger to use dot notation object: collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent logger = Logger ( service = \"payment\" ) def handler ( event , context ): event = APIGatewayProxyEvent ( event ) logger . set_correlation_id ( event . request_context . request_id ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:9\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"correlation_id\" : \"correlation_id_value\" , \"message\" : \"Collecting payment\" }","title":"set_correlation_id method"},{"location":"core/logger/#removing-additional-keys","text":"You can remove any additional key from Logger state using remove_keys . collect.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . append_keys ( sample_key = \"value\" ) logger . info ( \"Collecting payment\" ) logger . remove_keys ([ \"sample_key\" ]) logger . info ( \"Collecting payment without sample key\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"sample_key\" : \"value\" }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment without sample key\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" }","title":"Removing additional keys"},{"location":"core/logger/#clearing-all-state","text":"Logger is commonly initialized in the global scope. Due to Lambda Execution Context reuse , this means that custom keys can be persisted across invocations. If you want all custom keys to be deleted, you can use clear_state=True param in inject_lambda_context decorator. Tip: When is this useful? It is useful when you add multiple custom keys conditionally, instead of setting a default None value if not present. Any key with None value is automatically removed by Logger. Danger: This can have unintended side effects if you use Layers Lambda Layers code is imported before the Lambda handler. This means that clear_state=True will instruct Logger to remove any keys previously added before Lambda handler execution proceeds. You can either avoid running any code as part of Lambda Layers global scope, or override keys with their latest value as part of handler's execution. collect.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( clear_state = True ) def handler ( event , context ): if event . get ( \"special_key\" ): # Should only be available in the first request log # as the second request doesn't contain `special_key` logger . append_keys ( debugging_key = \"value\" ) logger . info ( \"Collecting payment\" ) #1 request 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"special_key\" : \"debug_key\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } #2 request 1 2 3 4 5 6 7 8 9 10 11 12 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : false , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" }","title":"Clearing all state"},{"location":"core/logger/#logging-exceptions","text":"Use logger.exception method to log contextual information about exceptions. Logger will include exception_name and exception keys to aid troubleshooting and error enumeration. Tip You can use your preferred Log Analytics tool to enumerate and visualize exceptions across all your services using exception_name key. collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) try : raise ValueError ( \"something went wrong\" ) except Exception : logger . exception ( \"Received an exception\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"level\" : \"ERROR\" , \"location\" : \"collect.handler:5\" , \"message\" : \"Received an exception\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"exception_name\" : \"ValueError\" , \"exception\" : \"Traceback (most recent call last):\\n File \\\"<input>\\\", line 2, in <module>\\nValueError: something went wrong\" }","title":"Logging exceptions"},{"location":"core/logger/#advanced","text":"","title":"Advanced"},{"location":"core/logger/#built-in-correlation-id-expressions","text":"You can use any of the following built-in JMESPath expressions as part of inject_lambda_context decorator . Note: Any object key named with - must be escaped For example, request.headers.\"x-amzn-trace-id\" . Name Expression Description API_GATEWAY_REST \"requestContext.requestId\" API Gateway REST API request ID API_GATEWAY_HTTP \"requestContext.requestId\" API Gateway HTTP API request ID APPSYNC_RESOLVER 'request.headers.\"x-amzn-trace-id\"' AppSync X-Ray Trace ID APPLICATION_LOAD_BALANCER 'headers.\"x-amzn-trace-id\"' ALB X-Ray Trace ID EVENT_BRIDGE \"id\" EventBridge Event ID","title":"Built-in Correlation ID expressions"},{"location":"core/logger/#reusing-logger-across-your-code","text":"Logger supports inheritance via child parameter. This allows you to create multiple Loggers across your code base, and propagate changes such as new keys to all Loggers. collect.py 1 2 3 4 5 6 7 8 import shared # Creates a child logger named \"payment.shared\" from aws_lambda_powertools import Logger logger = Logger () # POWERTOOLS_SERVICE_NAME: \"payment\" def handler ( event , context ): shared . inject_payment_id ( event ) ... shared.py 1 2 3 4 5 6 from aws_lambda_powertools import Logger logger = Logger ( child = True ) # POWERTOOLS_SERVICE_NAME: \"payment\" def inject_payment_id ( event ): logger . structure_logs ( append = True , payment_id = event . get ( \"payment_id\" )) In this example, Logger will create a parent logger named payment and a child logger named payment.shared . Changes in either parent or child logger will be propagated bi-directionally. Info: Child loggers will be named after the following convention {service}.{filename} If you forget to use child param but the service name is the same of the parent, we will return the existing parent Logger instead.","title":"Reusing Logger across your code"},{"location":"core/logger/#sampling-debug-logs","text":"Use sampling when you want to dynamically change your log level to DEBUG based on a percentage of your concurrent/cold start invocations . You can use values ranging from 0.0 to 1 (100%) when setting POWERTOOLS_LOGGER_SAMPLE_RATE env var or sample_rate parameter in Logger. Tip: When is this useful? Let's imagine a sudden spike increase in concurrency triggered a transient issue downstream. When looking into the logs you might not have enough information, and while you can adjust log levels it might not happen again. This feature takes into account transient issues where additional debugging information can be useful. Sampling decision happens at the Logger initialization. This means sampling may happen significantly more or less than depending on your traffic patterns, for example a steady low number of invocations and thus few cold starts. Note Open a feature request if you want Logger to calculate sampling for every invocation collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger # Sample 10% of debug logs e.g. 0.1 logger = Logger ( service = \"payment\" , sample_rate = 0.1 ) def handler ( event , context ): logger . debug ( \"Verifying whether order_id is present\" ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \"level\" : \"DEBUG\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Verifying whether order_id is present\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"sampling_rate\" : 0.1 }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"sampling_rate\" : 0.1 }","title":"Sampling debug logs"},{"location":"core/logger/#lambdapowertoolsformatter","text":"Logger propagates a few formatting configurations to the built-in LambdaPowertoolsFormatter logging formatter. If you prefer configuring it separately, or you'd want to bring this JSON Formatter to another application, these are the supported settings: Parameter Description Default json_serializer function to serialize obj to a JSON formatted str json.dumps json_deserializer function to deserialize str , bytes , bytearray containing a JSON document to a Python obj json.loads json_default function to coerce unserializable values, when no custom serializer/deserializer is set str datefmt string directives (strftime) to format log timestamp %Y-%m-%d %H:%M:%S,%F%z , where %F is a custom ms directive use_datetime_directive format the datefmt timestamps using datetime , not time (also supports the custom %F directive for milliseconds) False utc set logging timestamp to UTC False log_record_order set order of log keys when logging [\"level\", \"location\", \"message\", \"timestamp\"] kwargs key-value to be included in log messages None Pre-configuring Lambda Powertools Formatter 1 2 3 4 5 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import LambdaPowertoolsFormatter formatter = LambdaPowertoolsFormatter ( utc = True , log_record_order = [ \"message\" ]) logger = Logger ( service = \"example\" , logger_formatter = formatter )","title":"LambdaPowertoolsFormatter"},{"location":"core/logger/#migrating-from-other-loggers","text":"If you're migrating from other Loggers, there are few key points to be aware of: Service parameter , Inheriting Loggers , Overriding Log records , and Logging exceptions .","title":"Migrating from other Loggers"},{"location":"core/logger/#the-service-parameter","text":"Service is what defines the Logger name, including what the Lambda function is responsible for, or part of (e.g payment service). For Logger, the service is the logging key customers can use to search log operations for one or more functions - For example, search for all errors, or messages like X, where service is payment .","title":"The service parameter"},{"location":"core/logger/#inheriting-loggers","text":"Python Logging hierarchy happens via the dot notation: service , service.child , service.child_2 For inheritance, Logger uses a child=True parameter along with service being the same value across Loggers. For child Loggers, we introspect the name of your module where Logger(child=True, service=\"name\") is called, and we name your Logger as {service}.{filename} . Danger A common issue when migrating from other Loggers is that service might be defined in the parent Logger (no child param), and not defined in the child Logger: incorrect_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( child = True ) correct_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" , child = True ) In this case, Logger will register a Logger named payment , and a Logger named service_undefined . The latter isn't inheriting from the parent, and will have no handler, resulting in no message being logged to standard output. Tip This can be fixed by either ensuring both has the service value as payment , or simply use the environment variable POWERTOOLS_SERVICE_NAME to ensure service value will be the same across all Loggers when not explicitly set.","title":"Inheriting Loggers"},{"location":"core/logger/#overriding-log-records","text":"Tip Use datefmt for custom date formats - We honour standard logging library string formats . Prefer using datetime string formats ? Set use_datetime_directive at Logger constructor or at Lambda Powertools Formatter . You might want to continue to use the same date formatting style, or override location to display the package.function_name:line_number as you previously had. Logger allows you to either change the format or suppress the following keys altogether at the initialization: location , timestamp , level , xray_trace_id . lambda_handler.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger date_format = \"%m/ %d /%Y %I:%M:%S %p\" location_format = \"[ %(funcName)s ] %(module)s \" # override location and timestamp format logger = Logger ( service = \"payment\" , location = location_format , datefmt = date_format ) # suppress the location key with a None value logger_two = Logger ( service = \"payment\" , location = None ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 { \"level\" : \"INFO\" , \"location\" : \"[<module>] lambda_handler\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"02/09/2021 09:25:17 AM\" , \"service\" : \"payment\" }","title":"Overriding Log records"},{"location":"core/logger/#reordering-log-keys-position","text":"You can change the order of standard Logger keys or any keys that will be appended later at runtime via the log_record_order parameter. lambda_handler.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger # make message as the first key logger = Logger ( service = \"payment\" , log_record_order = [ \"message\" ]) # make request_id that will be added later as the first key # Logger(service=\"payment\", log_record_order=[\"request_id\"]) # Default key sorting order when omit # Logger(service=\"payment\", log_record_order=[\"level\",\"location\",\"message\",\"timestamp\"]) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"message\" : \"hello world\" , \"level\" : \"INFO\" , \"location\" : \"[<module>]:6\" , \"timestamp\" : \"2021-02-09 09:36:12,280\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 }","title":"Reordering log keys position"},{"location":"core/logger/#setting-timestamp-to-utc","text":"By default, this Logger and standard logging library emits records using local time timestamp. You can override this behaviour via utc parameter: Setting UTC timestamp by default 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) logger . info ( \"Local time\" ) logger_in_utc = Logger ( service = \"payment\" , utc = True ) logger_in_utc . info ( \"GMT time zone\" )","title":"Setting timestamp to UTC"},{"location":"core/logger/#custom-function-for-unserializable-values","text":"By default, Logger uses str to handle values non-serializable by JSON. You can override this behaviour via json_default parameter by passing a Callable: collect.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger def custom_json_default ( value ): return f \"<non-serializable: { type ( value ) . __name__ } >\" class Unserializable : pass logger = Logger ( service = \"payment\" , json_default = custom_json_default ) def handler ( event , context ): logger . info ( Unserializable ()) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:8\" , \"message\" : \"\" < n o n - serializable : U nser ializable> \"\" , \"timestamp\" : \"2021-05-03 15:17:23,632+0200\" , \"service\" : \"payment\" }","title":"Custom function for unserializable values"},{"location":"core/logger/#bring-your-own-handler","text":"By default, Logger uses StreamHandler and logs to standard output. You can override this behaviour via logger_handler parameter: Configure Logger to output to a file 1 2 3 4 5 6 7 8 9 10 import logging from pathlib import Path from aws_lambda_powertools import Logger log_file = Path ( \"/tmp/log.json\" ) log_file_handler = logging . FileHandler ( filename = log_file ) logger = Logger ( service = \"payment\" , logger_handler = log_file_handler ) logger . info ( \"Collecting payment\" )","title":"Bring your own handler"},{"location":"core/logger/#bring-your-own-formatter","text":"By default, Logger uses LambdaPowertoolsFormatter that persists its custom structure between non-cold start invocations. There could be scenarios where the existing feature set isn't sufficient to your formatting needs. For minor changes like remapping keys after all log record processing has completed, you can override serialize method from LambdaPowertoolsFormatter : custom_formatter.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import LambdaPowertoolsFormatter from typing import Dict class CustomFormatter ( LambdaPowertoolsFormatter ): def serialize ( self , log : Dict ) -> str : \"\"\"Serialize final structured log dict to JSON str\"\"\" log [ \"event\" ] = log . pop ( \"message\" ) # rename message key to event return self . json_serializer ( log ) # use configured json serializer logger = Logger ( service = \"example\" , logger_formatter = CustomFormatter ()) logger . info ( \"hello\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 { \"level\" : \"INFO\" , \"location\" : \"<module>:16\" , \"timestamp\" : \"2021-12-30 13:41:53,413+0100\" , \"event\" : \"hello\" } For replacing the formatter entirely , you can subclass BasePowertoolsFormatter , implement append_keys method, and override format standard logging method. This ensures the current feature set of Logger like injecting Lambda context and sampling will continue to work. Info You might need to implement remove_keys method if you make use of the feature too. collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import BasePowertoolsFormatter class CustomFormatter ( BasePowertoolsFormatter ): custom_format = {} # arbitrary dict to hold our structured keys def append_keys ( self , ** additional_keys ): # also used by `inject_lambda_context` decorator self . custom_format . update ( additional_keys ) # Optional unless you make use of this Logger feature def remove_keys ( self , keys : Iterable [ str ]): for key in keys : self . custom_format . pop ( key , None ) def format ( self , record : logging . LogRecord ) -> str : # noqa: A003 \"\"\"Format logging record as structured JSON str\"\"\" return json . dumps ( { \"event\" : super () . format ( record ), \"timestamp\" : self . formatTime ( record ), \"my_default_key\" : \"test\" , ** self . custom_format , } ) logger = Logger ( service = \"payment\" , logger_formatter = CustomFormatter ()) @logger . inject_lambda_context def handler ( event , context ): logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 { \"event\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494\" , \"my_default_key\" : \"test\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" }","title":"Bring your own formatter"},{"location":"core/logger/#bring-your-own-json-serializer","text":"By default, Logger uses json.dumps and json.loads as serializer and deserializer respectively. There could be scenarios where you are making use of alternative JSON libraries like orjson . As parameters don't always translate well between them, you can pass any callable that receives a Dict and return a str : Using Rust orjson library as serializer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import orjson from aws_lambda_powertools import Logger custom_serializer = orjson . dumps custom_deserializer = orjson . loads logger = Logger ( service = \"payment\" , json_serializer = custom_serializer , json_deserializer = custom_deserializer ) # when using parameters, you can pass a partial # custom_serializer=functools.partial(orjson.dumps, option=orjson.OPT_SERIALIZE_NUMPY)","title":"Bring your own JSON serializer"},{"location":"core/logger/#testing-your-code","text":"","title":"Testing your code"},{"location":"core/logger/#inject-lambda-context","text":"When unit testing your code that makes use of inject_lambda_context decorator, you need to pass a dummy Lambda Context, or else Logger will fail. This is a Pytest sample that provides the minimum information necessary for Logger to succeed: fake_lambda_context_for_logger.py Note that dataclasses are available in Python 3.7+ only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from dataclasses import dataclass import pytest @pytest . fixture def lambda_context (): @dataclass class LambdaContext : function_name : str = \"test\" memory_limit_in_mb : int = 128 invoked_function_arn : str = \"arn:aws:lambda:eu-west-1:809313241:function:test\" aws_request_id : str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\" return LambdaContext () def test_lambda_handler ( lambda_context ): test_event = { 'test' : 'event' } your_lambda_handler ( test_event , lambda_context ) # this will now have a Context object populated fake_lambda_context_for_logger_py36.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from collections import namedtuple import pytest @pytest . fixture def lambda_context (): lambda_context = { \"function_name\" : \"test\" , \"memory_limit_in_mb\" : 128 , \"invoked_function_arn\" : \"arn:aws:lambda:eu-west-1:809313241:function:test\" , \"aws_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , } return namedtuple ( \"LambdaContext\" , lambda_context . keys ())( * lambda_context . values ()) def test_lambda_handler ( lambda_context ): test_event = { 'test' : 'event' } # this will now have a Context object populated your_lambda_handler ( test_event , lambda_context ) Tip Check out the built-in Pytest caplog fixture to assert plain log messages","title":"Inject Lambda Context"},{"location":"core/logger/#pytest-live-log-feature","text":"Pytest Live Log feature duplicates emitted log messages in order to style log statements according to their levels, for this to work use POWERTOOLS_LOG_DEDUPLICATION_DISABLED env var. Disabling log deduplication to use Pytest live log 1 POWERTOOLS_LOG_DEDUPLICATION_DISABLED = \"1\" pytest -o log_cli = 1 Warning This feature should be used with care, as it explicitly disables our ability to filter propagated messages to the root logger (if configured).","title":"Pytest live log feature"},{"location":"core/logger/#faq","text":"How can I enable boto3 and botocore library logging? You can enable the botocore and boto3 logs by using the set_stream_logger method, this method will add a stream handler for the given name and level to the logging module. By default, this logs all boto3 messages to stdout. Enabling AWS SDK logging 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from typing import Dict , List from aws_lambda_powertools.utilities.typing import LambdaContext from aws_lambda_powertools import Logger import boto3 boto3 . set_stream_logger () boto3 . set_stream_logger ( 'botocore' ) logger = Logger () client = boto3 . client ( 's3' ) def handler ( event : Dict , context : LambdaContext ) -> List : response = client . list_buckets () return response . get ( \"Buckets\" , []) How can I enable powertools logging for imported libraries? You can copy the Logger setup to all or sub-sets of registered external loggers. Use the copy_config_to_registered_logger method to do this. By default all registered loggers will be modified. You can change this behaviour by providing include and exclude attributes. You can also provide optional log_level attribute external loggers will be configured with. Cloning Logger config to all other registered standard loggers 1 2 3 4 5 6 7 8 9 10 11 import logging from aws_lambda_powertools import Logger from aws_lambda_powertools.logging import utils logger = Logger () external_logger = logging . logger () utils . copy_config_to_registered_loggers ( source_logger = logger ) external_logger . info ( \"test message\" ) What's the difference between append_keys and extra ? Keys added with append_keys will persist across multiple log messages while keys added via extra will only be available in a given log message operation. Here's an example where we persist payment_id not request_id . Note that payment_id remains in both log messages while booking_id is only available in the first message. lambda_handler.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . append_keys ( payment_id = \"123456789\" ) try : booking_id = book_flight () logger . info ( \"Flight booked successfully\" , extra = { \"booking_id\" : booking_id }) except BookingReservationError : ... logger . info ( \"goodbye\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"level\" : \"INFO\" , \"location\" : \"<module>:10\" , \"message\" : \"Flight booked successfully\" , \"timestamp\" : \"2021-01-12 14:09:10,859\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" , \"booking_id\" : \"75edbad0-0857-4fc9-b547-6180e2f7959b\" }, { \"level\" : \"INFO\" , \"location\" : \"<module>:14\" , \"message\" : \"goodbye\" , \"timestamp\" : \"2021-01-12 14:09:10,860\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" } How do I aggregate and search Powertools logs across accounts? As of now, ElasticSearch (ELK) or 3rd party solutions are best suited to this task. Please refer to this discussion for more details","title":"FAQ"},{"location":"core/metrics/","text":"Metrics creates custom metrics asynchronously by logging metrics to standard output following Amazon CloudWatch Embedded Metric Format (EMF) . These metrics can be visualized through Amazon CloudWatch Console . Key features \u00b6 Aggregate up to 100 metrics using a single CloudWatch EMF object (large JSON blob) Validate against common metric definitions mistakes (metric unit, values, max dimensions, max metrics, etc) Metrics are created asynchronously by CloudWatch service, no custom stacks needed Context manager to create a one off metric with a different dimension Terminologies \u00b6 If you're new to Amazon CloudWatch, there are two terminologies you must be aware of before using this utility: Namespace . It's the highest level container that will group multiple metrics from multiple services for a given application, for example ServerlessEcommerce . Dimensions . Metrics metadata in key-value format. They help you slice and dice metrics visualization, for example ColdStart metric by Payment service . Metric terminology, visually explained Getting started \u00b6 Metric has two global settings that will be used across all metrics emitted: Setting Description Environment variable Constructor parameter Metric namespace Logical container where all metrics will be placed e.g. ServerlessAirline POWERTOOLS_METRICS_NAMESPACE namespace Service Optionally, sets service metric dimension across all metrics e.g. payment POWERTOOLS_SERVICE_NAME service Tip Use your application or main service as the metric namespace to easily group all metrics. Example AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Environment : Variables : POWERTOOLS_SERVICE_NAME : payment POWERTOOLS_METRICS_NAMESPACE : ServerlessAirline app.py 1 2 3 4 5 6 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics () # Sets metric namespace and service via env var # OR metrics = Metrics ( namespace = \"ServerlessAirline\" , service = \"orders\" ) # Sets metric namespace, and service as a metric dimension Creating metrics \u00b6 You can create metrics using add_metric , and you can create dimensions for all your aggregate metrics using add_dimension method. Tip You can initialize Metrics in any other module too. It'll keep track of your aggregate metrics in memory to optimize costs (one blob instead of multiples). Metrics 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Metrics with custom dimensions 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Tip: Autocomplete Metric Units MetricUnit enum facilitate finding a supported metric unit by CloudWatch. Alternatively, you can pass the value as a string if you already know them e.g. \"Count\". Note: Metrics overflow CloudWatch EMF supports a max of 100 metrics per batch. Metrics utility will flush all metrics when adding the 100th metric. Subsequent metrics, e.g. 101th, will be aggregated into a new EMF object, for your convenience. Warning: Do not create metrics or dimensions outside the handler Metrics or dimensions added in the global scope will only be added during cold start. Disregard if you that's the intended behaviour. Adding default dimensions \u00b6 You can use either set_default_dimensions method or default_permissions parameter in log_metrics decorator to persist dimensions across Lambda invocations. If you'd like to remove them at some point, you can use clear_default_dimensions method. set_default_dimensions method 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . set_default_dimensions ( environment = \"prod\" , another = \"one\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) with log_metrics decorator 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) DEFAULT_DIMENSIONS = { \"environment\" : \"prod\" , \"another\" : \"one\" } @metrics . log_metrics ( default_dimensions = DEFAULT_DIMENSIONS ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Flushing metrics \u00b6 As you finish adding all your metrics, you need to serialize and flush them to standard output. You can do that automatically with the log_metrics decorator. This decorator also validates , serializes , and flushes all your metrics. During metrics validation, if no metrics are provided then a warning will be logged, but no exception will be raised. app.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"ExampleService\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = MetricUnit . Count , value = 1 ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"BookingConfirmation\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"BookingConfirmation\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"ExampleService\" } Tip: Metric validation If metrics are provided, and any of the following criteria are not met, SchemaValidationError exception will be raised: Maximum of 9 dimensions Namespace is set, and no more than one Metric units must be supported by CloudWatch Raising SchemaValidationError on empty metrics \u00b6 If you want to ensure at least one metric is always emitted, you can pass raise_on_empty_metrics to the log_metrics decorator: Raising SchemaValidationError exception if no metrics are added 1 2 3 4 5 6 7 from aws_lambda_powertools.metrics import Metrics metrics = Metrics () @metrics . log_metrics ( raise_on_empty_metrics = True ) def lambda_handler ( evt , ctx ): ... Suppressing warning messages on empty metrics If you expect your function to execute without publishing metrics every time, you can suppress the warning with warnings.filterwarnings(\"ignore\", \"No metrics to publish*\") . Nesting multiple middlewares \u00b6 When using multiple middlewares, use log_metrics as your last decorator wrapping all subsequent ones to prevent early Metric validations when code hasn't been run yet. Example with multiple decorators 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Metrics , Tracer from aws_lambda_powertools.metrics import MetricUnit tracer = Tracer ( service = \"booking\" ) metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics @tracer . capture_lambda_handler def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = MetricUnit . Count , value = 1 ) Capturing cold start metric \u00b6 You can optionally capture cold start metrics with log_metrics decorator via capture_cold_start_metric param. Generating function cold start metric 1 2 3 4 5 6 7 from aws_lambda_powertools import Metrics metrics = Metrics ( service = \"ExampleService\" ) @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( evt , ctx ): ... If it's a cold start invocation, this feature will: Create a separate EMF blob solely containing a metric named ColdStart Add function_name and service dimensions This has the advantage of keeping cold start metric separate from your application metrics, where you might have unrelated dimensions. Info We do not emit 0 as a value for ColdStart metric for cost reasons. Let us know if you'd prefer a flag to override it. Advanced \u00b6 Adding metadata \u00b6 You can add high-cardinality data as part of your Metrics log with add_metadata method. This is useful when you want to search highly contextual information along with your metrics in your logs. Info This will not be available during metrics visualization - Use dimensions for this purpose app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_metadata ( key = \"booking_id\" , value = \"booking_uuid\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"SuccessfulBooking\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"SuccessfulBooking\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"booking\" , \"booking_id\" : \"booking_uuid\" } Single metric with a different dimension \u00b6 CloudWatch EMF uses the same dimensions across all your metrics. Use single_metric if you have a metric that should have different dimensions. Info Generally, this would be an edge case since you pay for unique metric . Keep the following formula in mind: unique metric = (metric_name + dimension_name + dimension_value) Generating an EMF blob with a single metric 1 2 3 4 5 6 7 8 from aws_lambda_powertools import single_metric from aws_lambda_powertools.metrics import MetricUnit def lambda_handler ( evt , ctx ): with single_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 , namespace = \"ExampleApplication\" ) as metric : metric . add_dimension ( name = \"function_context\" , value = \"$LATEST\" ) ... Flushing metrics manually \u00b6 If you prefer not to use log_metrics because you might want to encapsulate additional logic when doing so, you can manually flush and clear metrics as follows: Warning Metrics, dimensions and namespace validation still applies Manually flushing and clearing metrics from memory 1 2 3 4 5 6 7 8 9 10 11 import json from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 ) your_metrics_object = metrics . serialize_metric_set () metrics . clear_metrics () print ( json . dumps ( your_metrics_object )) Testing your code \u00b6 Environment variables \u00b6 Tip Ignore this section, if you are explicitly setting namespace/default dimension via namespace and service parameters. For example, Metrics(namespace=ApplicationName, service=ServiceName) Use POWERTOOLS_METRICS_NAMESPACE and POWERTOOLS_SERVICE_NAME env vars when unit testing your code to ensure metric namespace and dimension objects are created, and your code doesn't fail validation. Injecting dummy Metric Namespace before running tests 1 POWERTOOLS_SERVICE_NAME = \"Example\" POWERTOOLS_METRICS_NAMESPACE = \"Application\" python -m pytest Clearing metrics \u00b6 Metrics keep metrics in memory across multiple instances. If you need to test this behaviour, you can use the following Pytest fixture to ensure metrics are reset incl. cold start: Clearing metrics between tests 1 2 3 4 5 6 7 8 @pytest . fixture ( scope = \"function\" , autouse = True ) def reset_metric_set (): # Clear out every metric data prior to every test metrics = Metrics () metrics . clear_metrics () metrics_global . is_cold_start = True # ensure each test has cold start metrics . clear_default_dimensions () # remove persisted default dimensions, if any yield Functional testing \u00b6 As metrics are logged to standard output, you can read standard output and assert whether metrics are present. Here's an example using pytest with capsys built-in fixture: Assert single EMF blob with pytest.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit import json def test_log_metrics ( capsys ): # GIVEN Metrics is initialized metrics = Metrics ( namespace = \"ServerlessAirline\" ) # WHEN we utilize log_metrics to serialize # and flush all metrics at the end of a function execution @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) lambda_handler ({}, {}) log = capsys . readouterr () . out . strip () # remove any extra line metrics_output = json . loads ( log ) # deserialize JSON str # THEN we should have no exceptions # and a valid EMF object should be flushed correctly assert \"SuccessfulBooking\" in log # basic string assertion in JSON str assert \"SuccessfulBooking\" in metrics_output [ \"_aws\" ][ \"CloudWatchMetrics\" ][ 0 ][ \"Metrics\" ][ 0 ][ \"Name\" ] Assert multiple EMF blobs with pytest 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit from collections import namedtuple import json def capture_metrics_output_multiple_emf_objects ( capsys ): return [ json . loads ( line . strip ()) for line in capsys . readouterr () . out . split ( \" \\n \" ) if line ] def test_log_metrics ( capsys ): # GIVEN Metrics is initialized metrics = Metrics ( namespace = \"ServerlessAirline\" ) # WHEN log_metrics is used with capture_cold_start_metric @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) # log_metrics uses function_name property from context to add as a dimension for cold start metric LambdaContext = namedtuple ( \"LambdaContext\" , \"function_name\" ) lambda_handler ({}, LambdaContext ( \"example_fn\" ) cold_start_blob , custom_metrics_blob = capture_metrics_output_multiple_emf_objects ( capsys ) # THEN ColdStart metric and function_name dimension should be logged # in a separate EMF blob than the application metrics assert cold_start_blob [ \"ColdStart\" ] == [ 1.0 ] assert cold_start_blob [ \"function_name\" ] == \"example_fn\" assert \"SuccessfulBooking\" in custom_metrics_blob # as per previous example Tip For more elaborate assertions and comparisons, check out our functional testing for Metrics utility.","title":"Metrics"},{"location":"core/metrics/#key-features","text":"Aggregate up to 100 metrics using a single CloudWatch EMF object (large JSON blob) Validate against common metric definitions mistakes (metric unit, values, max dimensions, max metrics, etc) Metrics are created asynchronously by CloudWatch service, no custom stacks needed Context manager to create a one off metric with a different dimension","title":"Key features"},{"location":"core/metrics/#terminologies","text":"If you're new to Amazon CloudWatch, there are two terminologies you must be aware of before using this utility: Namespace . It's the highest level container that will group multiple metrics from multiple services for a given application, for example ServerlessEcommerce . Dimensions . Metrics metadata in key-value format. They help you slice and dice metrics visualization, for example ColdStart metric by Payment service . Metric terminology, visually explained","title":"Terminologies"},{"location":"core/metrics/#getting-started","text":"Metric has two global settings that will be used across all metrics emitted: Setting Description Environment variable Constructor parameter Metric namespace Logical container where all metrics will be placed e.g. ServerlessAirline POWERTOOLS_METRICS_NAMESPACE namespace Service Optionally, sets service metric dimension across all metrics e.g. payment POWERTOOLS_SERVICE_NAME service Tip Use your application or main service as the metric namespace to easily group all metrics. Example AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Environment : Variables : POWERTOOLS_SERVICE_NAME : payment POWERTOOLS_METRICS_NAMESPACE : ServerlessAirline app.py 1 2 3 4 5 6 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics () # Sets metric namespace and service via env var # OR metrics = Metrics ( namespace = \"ServerlessAirline\" , service = \"orders\" ) # Sets metric namespace, and service as a metric dimension","title":"Getting started"},{"location":"core/metrics/#creating-metrics","text":"You can create metrics using add_metric , and you can create dimensions for all your aggregate metrics using add_dimension method. Tip You can initialize Metrics in any other module too. It'll keep track of your aggregate metrics in memory to optimize costs (one blob instead of multiples). Metrics 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Metrics with custom dimensions 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Tip: Autocomplete Metric Units MetricUnit enum facilitate finding a supported metric unit by CloudWatch. Alternatively, you can pass the value as a string if you already know them e.g. \"Count\". Note: Metrics overflow CloudWatch EMF supports a max of 100 metrics per batch. Metrics utility will flush all metrics when adding the 100th metric. Subsequent metrics, e.g. 101th, will be aggregated into a new EMF object, for your convenience. Warning: Do not create metrics or dimensions outside the handler Metrics or dimensions added in the global scope will only be added during cold start. Disregard if you that's the intended behaviour.","title":"Creating metrics"},{"location":"core/metrics/#adding-default-dimensions","text":"You can use either set_default_dimensions method or default_permissions parameter in log_metrics decorator to persist dimensions across Lambda invocations. If you'd like to remove them at some point, you can use clear_default_dimensions method. set_default_dimensions method 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . set_default_dimensions ( environment = \"prod\" , another = \"one\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) with log_metrics decorator 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) DEFAULT_DIMENSIONS = { \"environment\" : \"prod\" , \"another\" : \"one\" } @metrics . log_metrics ( default_dimensions = DEFAULT_DIMENSIONS ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 )","title":"Adding default dimensions"},{"location":"core/metrics/#flushing-metrics","text":"As you finish adding all your metrics, you need to serialize and flush them to standard output. You can do that automatically with the log_metrics decorator. This decorator also validates , serializes , and flushes all your metrics. During metrics validation, if no metrics are provided then a warning will be logged, but no exception will be raised. app.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"ExampleService\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = MetricUnit . Count , value = 1 ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"BookingConfirmation\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"BookingConfirmation\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"ExampleService\" } Tip: Metric validation If metrics are provided, and any of the following criteria are not met, SchemaValidationError exception will be raised: Maximum of 9 dimensions Namespace is set, and no more than one Metric units must be supported by CloudWatch","title":"Flushing metrics"},{"location":"core/metrics/#raising-schemavalidationerror-on-empty-metrics","text":"If you want to ensure at least one metric is always emitted, you can pass raise_on_empty_metrics to the log_metrics decorator: Raising SchemaValidationError exception if no metrics are added 1 2 3 4 5 6 7 from aws_lambda_powertools.metrics import Metrics metrics = Metrics () @metrics . log_metrics ( raise_on_empty_metrics = True ) def lambda_handler ( evt , ctx ): ... Suppressing warning messages on empty metrics If you expect your function to execute without publishing metrics every time, you can suppress the warning with warnings.filterwarnings(\"ignore\", \"No metrics to publish*\") .","title":"Raising SchemaValidationError on empty metrics"},{"location":"core/metrics/#nesting-multiple-middlewares","text":"When using multiple middlewares, use log_metrics as your last decorator wrapping all subsequent ones to prevent early Metric validations when code hasn't been run yet. Example with multiple decorators 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Metrics , Tracer from aws_lambda_powertools.metrics import MetricUnit tracer = Tracer ( service = \"booking\" ) metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics @tracer . capture_lambda_handler def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = MetricUnit . Count , value = 1 )","title":"Nesting multiple middlewares"},{"location":"core/metrics/#capturing-cold-start-metric","text":"You can optionally capture cold start metrics with log_metrics decorator via capture_cold_start_metric param. Generating function cold start metric 1 2 3 4 5 6 7 from aws_lambda_powertools import Metrics metrics = Metrics ( service = \"ExampleService\" ) @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( evt , ctx ): ... If it's a cold start invocation, this feature will: Create a separate EMF blob solely containing a metric named ColdStart Add function_name and service dimensions This has the advantage of keeping cold start metric separate from your application metrics, where you might have unrelated dimensions. Info We do not emit 0 as a value for ColdStart metric for cost reasons. Let us know if you'd prefer a flag to override it.","title":"Capturing cold start metric"},{"location":"core/metrics/#advanced","text":"","title":"Advanced"},{"location":"core/metrics/#adding-metadata","text":"You can add high-cardinality data as part of your Metrics log with add_metadata method. This is useful when you want to search highly contextual information along with your metrics in your logs. Info This will not be available during metrics visualization - Use dimensions for this purpose app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_metadata ( key = \"booking_id\" , value = \"booking_uuid\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"SuccessfulBooking\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"SuccessfulBooking\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"booking\" , \"booking_id\" : \"booking_uuid\" }","title":"Adding metadata"},{"location":"core/metrics/#single-metric-with-a-different-dimension","text":"CloudWatch EMF uses the same dimensions across all your metrics. Use single_metric if you have a metric that should have different dimensions. Info Generally, this would be an edge case since you pay for unique metric . Keep the following formula in mind: unique metric = (metric_name + dimension_name + dimension_value) Generating an EMF blob with a single metric 1 2 3 4 5 6 7 8 from aws_lambda_powertools import single_metric from aws_lambda_powertools.metrics import MetricUnit def lambda_handler ( evt , ctx ): with single_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 , namespace = \"ExampleApplication\" ) as metric : metric . add_dimension ( name = \"function_context\" , value = \"$LATEST\" ) ...","title":"Single metric with a different dimension"},{"location":"core/metrics/#flushing-metrics-manually","text":"If you prefer not to use log_metrics because you might want to encapsulate additional logic when doing so, you can manually flush and clear metrics as follows: Warning Metrics, dimensions and namespace validation still applies Manually flushing and clearing metrics from memory 1 2 3 4 5 6 7 8 9 10 11 import json from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 ) your_metrics_object = metrics . serialize_metric_set () metrics . clear_metrics () print ( json . dumps ( your_metrics_object ))","title":"Flushing metrics manually"},{"location":"core/metrics/#testing-your-code","text":"","title":"Testing your code"},{"location":"core/metrics/#environment-variables","text":"Tip Ignore this section, if you are explicitly setting namespace/default dimension via namespace and service parameters. For example, Metrics(namespace=ApplicationName, service=ServiceName) Use POWERTOOLS_METRICS_NAMESPACE and POWERTOOLS_SERVICE_NAME env vars when unit testing your code to ensure metric namespace and dimension objects are created, and your code doesn't fail validation. Injecting dummy Metric Namespace before running tests 1 POWERTOOLS_SERVICE_NAME = \"Example\" POWERTOOLS_METRICS_NAMESPACE = \"Application\" python -m pytest","title":"Environment variables"},{"location":"core/metrics/#clearing-metrics","text":"Metrics keep metrics in memory across multiple instances. If you need to test this behaviour, you can use the following Pytest fixture to ensure metrics are reset incl. cold start: Clearing metrics between tests 1 2 3 4 5 6 7 8 @pytest . fixture ( scope = \"function\" , autouse = True ) def reset_metric_set (): # Clear out every metric data prior to every test metrics = Metrics () metrics . clear_metrics () metrics_global . is_cold_start = True # ensure each test has cold start metrics . clear_default_dimensions () # remove persisted default dimensions, if any yield","title":"Clearing metrics"},{"location":"core/metrics/#functional-testing","text":"As metrics are logged to standard output, you can read standard output and assert whether metrics are present. Here's an example using pytest with capsys built-in fixture: Assert single EMF blob with pytest.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit import json def test_log_metrics ( capsys ): # GIVEN Metrics is initialized metrics = Metrics ( namespace = \"ServerlessAirline\" ) # WHEN we utilize log_metrics to serialize # and flush all metrics at the end of a function execution @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) lambda_handler ({}, {}) log = capsys . readouterr () . out . strip () # remove any extra line metrics_output = json . loads ( log ) # deserialize JSON str # THEN we should have no exceptions # and a valid EMF object should be flushed correctly assert \"SuccessfulBooking\" in log # basic string assertion in JSON str assert \"SuccessfulBooking\" in metrics_output [ \"_aws\" ][ \"CloudWatchMetrics\" ][ 0 ][ \"Metrics\" ][ 0 ][ \"Name\" ] Assert multiple EMF blobs with pytest 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit from collections import namedtuple import json def capture_metrics_output_multiple_emf_objects ( capsys ): return [ json . loads ( line . strip ()) for line in capsys . readouterr () . out . split ( \" \\n \" ) if line ] def test_log_metrics ( capsys ): # GIVEN Metrics is initialized metrics = Metrics ( namespace = \"ServerlessAirline\" ) # WHEN log_metrics is used with capture_cold_start_metric @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) # log_metrics uses function_name property from context to add as a dimension for cold start metric LambdaContext = namedtuple ( \"LambdaContext\" , \"function_name\" ) lambda_handler ({}, LambdaContext ( \"example_fn\" ) cold_start_blob , custom_metrics_blob = capture_metrics_output_multiple_emf_objects ( capsys ) # THEN ColdStart metric and function_name dimension should be logged # in a separate EMF blob than the application metrics assert cold_start_blob [ \"ColdStart\" ] == [ 1.0 ] assert cold_start_blob [ \"function_name\" ] == \"example_fn\" assert \"SuccessfulBooking\" in custom_metrics_blob # as per previous example Tip For more elaborate assertions and comparisons, check out our functional testing for Metrics utility.","title":"Functional testing"},{"location":"core/tracer/","text":"Tracer is an opinionated thin wrapper for AWS X-Ray Python SDK . Key features \u00b6 Auto capture cold start as annotation, and responses or full exceptions as metadata Auto-disable when not running in AWS Lambda environment Support tracing async methods, generators, and context managers Auto patch supported modules by AWS X-Ray Getting started \u00b6 Permissions \u00b6 Before your use this utility, your AWS Lambda function must have permissions to send traces to AWS X-Ray. AWS Serverless Application Model (SAM) example 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Tracing : Active Environment : Variables : POWERTOOLS_SERVICE_NAME : example Lambda handler \u00b6 You can quickly start by initializing Tracer and use capture_lambda_handler decorator for your Lambda handler. Tracing Lambda handler with capture_lambda_handler 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Tracer tracer = Tracer () # Sets service via env var # OR tracer = Tracer(service=\"example\") @tracer . capture_lambda_handler def handler ( event , context ): charge_id = event . get ( 'charge_id' ) payment = collect_payment ( charge_id ) ... capture_lambda_handler performs these additional tasks to ease operations: Creates a ColdStart annotation to easily filter traces that have had an initialization overhead Creates a Service annotation if service parameter or POWERTOOLS_SERVICE_NAME is set Captures any response, or full exceptions generated by the handler, and include as tracing metadata Annotations & Metadata \u00b6 Annotations are key-values associated with traces and indexed by AWS X-Ray. You can use them to filter traces and to create Trace Groups to slice and dice your transactions. Adding annotations with put_annotation method 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): ... tracer . put_annotation ( key = \"PaymentStatus\" , value = \"SUCCESS\" ) Metadata are key-values also associated with traces but not indexed by AWS X-Ray. You can use them to add additional context for an operation using any native object. Adding arbitrary metadata with put_metadata method 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): ... ret = some_logic () tracer . put_metadata ( key = \"payment_response\" , value = ret ) Synchronous functions \u00b6 You can trace synchronous functions using the capture_method decorator. Tracing an arbitrary function with capture_method 1 2 3 4 5 @tracer . capture_method def collect_payment ( charge_id ): ret = requests . post ( PAYMENT_ENDPOINT ) # logic tracer . put_annotation ( \"PAYMENT_STATUS\" , \"SUCCESS\" ) # custom annotation return ret Note: Function responses are auto-captured and stored as JSON, by default. Use capture_response parameter to override this behaviour. The serialization is performed by aws-xray-sdk via jsonpickle module. This can cause side effects for file-like objects like boto S3 StreamingBody , where its response will be read only once during serialization. Asynchronous and generator functions \u00b6 Warning We do not support asynchronous Lambda handler You can trace asynchronous functions and generator functions (including context managers) using capture_method . Async 1 2 3 4 5 6 7 8 9 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_method async def collect_payment (): ... Context manager 1 2 3 4 5 6 7 8 9 10 11 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @contextlib . contextmanager @tracer . capture_method def collect_payment_ctxman (): yield result ... Generators 1 2 3 4 5 6 7 8 9 10 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_method def collect_payment_gen (): yield result ... Advanced \u00b6 Patching modules \u00b6 Tracer automatically patches all supported libraries by X-Ray during initialization, by default. Underneath, AWS X-Ray SDK checks whether a supported library has been imported before patching. If you're looking to shave a few microseconds, or milliseconds depending on your function memory configuration, you can patch specific modules using patch_modules param: Example of explicitly patching boto3 and requests only 1 2 3 4 5 6 7 import boto3 import requests from aws_lambda_powertools import Tracer modules_to_be_patched = [ \"boto3\" , \"requests\" ] tracer = Tracer ( patch_modules = modules_to_be_patched ) Disabling response auto-capture \u00b6 Use capture_response=False parameter in both capture_lambda_handler and capture_method decorators to instruct Tracer not to serialize function responses as metadata. Info: This is useful in three common scenarios You might return sensitive information you don't want it to be added to your traces You might manipulate streaming objects that can be read only once ; this prevents subsequent calls from being empty You might return more than 64K of data e.g., message too long error sensitive_data_scenario.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer @tracer . capture_method ( capture_response = False ) def fetch_sensitive_information (): return \"sensitive_information\" @tracer . capture_lambda_handler ( capture_response = False ) def handler ( event , context ): sensitive_information = fetch_sensitive_information () streaming_object_scenario.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer @tracer . capture_method ( capture_response = False ) def get_s3_object ( bucket_name , object_key ): s3 = boto3 . client ( \"s3\" ) s3_object = get_object ( Bucket = bucket_name , Key = object_key ) return s3_object Disabling exception auto-capture \u00b6 Use capture_error=False parameter in both capture_lambda_handler and capture_method decorators to instruct Tracer not to serialize exceptions as metadata. Info Useful when returning sensitive information in exceptions/stack traces you don't control Disabling exception auto-capture for tracing metadata 1 2 3 4 5 from aws_lambda_powertools import Tracer @tracer . capture_lambda_handler ( capture_error = False ) def handler ( event , context ): raise ValueError ( \"some sensitive info in the stack trace...\" ) Ignoring certain HTTP endpoints \u00b6 You might have endpoints you don't want requests to be traced, perhaps due to the volume of calls or sensitive URLs. You can use ignore_endpoint method with the hostname and/or URLs you'd like it to be ignored - globs ( * ) are allowed. Ignoring certain HTTP endpoints from being traced 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from aws_lambda_powertools import Tracer tracer = Tracer () # ignore all calls to `ec2.amazon.com` tracer . ignore_endpoint ( hostname = \"ec2.amazon.com\" ) # ignore calls to `*.sensitive.com/password` and `*.sensitive.com/credit-card` tracer . ignore_endpoint ( hostname = \"*.sensitive.com\" , urls = [ \"/password\" , \"/credit-card\" ]) def ec2_api_calls (): return \"suppress_api_responses\" @tracer . capture_lambda_handler def handler ( event , context ): for x in long_list : ec2_api_calls () Tracing aiohttp requests \u00b6 Info This snippet assumes you have aiohttp as a dependency You can use aiohttp_trace_config function to create a valid aiohttp trace_config object . This is necessary since X-Ray utilizes aiohttp trace hooks to capture requests end-to-end. Tracing aiohttp requests 1 2 3 4 5 6 7 8 9 10 11 12 13 import asyncio import aiohttp from aws_lambda_powertools import Tracer from aws_lambda_powertools.tracing import aiohttp_trace_config tracer = Tracer () async def aiohttp_task (): async with aiohttp . ClientSession ( trace_configs = [ aiohttp_trace_config ()]) as session : async with session . get ( \"https://httpbin.org/json\" ) as resp : resp = await resp . json () return resp Escape hatch mechanism \u00b6 You can use tracer.provider attribute to access all methods provided by AWS X-Ray xray_recorder object. This is useful when you need a feature available in X-Ray that is not available in the Tracer utility, for example thread-safe , or context managers . Tracing a code block with in_subsegment escape hatch 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): with tracer . provider . in_subsegment ( '## custom subsegment' ) as subsegment : ret = some_work () subsegment . put_metadata ( 'response' , ret ) Concurrent asynchronous functions \u00b6 Warning X-Ray SDK will raise an exception when async functions are run and traced concurrently A safe workaround mechanism is to use in_subsegment_async available via Tracer escape hatch ( tracer.provider ). Workaround to safely trace async concurrent functions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import asyncio from aws_lambda_powertools import Tracer tracer = Tracer () async def another_async_task (): async with tracer . provider . in_subsegment_async ( \"## another_async_task\" ) as subsegment : subsegment . put_annotation ( key = \"key\" , value = \"value\" ) subsegment . put_metadata ( key = \"key\" , value = \"value\" , namespace = \"namespace\" ) ... async def another_async_task_2 (): ... @tracer . capture_method async def collect_payment ( charge_id ): asyncio . gather ( another_async_task (), another_async_task_2 ()) ... Reusing Tracer across your code \u00b6 Tracer keeps a copy of its configuration after the first initialization. This is useful for scenarios where you want to use Tracer in more than one location across your code base. Warning: Import order matters when using Lambda Layers or multiple modules Do not set auto_patch=False when reusing Tracer in Lambda Layers, or in multiple modules. This can result in the first Tracer config being inherited by new instances, and their modules not being patched. Tracer will automatically ignore imported modules that have been patched. handler.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer from payment import collect_payment tracer = Tracer ( service = \"payment\" ) @tracer . capture_lambda_handler def handler ( event , context ): charge_id = event . get ( 'charge_id' ) payment = collect_payment ( charge_id ) payment.py A new instance of Tracer will be created but will reuse the previous Tracer instance configuration, similar to a Singleton. 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer tracer = Tracer ( service = \"payment\" ) @tracer . capture_method def collect_payment ( charge_id : str ): ... Testing your code \u00b6 Tracer is disabled by default when not running in the AWS Lambda environment - This means no code changes or environment variables to be set. Tips \u00b6 Use annotations on key operations to slice and dice traces, create unique views, and create metrics from it via Trace Groups Use a namespace when adding metadata to group data more easily Annotations and metadata are added to the current subsegment opened. If you want them in a specific subsegment, use a context manager via the escape hatch mechanism","title":"Tracer"},{"location":"core/tracer/#key-features","text":"Auto capture cold start as annotation, and responses or full exceptions as metadata Auto-disable when not running in AWS Lambda environment Support tracing async methods, generators, and context managers Auto patch supported modules by AWS X-Ray","title":"Key features"},{"location":"core/tracer/#getting-started","text":"","title":"Getting started"},{"location":"core/tracer/#permissions","text":"Before your use this utility, your AWS Lambda function must have permissions to send traces to AWS X-Ray. AWS Serverless Application Model (SAM) example 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Tracing : Active Environment : Variables : POWERTOOLS_SERVICE_NAME : example","title":"Permissions"},{"location":"core/tracer/#lambda-handler","text":"You can quickly start by initializing Tracer and use capture_lambda_handler decorator for your Lambda handler. Tracing Lambda handler with capture_lambda_handler 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Tracer tracer = Tracer () # Sets service via env var # OR tracer = Tracer(service=\"example\") @tracer . capture_lambda_handler def handler ( event , context ): charge_id = event . get ( 'charge_id' ) payment = collect_payment ( charge_id ) ... capture_lambda_handler performs these additional tasks to ease operations: Creates a ColdStart annotation to easily filter traces that have had an initialization overhead Creates a Service annotation if service parameter or POWERTOOLS_SERVICE_NAME is set Captures any response, or full exceptions generated by the handler, and include as tracing metadata","title":"Lambda handler"},{"location":"core/tracer/#annotations-metadata","text":"Annotations are key-values associated with traces and indexed by AWS X-Ray. You can use them to filter traces and to create Trace Groups to slice and dice your transactions. Adding annotations with put_annotation method 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): ... tracer . put_annotation ( key = \"PaymentStatus\" , value = \"SUCCESS\" ) Metadata are key-values also associated with traces but not indexed by AWS X-Ray. You can use them to add additional context for an operation using any native object. Adding arbitrary metadata with put_metadata method 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): ... ret = some_logic () tracer . put_metadata ( key = \"payment_response\" , value = ret )","title":"Annotations &amp; Metadata"},{"location":"core/tracer/#synchronous-functions","text":"You can trace synchronous functions using the capture_method decorator. Tracing an arbitrary function with capture_method 1 2 3 4 5 @tracer . capture_method def collect_payment ( charge_id ): ret = requests . post ( PAYMENT_ENDPOINT ) # logic tracer . put_annotation ( \"PAYMENT_STATUS\" , \"SUCCESS\" ) # custom annotation return ret Note: Function responses are auto-captured and stored as JSON, by default. Use capture_response parameter to override this behaviour. The serialization is performed by aws-xray-sdk via jsonpickle module. This can cause side effects for file-like objects like boto S3 StreamingBody , where its response will be read only once during serialization.","title":"Synchronous functions"},{"location":"core/tracer/#asynchronous-and-generator-functions","text":"Warning We do not support asynchronous Lambda handler You can trace asynchronous functions and generator functions (including context managers) using capture_method . Async 1 2 3 4 5 6 7 8 9 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_method async def collect_payment (): ... Context manager 1 2 3 4 5 6 7 8 9 10 11 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @contextlib . contextmanager @tracer . capture_method def collect_payment_ctxman (): yield result ... Generators 1 2 3 4 5 6 7 8 9 10 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_method def collect_payment_gen (): yield result ...","title":"Asynchronous and generator functions"},{"location":"core/tracer/#advanced","text":"","title":"Advanced"},{"location":"core/tracer/#patching-modules","text":"Tracer automatically patches all supported libraries by X-Ray during initialization, by default. Underneath, AWS X-Ray SDK checks whether a supported library has been imported before patching. If you're looking to shave a few microseconds, or milliseconds depending on your function memory configuration, you can patch specific modules using patch_modules param: Example of explicitly patching boto3 and requests only 1 2 3 4 5 6 7 import boto3 import requests from aws_lambda_powertools import Tracer modules_to_be_patched = [ \"boto3\" , \"requests\" ] tracer = Tracer ( patch_modules = modules_to_be_patched )","title":"Patching modules"},{"location":"core/tracer/#disabling-response-auto-capture","text":"Use capture_response=False parameter in both capture_lambda_handler and capture_method decorators to instruct Tracer not to serialize function responses as metadata. Info: This is useful in three common scenarios You might return sensitive information you don't want it to be added to your traces You might manipulate streaming objects that can be read only once ; this prevents subsequent calls from being empty You might return more than 64K of data e.g., message too long error sensitive_data_scenario.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer @tracer . capture_method ( capture_response = False ) def fetch_sensitive_information (): return \"sensitive_information\" @tracer . capture_lambda_handler ( capture_response = False ) def handler ( event , context ): sensitive_information = fetch_sensitive_information () streaming_object_scenario.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer @tracer . capture_method ( capture_response = False ) def get_s3_object ( bucket_name , object_key ): s3 = boto3 . client ( \"s3\" ) s3_object = get_object ( Bucket = bucket_name , Key = object_key ) return s3_object","title":"Disabling response auto-capture"},{"location":"core/tracer/#disabling-exception-auto-capture","text":"Use capture_error=False parameter in both capture_lambda_handler and capture_method decorators to instruct Tracer not to serialize exceptions as metadata. Info Useful when returning sensitive information in exceptions/stack traces you don't control Disabling exception auto-capture for tracing metadata 1 2 3 4 5 from aws_lambda_powertools import Tracer @tracer . capture_lambda_handler ( capture_error = False ) def handler ( event , context ): raise ValueError ( \"some sensitive info in the stack trace...\" )","title":"Disabling exception auto-capture"},{"location":"core/tracer/#ignoring-certain-http-endpoints","text":"You might have endpoints you don't want requests to be traced, perhaps due to the volume of calls or sensitive URLs. You can use ignore_endpoint method with the hostname and/or URLs you'd like it to be ignored - globs ( * ) are allowed. Ignoring certain HTTP endpoints from being traced 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from aws_lambda_powertools import Tracer tracer = Tracer () # ignore all calls to `ec2.amazon.com` tracer . ignore_endpoint ( hostname = \"ec2.amazon.com\" ) # ignore calls to `*.sensitive.com/password` and `*.sensitive.com/credit-card` tracer . ignore_endpoint ( hostname = \"*.sensitive.com\" , urls = [ \"/password\" , \"/credit-card\" ]) def ec2_api_calls (): return \"suppress_api_responses\" @tracer . capture_lambda_handler def handler ( event , context ): for x in long_list : ec2_api_calls ()","title":"Ignoring certain HTTP endpoints"},{"location":"core/tracer/#tracing-aiohttp-requests","text":"Info This snippet assumes you have aiohttp as a dependency You can use aiohttp_trace_config function to create a valid aiohttp trace_config object . This is necessary since X-Ray utilizes aiohttp trace hooks to capture requests end-to-end. Tracing aiohttp requests 1 2 3 4 5 6 7 8 9 10 11 12 13 import asyncio import aiohttp from aws_lambda_powertools import Tracer from aws_lambda_powertools.tracing import aiohttp_trace_config tracer = Tracer () async def aiohttp_task (): async with aiohttp . ClientSession ( trace_configs = [ aiohttp_trace_config ()]) as session : async with session . get ( \"https://httpbin.org/json\" ) as resp : resp = await resp . json () return resp","title":"Tracing aiohttp requests"},{"location":"core/tracer/#escape-hatch-mechanism","text":"You can use tracer.provider attribute to access all methods provided by AWS X-Ray xray_recorder object. This is useful when you need a feature available in X-Ray that is not available in the Tracer utility, for example thread-safe , or context managers . Tracing a code block with in_subsegment escape hatch 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): with tracer . provider . in_subsegment ( '## custom subsegment' ) as subsegment : ret = some_work () subsegment . put_metadata ( 'response' , ret )","title":"Escape hatch mechanism"},{"location":"core/tracer/#concurrent-asynchronous-functions","text":"Warning X-Ray SDK will raise an exception when async functions are run and traced concurrently A safe workaround mechanism is to use in_subsegment_async available via Tracer escape hatch ( tracer.provider ). Workaround to safely trace async concurrent functions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import asyncio from aws_lambda_powertools import Tracer tracer = Tracer () async def another_async_task (): async with tracer . provider . in_subsegment_async ( \"## another_async_task\" ) as subsegment : subsegment . put_annotation ( key = \"key\" , value = \"value\" ) subsegment . put_metadata ( key = \"key\" , value = \"value\" , namespace = \"namespace\" ) ... async def another_async_task_2 (): ... @tracer . capture_method async def collect_payment ( charge_id ): asyncio . gather ( another_async_task (), another_async_task_2 ()) ...","title":"Concurrent asynchronous functions"},{"location":"core/tracer/#reusing-tracer-across-your-code","text":"Tracer keeps a copy of its configuration after the first initialization. This is useful for scenarios where you want to use Tracer in more than one location across your code base. Warning: Import order matters when using Lambda Layers or multiple modules Do not set auto_patch=False when reusing Tracer in Lambda Layers, or in multiple modules. This can result in the first Tracer config being inherited by new instances, and their modules not being patched. Tracer will automatically ignore imported modules that have been patched. handler.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer from payment import collect_payment tracer = Tracer ( service = \"payment\" ) @tracer . capture_lambda_handler def handler ( event , context ): charge_id = event . get ( 'charge_id' ) payment = collect_payment ( charge_id ) payment.py A new instance of Tracer will be created but will reuse the previous Tracer instance configuration, similar to a Singleton. 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer tracer = Tracer ( service = \"payment\" ) @tracer . capture_method def collect_payment ( charge_id : str ): ...","title":"Reusing Tracer across your code"},{"location":"core/tracer/#testing-your-code","text":"Tracer is disabled by default when not running in the AWS Lambda environment - This means no code changes or environment variables to be set.","title":"Testing your code"},{"location":"core/tracer/#tips","text":"Use annotations on key operations to slice and dice traces, create unique views, and create metrics from it via Trace Groups Use a namespace when adding metadata to group data more easily Annotations and metadata are added to the current subsegment opened. If you want them in a specific subsegment, use a context manager via the escape hatch mechanism","title":"Tips"},{"location":"core/event_handler/api_gateway/","text":"Event handler for Amazon API Gateway REST/HTTP APIs and Application Loader Balancer (ALB). Key Features \u00b6 Lightweight routing to reduce boilerplate for API Gateway REST/HTTP API and ALB Seamless support for CORS, binary and Gzip compression Integrates with Data classes utilities to easily access event and identity information Built-in support for Decimals JSON encoding Support for dynamic path expressions Router to allow for splitting up the handler accross multiple files Getting started \u00b6 Required resources \u00b6 You must have an existing API Gateway Proxy integration or ALB configured to invoke your Lambda function. There is no additional permissions or dependencies required to use this utility. This is the sample infrastructure for API Gateway we are using for the examples in this documentation. AWS Serverless Application Model (SAM) example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 AWSTemplateFormatVersion : '2010-09-09' Transform : AWS::Serverless-2016-10-31 Description : Hello world event handler API Gateway Globals : Api : TracingEnabled : true Cors : # see CORS section AllowOrigin : \"'https://example.com'\" AllowHeaders : \"'Content-Type,Authorization,X-Amz-Date'\" MaxAge : \"'300'\" BinaryMediaTypes : # see Binary responses section - '*~1*' # converts to */* for any binary type Function : Timeout : 5 Runtime : python3.8 Tracing : Active Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_LOGGER_SAMPLE_RATE : 0.1 POWERTOOLS_LOGGER_LOG_EVENT : true POWERTOOLS_METRICS_NAMESPACE : MyServerlessApplication POWERTOOLS_SERVICE_NAME : my_api-service Resources : ApiFunction : Type : AWS::Serverless::Function Properties : Handler : app.lambda_handler CodeUri : api_handler/ Description : API handler function Events : ApiEvent : Type : Api Properties : Path : /{proxy+} # Send requests on any path to the lambda function Method : ANY # Send requests using any http method to the lambda function API Gateway decorator \u00b6 You can define your functions to match a path and HTTP method, when you use the decorator ApiGatewayResolver . Here's an example where we have two separate functions to resolve two paths: /hello . Info We automatically serialize Dict responses as JSON, trim whitespaces for compact responses, and set content-type to application/json . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver tracer = Tracer () logger = Logger () app = ApiGatewayResolver () # by default API Gateway REST API (v1) @app . get ( \"/hello\" ) @tracer . capture_method def get_hello_universe (): return { \"message\" : \"hello universe\" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) hello_event.json This utility uses path and httpMethod to route to the right function. This helps make unit tests and local invocation easier too. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 { \"body\" : \"hello\" , \"resource\" : \"/hello\" , \"path\" : \"/hello\" , \"httpMethod\" : \"GET\" , \"isBase64Encoded\" : false , \"queryStringParameters\" : { \"foo\" : \"bar\" }, \"multiValueQueryStringParameters\" : {}, \"pathParameters\" : { \"hello\" : \"/hello\" }, \"stageVariables\" : {}, \"headers\" : { \"Accept\" : \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\" , \"Accept-Encoding\" : \"gzip, deflate, sdch\" , \"Accept-Language\" : \"en-US,en;q=0.8\" , \"Cache-Control\" : \"max-age=0\" , \"CloudFront-Forwarded-Proto\" : \"https\" , \"CloudFront-Is-Desktop-Viewer\" : \"true\" , \"CloudFront-Is-Mobile-Viewer\" : \"false\" , \"CloudFront-Is-SmartTV-Viewer\" : \"false\" , \"CloudFront-Is-Tablet-Viewer\" : \"false\" , \"CloudFront-Viewer-Country\" : \"US\" , \"Host\" : \"1234567890.execute-api.us-east-1.amazonaws.com\" , \"Upgrade-Insecure-Requests\" : \"1\" , \"User-Agent\" : \"Custom User Agent String\" , \"Via\" : \"1.1 08f323deadbeefa7af34d5feb414ce27.cloudfront.net (CloudFront)\" , \"X-Amz-Cf-Id\" : \"cDehVQoZnx43VYQb9j2-nvCh-9z396Uhbp027Y2JvkCPNLmGJHqlaA==\" , \"X-Forwarded-For\" : \"127.0.0.1, 127.0.0.2\" , \"X-Forwarded-Port\" : \"443\" , \"X-Forwarded-Proto\" : \"https\" }, \"multiValueHeaders\" : {}, \"requestContext\" : { \"accountId\" : \"123456789012\" , \"resourceId\" : \"123456\" , \"stage\" : \"Prod\" , \"requestId\" : \"c6af9ac6-7b61-11e6-9a41-93e8deadbeef\" , \"requestTime\" : \"25/Jul/2020:12:34:56 +0000\" , \"requestTimeEpoch\" : 1428582896000 , \"identity\" : { \"cognitoIdentityPoolId\" : null , \"accountId\" : null , \"cognitoIdentityId\" : null , \"caller\" : null , \"accessKey\" : null , \"sourceIp\" : \"127.0.0.1\" , \"cognitoAuthenticationType\" : null , \"cognitoAuthenticationProvider\" : null , \"userArn\" : null , \"userAgent\" : \"Custom User Agent String\" , \"user\" : null }, \"path\" : \"/Prod/hello\" , \"resourcePath\" : \"/hello\" , \"httpMethod\" : \"POST\" , \"apiId\" : \"1234567890\" , \"protocol\" : \"HTTP/1.1\" } } response.json 1 2 3 4 5 6 7 8 { \"statusCode\" : 200 , \"headers\" : { \"Content-Type\" : \"application/json\" }, \"body\" : \"{\\\"message\\\":\\\"hello universe\\\"}\" , \"isBase64Encoded\" : false } HTTP API \u00b6 When using API Gateway HTTP API to front your Lambda functions, you can instruct ApiGatewayResolver to conform with their contract via proxy_type param: Using HTTP API resolver 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver , ProxyEventType tracer = Tracer () logger = Logger () app = ApiGatewayResolver ( proxy_type = ProxyEventType . APIGatewayProxyEventV2 ) @app . get ( \"/hello\" ) @tracer . capture_method def get_hello_universe (): return { \"message\" : \"hello universe\" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_HTTP ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) ALB \u00b6 When using ALB to front your Lambda functions, you can instruct ApiGatewayResolver to conform with their contract via proxy_type param: Using ALB resolver 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver , ProxyEventType tracer = Tracer () logger = Logger () app = ApiGatewayResolver ( proxy_type = ProxyEventType . ALBEvent ) @app . get ( \"/hello\" ) @tracer . capture_method def get_hello_universe (): return { \"message\" : \"hello universe\" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPLICATION_LOAD_BALANCER ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) Dynamic routes \u00b6 You can use /path/{dynamic_value} when configuring dynamic URL paths. This allows you to define such dynamic value as part of your function signature. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver tracer = Tracer () logger = Logger () app = ApiGatewayResolver () @app . get ( \"/hello/<name>\" ) @tracer . capture_method def get_hello_you ( name ): return { \"message\" : f \"hello { name } \" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) sample_request.json 1 2 3 4 5 6 { \"resource\" : \"/hello/{name}\" , \"path\" : \"/hello/lessa\" , \"httpMethod\" : \"GET\" , ... } Nested routes \u00b6 You can also nest paths as configured earlier in our sample infrastructure : /{message}/{name} . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver tracer = Tracer () logger = Logger () app = ApiGatewayResolver () @app . get ( \"/<message>/<name>\" ) @tracer . capture_method def get_message ( message , name ): return { \"message\" : f \" { message } , { name }} \" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) sample_request.json 1 2 3 4 5 6 { \"resource\" : \"/{message}/{name}\" , \"path\" : \"/hi/michael\" , \"httpMethod\" : \"GET\" , ... } Catch-all routes \u00b6 Note We recommend having explicit routes whenever possible; use catch-all routes sparingly. You can use a regex string to handle an arbitrary number of paths within a request, for example .+ . You can also combine nested paths with greedy regex to catch in between routes. Warning We will choose the more explicit registered route that match incoming event. app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver app = ApiGatewayResolver () @app . get ( \".+\" ) def catch_any_route_after_any (): return { \"path_received\" : app . current_event . path } def lambda_handler ( event , context ): return app . resolve ( event , context ) sample_request.json 1 2 3 4 5 6 { \"resource\" : \"/any/route/should/work\" , \"path\" : \"/any/route/should/work\" , \"httpMethod\" : \"GET\" , ... } HTTP Methods \u00b6 You can use named decorators to specify the HTTP method that should be handled in your functions. As well as the get method already shown above, you can use post , put , patch , delete , and patch . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver tracer = Tracer () logger = Logger () app = ApiGatewayResolver () # Only POST HTTP requests to the path /hello will route to this function @app . post ( \"/hello\" ) @tracer . capture_method def get_hello_you (): name = app . current_event . json_body . get ( \"name\" ) return { \"message\" : f \"hello { name } \" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) sample_request.json 1 2 3 4 5 6 { \"resource\" : \"/hello/{name}\" , \"path\" : \"/hello/lessa\" , \"httpMethod\" : \"GET\" , ... } If you need to accept multiple HTTP methods in a single function, you can use the route method and pass a list of HTTP methods. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver tracer = Tracer () logger = Logger () app = ApiGatewayResolver () # PUT and POST HTTP requests to the path /hello will route to this function @app . route ( \"/hello\" , method = [ \"PUT\" , \"POST\" ]) @tracer . capture_method def get_hello_you (): name = app . current_event . json_body . get ( \"name\" ) return { \"message\" : f \"hello { name } \" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) sample_request.json 1 2 3 4 5 6 { \"resource\" : \"/hello/{name}\" , \"path\" : \"/hello/lessa\" , \"httpMethod\" : \"GET\" , ... } Note It is usually better to have separate functions for each HTTP method, as the functionality tends to differ depending on which method is used. Accessing request details \u00b6 By integrating with Data classes utilities , you have access to request details, Lambda context and also some convenient methods. These are made available in the response returned when instantiating ApiGatewayResolver , for example app.current_event and app.lambda_context . Query strings and payload \u00b6 Within app.current_event property, you can access query strings as dictionary via query_string_parameters , or by name via get_query_string_value method. You can access the raw payload via body property, or if it's a JSON string you can quickly deserialize it via json_body property. Accessing query strings, JSON payload, and raw payload 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver app = ApiGatewayResolver () @app . get ( \"/hello\" ) def get_hello_you (): query_strings_as_dict = app . current_event . query_string_parameters json_payload = app . current_event . json_body payload = app . current_event . body name = app . current_event . get_query_string_value ( name = \"name\" , default_value = \"\" ) return { \"message\" : f \"hello { name }} \" } def lambda_handler ( event , context ): return app . resolve ( event , context ) Headers \u00b6 Similarly to Query strings , you can access headers as dictionary via app.current_event.headers , or by name via get_header_value . Accessing HTTP Headers 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver app = ApiGatewayResolver () @app . get ( \"/hello\" ) def get_hello_you (): headers_as_dict = app . current_event . headers name = app . current_event . get_header_value ( name = \"X-Name\" , default_value = \"\" ) return { \"message\" : f \"hello { name }} \" } def lambda_handler ( event , context ): return app . resolve ( event , context ) Handling not found routes \u00b6 By default, we return 404 for any unmatched route. You can use not_found decorator to override this behaviour, and return a custom Response . Handling not found 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler import content_types from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver , Response from aws_lambda_powertools.event_handler.exceptions import NotFoundError tracer = Tracer () logger = Logger () app = ApiGatewayResolver () @app . not_found @tracer . capture_method def handle_not_found_errors ( exc : NotFoundError ) -> Response : # Return 418 upon 404 errors logger . info ( f \"Not found route: { app . current_event . path } \" ) return Response ( status_code = 418 , content_type = content_types . TEXT_PLAIN , body = \"I'm a teapot!\" ) @app . get ( \"/catch/me/if/you/can\" ) @tracer . capture_method def catch_me_if_you_can (): return { \"message\" : \"oh hey\" } @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) Exception handling \u00b6 You can use exception_handler decorator with any Python exception. This allows you to handle a common exception outside your route, for example validation errors. Exception handling 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler import content_types from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver , Response tracer = Tracer () logger = Logger () app = ApiGatewayResolver () @app . exception_handler ( ValueError ) def handle_value_error ( ex : ValueError ): metadata = { \"path\" : app . current_event . path } logger . error ( f \"Malformed request: { ex } \" , extra = metadata ) return Response ( status_code = 400 , content_type = content_types . TEXT_PLAIN , body = \"Invalid request\" , ) @app . get ( \"/hello\" ) @tracer . capture_method def hello_name (): name = app . current_event . get_query_string_value ( name = \"name\" ) if name is not None : raise ValueError ( \"name query string must be present\" ) return { \"message\" : f \"hello { name } \" } @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) Raising HTTP errors \u00b6 You can easily raise any HTTP Error back to the client using ServiceError exception. Info If you need to send custom headers, use Response class instead. Additionally, we provide pre-defined errors for the most popular ones such as HTTP 400, 401, 404, 500. Raising common HTTP Status errors (4xx, 5xx) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver from aws_lambda_powertools.event_handler.exceptions import ( BadRequestError , InternalServerError , NotFoundError , ServiceError , UnauthorizedError , ) tracer = Tracer () logger = Logger () app = ApiGatewayResolver () @app . get ( rule = \"/bad-request-error\" ) def bad_request_error (): # HTTP 400 raise BadRequestError ( \"Missing required parameter\" ) @app . get ( rule = \"/unauthorized-error\" ) def unauthorized_error (): # HTTP 401 raise UnauthorizedError ( \"Unauthorized\" ) @app . get ( rule = \"/not-found-error\" ) def not_found_error (): # HTTP 404 raise NotFoundError @app . get ( rule = \"/internal-server-error\" ) def internal_server_error (): # HTTP 500 raise InternalServerError ( \"Internal server error\" ) @app . get ( rule = \"/service-error\" , cors = True ) def service_error (): raise ServiceError ( 502 , \"Something went wrong!\" ) # alternatively # from http import HTTPStatus # raise ServiceError(HTTPStatus.BAD_GATEWAY.value, \"Something went wrong) def handler ( event , context ): return app . resolve ( event , context ) Custom Domain API Mappings \u00b6 When using Custom Domain API Mappings feature, you must use strip_prefixes param in the ApiGatewayResolver constructor. Scenario: You have a custom domain api.mydomain.dev and set an API Mapping payment to forward requests to your Payments API, the path argument will be /payment/<your_actual_path> . This will lead to a HTTP 404 despite having your Lambda configured correctly. See the example below on how to account for this change. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver tracer = Tracer () logger = Logger () app = ApiGatewayResolver ( strip_prefixes = [ \"/payment\" ]) @app . get ( \"/subscriptions/<subscription>\" ) @tracer . capture_method def get_subscription ( subscription ): return { \"subscription_id\" : subscription } @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) sample_request.json 1 2 3 4 5 6 { \"resource\" : \"/subscriptions/{subscription}\" , \"path\" : \"/payment/subscriptions/123\" , \"httpMethod\" : \"GET\" , ... } Note After removing a path prefix with strip_prefixes , the new root path will automatically be mapped to the path argument of / . For example, when using strip_prefixes value of /pay , there is no difference between a request path of /pay and /pay/ ; and the path argument would be defined as / . Advanced \u00b6 CORS \u00b6 You can configure CORS at the ApiGatewayResolver constructor via cors parameter using the CORSConfig class. This will ensure that CORS headers are always returned as part of the response when your functions match the path invoked. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver , CORSConfig tracer = Tracer () logger = Logger () cors_config = CORSConfig ( allow_origin = \"https://example.com\" , max_age = 300 ) app = ApiGatewayResolver ( cors = cors_config ) @app . get ( \"/hello/<name>\" ) @tracer . capture_method def get_hello_you ( name ): return { \"message\" : f \"hello { name } \" } @app . get ( \"/hello\" , cors = False ) # optionally exclude CORS from response, if needed @tracer . capture_method def get_hello_no_cors_needed (): return { \"message\" : \"hello, no CORS needed for this path ;)\" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) response.json 1 2 3 4 5 6 7 8 9 10 { \"statusCode\" : 200 , \"headers\" : { \"Content-Type\" : \"application/json\" , \"Access-Control-Allow-Origin\" : \"https://www.example.com\" , \"Access-Control-Allow-Headers\" : \"Authorization,Content-Type,X-Amz-Date,X-Amz-Security-Token,X-Api-Key\" }, \"body\" : \"{\\\"message\\\":\\\"hello lessa\\\"}\" , \"isBase64Encoded\" : false } response_no_cors.json 1 2 3 4 5 6 7 8 { \"statusCode\" : 200 , \"headers\" : { \"Content-Type\" : \"application/json\" }, \"body\" : \"{\\\"message\\\":\\\"hello lessa\\\"}\" , \"isBase64Encoded\" : false } Tip Optionally disable CORS on a per path basis with cors=False parameter. Pre-flight \u00b6 Pre-flight (OPTIONS) calls are typically handled at the API Gateway level as per our sample infrastructure , no Lambda integration necessary. However, ALB expects you to handle pre-flight requests. For convenience, we automatically handle that for you as long as you setup CORS in the constructor level . Defaults \u00b6 For convenience, these are the default values when using CORSConfig to enable CORS: Warning Always configure allow_origin when using in production. Key Value Note allow_origin : str * Only use the default value for development. Never use * for production unless your use case requires it allow_headers : List[str] [Authorization, Content-Type, X-Amz-Date, X-Api-Key, X-Amz-Security-Token] Additional headers will be appended to the default list for your convenience expose_headers : List[str] [] Any additional header beyond the safe listed by CORS specification . max_age : int `` Only for pre-flight requests if you choose to have your function to handle it instead of API Gateway allow_credentials : bool False Only necessary when you need to expose cookies, authorization headers or TLS client certificates. Fine grained responses \u00b6 You can use the Response class to have full control over the response, for example you might want to add additional headers or set a custom Content-type. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver , Response app = ApiGatewayResolver () @app . get ( \"/hello\" ) def get_hello_you (): payload = json . dumps ({ \"message\" : \"I'm a teapot\" }) custom_headers = { \"X-Custom\" : \"X-Value\" } return Response ( status_code = 418 , content_type = \"application/json\" , body = payload , headers = custom_headers ) def lambda_handler ( event , context ): return app . resolve ( event , context ) response.json ```json { \"body\": \"{\\\"message\\\":\\\"I\\'m a teapot\\\"}\", \"headers\": { \"Content-Type\": \"application/json\", \"X-Custom\": \"X-Value\" }, \"isBase64Encoded\": false, \"statusCode\": 418 } Compress \u00b6 You can compress with gzip and base64 encode your responses via compress parameter. Warning The client must send the Accept-Encoding header, otherwise a normal response will be sent. app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver app = ApiGatewayResolver () @app . get ( \"/hello\" , compress = True ) def get_hello_you (): return { \"message\" : \"hello universe\" } def lambda_handler ( event , context ): return app . resolve ( event , context ) sample_request.json 1 2 3 4 5 6 7 8 { \"headers\" : { \"Accept-Encoding\" : \"gzip\" }, \"httpMethod\" : \"GET\" , \"path\" : \"/hello\" , ... } response.json 1 2 3 4 5 6 7 8 9 { \"body\" : \"H4sIAAAAAAACE6tWyk0tLk5MT1WyUspIzcnJVyjNyyxLLSpOVaoFANha8kEcAAAA\" , \"headers\" : { \"Content-Encoding\" : \"gzip\" , \"Content-Type\" : \"application/json\" }, \"isBase64Encoded\" : true , \"statusCode\" : 200 } Binary responses \u00b6 For convenience, we automatically base64 encode binary responses. You can also use in combination with compress parameter if your client supports gzip. Like compress feature, the client must send the Accept header with the correct media type. Warning This feature requires API Gateway to configure binary media types, see our sample infrastructure for reference. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import os from pathlib import Path from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver , Response app = ApiGatewayResolver () logo_file : bytes = Path ( os . getenv ( \"LAMBDA_TASK_ROOT\" ) + \"/logo.svg\" ) . read_bytes () @app . get ( \"/logo\" ) def get_logo (): return Response ( status_code = 200 , content_type = \"image/svg+xml\" , body = logo_file ) def lambda_handler ( event , context ): return app . resolve ( event , context ) logo.svg 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 <?xml version=\"1.0\" encoding=\"utf-8\"?> <!-- Generator: Adobe Illustrator 19.0.1, SVG Export Plug-In . SVG Version: 6.00 Build 0) --> <svg version= \"1.1\" id= \"Layer_1\" xmlns= \"http://www.w3.org/2000/svg\" xmlns:xlink= \"http://www.w3.org/1999/xlink\" x= \"0px\" y= \"0px\" viewBox= \"0 0 304 182\" style= \"enable-background:new 0 0 304 182;\" xml:space= \"preserve\" > <style type= \"text/css\" > .st0{fill:#FFFFFF;} .st1{fill-rule:evenodd;clip-rule:evenodd;fill:#FFFFFF;} </style> <g> <path class= \"st0\" d= \"M86.4,66.4c0,3.7,0.4,6.7,1.1,8.9c0.8,2.2,1.8,4.6,3.2,7.2c0.5,0.8,0.7,1.6,0.7,2.3c0,1-0.6,2-1.9,3l-6.3,4.2 c-0.9,0.6-1.8,0.9-2.6,0.9c-1,0-2-0.5-3-1.4C76.2,90,75,88.4,74,86.8c-1-1.7-2-3.6-3.1-5.9c-7.8,9.2-17.6,13.8-29.4,13.8 c-8.4,0-15.1-2.4-20-7.2c-4.9-4.8-7.4-11.2-7.4-19.2c0-8.5,3-15.4,9.1-20.6c6.1-5.2,14.2-7.8,24.5-7.8c3.4,0,6.9,0.3,10.6,0.8 c3.7,0.5,7.5,1.3,11.5,2.2v-7.3c0-7.6-1.6-12.9-4.7-16c-3.2-3.1-8.6-4.6-16.3-4.6c-3.5,0-7.1,0.4-10.8,1.3c-3.7,0.9-7.3,2-10.8,3.4 c-1.6,0.7-2.8,1.1-3.5,1.3c-0.7,0.2-1.2,0.3-1.6,0.3c-1.4,0-2.1-1-2.1-3.1v-4.9c0-1.6,0.2-2.8,0.7-3.5c0.5-0.7,1.4-1.4,2.8-2.1 c3.5-1.8,7.7-3.3,12.6-4.5c4.9-1.3,10.1-1.9,15.6-1.9c11.9,0,20.6,2.7,26.2,8.1c5.5,5.4,8.3,13.6,8.3,24.6V66.4z M45.8,81.6 c3.3,0,6.7-0.6,10.3-1.8c3.6-1.2,6.8-3.4,9.5-6.4c1.6-1.9,2.8-4,3.4-6.4c0.6-2.4,1-5.3,1-8.7v-4.2c-2.9-0.7-6-1.3-9.2-1.7 c-3.2-0.4-6.3-0.6-9.4-0.6c-6.7,0-11.6,1.3-14.9,4c-3.3,2.7-4.9,6.5-4.9,11.5c0,4.7,1.2,8.2,3.7,10.6 C37.7,80.4,41.2,81.6,45.8,81.6z M126.1,92.4c-1.8,0-3-0.3-3.8-1c-0.8-0.6-1.5-2-2.1-3.9L96.7,10.2c-0.6-2-0.9-3.3-0.9-4 c0-1.6,0.8-2.5,2.4-2.5h9.8c1.9,0,3.2,0.3,3.9,1c0.8,0.6,1.4,2,2,3.9l16.8,66.2l15.6-66.2c0.5-2,1.1-3.3,1.9-3.9c0.8-0.6,2.2-1,4-1 h8c1.9,0,3.2,0.3,4,1c0.8,0.6,1.5,2,1.9,3.9l15.8,67l17.3-67c0.6-2,1.3-3.3,2-3.9c0.8-0.6,2.1-1,3.9-1h9.3c1.6,0,2.5,0.8,2.5,2.5 c0,0.5-0.1,1-0.2,1.6c-0.1,0.6-0.3,1.4-0.7,2.5l-24.1,77.3c-0.6,2-1.3,3.3-2.1,3.9c-0.8,0.6-2.1,1-3.8,1h-8.6c-1.9,0-3.2-0.3-4-1 c-0.8-0.7-1.5-2-1.9-4L156,23l-15.4,64.4c-0.5,2-1.1,3.3-1.9,4c-0.8,0.7-2.2,1-4,1H126.1z M254.6,95.1c-5.2,0-10.4-0.6-15.4-1.8 c-5-1.2-8.9-2.5-11.5-4c-1.6-0.9-2.7-1.9-3.1-2.8c-0.4-0.9-0.6-1.9-0.6-2.8v-5.1c0-2.1,0.8-3.1,2.3-3.1c0.6,0,1.2,0.1,1.8,0.3 c0.6,0.2,1.5,0.6,2.5,1c3.4,1.5,7.1,2.7,11,3.5c4,0.8,7.9,1.2,11.9,1.2c6.3,0,11.2-1.1,14.6-3.3c3.4-2.2,5.2-5.4,5.2-9.5 c0-2.8-0.9-5.1-2.7-7c-1.8-1.9-5.2-3.6-10.1-5.2L246,52c-7.3-2.3-12.7-5.7-16-10.2c-3.3-4.4-5-9.3-5-14.5c0-4.2,0.9-7.9,2.7-11.1 c1.8-3.2,4.2-6,7.2-8.2c3-2.3,6.4-4,10.4-5.2c4-1.2,8.2-1.7,12.6-1.7c2.2,0,4.5,0.1,6.7,0.4c2.3,0.3,4.4,0.7,6.5,1.1 c2,0.5,3.9,1,5.7,1.6c1.8,0.6,3.2,1.2,4.2,1.8c1.4,0.8,2.4,1.6,3,2.5c0.6,0.8,0.9,1.9,0.9,3.3v4.7c0,2.1-0.8,3.2-2.3,3.2 c-0.8,0-2.1-0.4-3.8-1.2c-5.7-2.6-12.1-3.9-19.2-3.9c-5.7,0-10.2,0.9-13.3,2.8c-3.1,1.9-4.7,4.8-4.7,8.9c0,2.8,1,5.2,3,7.1 c2,1.9,5.7,3.8,11,5.5l14.2,4.5c7.2,2.3,12.4,5.5,15.5,9.6c3.1,4.1,4.6,8.8,4.6,14c0,4.3-0.9,8.2-2.6,11.6 c-1.8,3.4-4.2,6.4-7.3,8.8c-3.1,2.5-6.8,4.3-11.1,5.6C264.4,94.4,259.7,95.1,254.6,95.1z\" /> <g> <path class= \"st1\" d= \"M273.5,143.7c-32.9,24.3-80.7,37.2-121.8,37.2c-57.6,0-109.5-21.3-148.7-56.7c-3.1-2.8-0.3-6.6,3.4-4.4 c42.4,24.6,94.7,39.5,148.8,39.5c36.5,0,76.6-7.6,113.5-23.2C274.2,133.6,278.9,139.7,273.5,143.7z\" /> <path class= \"st1\" d= \"M287.2,128.1c-4.2-5.4-27.8-2.6-38.5-1.3c-3.2,0.4-3.7-2.4-0.8-4.5c18.8-13.2,49.7-9.4,53.3-5 c3.6,4.5-1,35.4-18.6,50.2c-2.7,2.3-5.3,1.1-4.1-1.9C282.5,155.7,291.4,133.4,287.2,128.1z\" /> </g> </g> </svg> sample_request.json 1 2 3 4 5 6 7 8 { \"headers\" : { \"Accept\" : \"image/svg+xml\" }, \"httpMethod\" : \"GET\" , \"path\" : \"/logo\" , ... } response.json 1 2 3 4 5 6 7 8 { \"body\" : \"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\" , \"headers\" : { \"Content-Type\" : \"image/svg+xml\" }, \"isBase64Encoded\" : true , \"statusCode\" : 200 } Debug mode \u00b6 You can enable debug mode via debug param, or via POWERTOOLS_EVENT_HANDLER_DEBUG environment variable . This will enable full tracebacks errors in the response, print request and responses, and set CORS in development mode. Danger This might reveal sensitive information in your logs and relax CORS restrictions, use it sparingly. Enabling debug mode 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver app = ApiGatewayResolver ( debug = True ) @app . get ( \"/hello\" ) def get_hello_universe (): return { \"message\" : \"hello universe\" } def lambda_handler ( event , context ): return app . resolve ( event , context ) Custom serializer \u00b6 You can instruct API Gateway handler to use a custom serializer to best suit your needs, for example take into account Enums when serializing. Using a custom JSON serializer for responses 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import json from enum import Enum from json import JSONEncoder from typing import Dict class CustomEncoder ( JSONEncoder ): \"\"\"Your customer json encoder\"\"\" def default ( self , obj ): if isinstance ( obj , Enum ): return obj . value try : iterable = iter ( obj ) except TypeError : pass else : return sorted ( iterable ) return JSONEncoder . default ( self , obj ) def custom_serializer ( obj ) -> str : \"\"\"Your custom serializer function ApiGatewayResolver will use\"\"\" return json . dumps ( obj , cls = CustomEncoder ) # Assigning your custom serializer app = ApiGatewayResolver ( serializer = custom_serializer ) class Color ( Enum ): RED = 1 BLUE = 2 @app . get ( \"/colors\" ) def get_color () -> Dict : return { # Color.RED will be serialized to 1 as expected now \"color\" : Color . RED , \"variations\" : { \"light\" , \"dark\" }, } Split routes with Router \u00b6 As you grow the number of routes a given Lambda function should handle, it is natural to split routes into separate files to ease maintenance - That's where the Router feature is useful. Let's assume you have app.py as your Lambda function entrypoint and routes in users.py , this is how you'd use the Router feature. users.py We import Router instead of ApiGatewayResolver ; syntax wise is exactly the same. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import itertools from typing import Dict from aws_lambda_powertools import Logger from aws_lambda_powertools.event_handler.api_gateway import Router logger = Logger ( child = True ) router = Router () USERS = { \"user1\" : \"details_here\" , \"user2\" : \"details_here\" , \"user3\" : \"details_here\" } @router . get ( \"/users\" ) def get_users () -> Dict : # /users?limit=1 pagination_limit = router . current_event . get_query_string_value ( name = \"limit\" , default_value = 10 ) logger . info ( f \"Fetching the first { pagination_limit } users...\" ) ret = dict ( itertools . islice ( USERS . items (), int ( pagination_limit ))) return { \"items\" : [ ret ]} @router . get ( \"/users/<username>\" ) def get_user ( username : str ) -> Dict : logger . info ( f \"Fetching username { username } \" ) return { \"details\" : USERS . get ( username , {})} # many other related /users routing app.py We use include_router method and include all user routers registered in the router global object. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import Dict from aws_lambda_powertools import Logger from aws_lambda_powertools.event_handler import ApiGatewayResolver from aws_lambda_powertools.utilities.typing import LambdaContext import users logger = Logger () app = ApiGatewayResolver () app . include_router ( users . router ) def lambda_handler ( event : Dict , context : LambdaContext ): return app . resolve ( event , context ) Route prefix \u00b6 In the previous example, users.py routes had a /users prefix. This might grow over time and become repetitive. When necessary, you can set a prefix when including a router object. This means you could remove /users prefix in users.py altogether. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import Dict from aws_lambda_powertools.event_handler import ApiGatewayResolver from aws_lambda_powertools.utilities.typing import LambdaContext import users app = ApiGatewayResolver () app . include_router ( users . router , prefix = \"/users\" ) # prefix '/users' to any route in `users.router` def lambda_handler ( event : Dict , context : LambdaContext ): return app . resolve ( event , context ) users.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from typing import Dict from aws_lambda_powertools import Logger from aws_lambda_powertools.event_handler.api_gateway import Router logger = Logger ( child = True ) router = Router () USERS = { \"user1\" : \"details\" , \"user2\" : \"details\" , \"user3\" : \"details\" } @router . get ( \"/\" ) # /users, when we set the prefix in app.py def get_users () -> Dict : ... @router . get ( \"/<username>\" ) def get_user ( username : str ) -> Dict : ... # many other related /users routing Sample layout \u00b6 This sample project contains a Users function with two distinct set of routes, /users and /health . The layout optimizes for code sharing, no custom build tooling, and it uses Lambda Layers to install Lambda Powertools. Project layout 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 . \u251c\u2500\u2500 Pipfile # project app & dev dependencies; poetry, pipenv, etc. \u251c\u2500\u2500 Pipfile . lock \u251c\u2500\u2500 README . md \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 __init__ . py \u2502 \u251c\u2500\u2500 requirements . txt # sam build detect it automatically due to CodeUri: src, e.g. pipenv lock -r > src/requirements.txt \u2502 \u2514\u2500\u2500 users \u2502 \u251c\u2500\u2500 __init__ . py \u2502 \u251c\u2500\u2500 main . py # this will be our users Lambda fn; it could be split in folders if we want separate fns same code base \u2502 \u2514\u2500\u2500 routers # routers module \u2502 \u251c\u2500\u2500 __init__ . py \u2502 \u251c\u2500\u2500 health . py # /users routes, e.g. from routers import users; users.router \u2502 \u2514\u2500\u2500 users . py # /users routes, e.g. from .routers import users; users.router \u251c\u2500\u2500 template . yml # SAM template.yml, CodeUri: src, Handler: users.main.lambda_handler \u2514\u2500\u2500 tests \u251c\u2500\u2500 __init__ . py \u251c\u2500\u2500 unit \u2502 \u251c\u2500\u2500 __init__ . py \u2502 \u2514\u2500\u2500 test_users . py # unit tests for the users router \u2502 \u2514\u2500\u2500 test_health . py # unit tests for the health router \u2514\u2500\u2500 functional \u251c\u2500\u2500 __init__ . py \u251c\u2500\u2500 conftest . py # pytest fixtures for the functional tests \u2514\u2500\u2500 test_main . py # functional tests for the main lambda handler template.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 AWSTemplateFormatVersion : '2010-09-09' Transform : AWS::Serverless-2016-10-31 Description : Example service with multiple routes Globals : Function : Timeout : 10 MemorySize : 512 Runtime : python3.9 Tracing : Active Architectures : - x86_64 Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_LOGGER_LOG_EVENT : true POWERTOOLS_METRICS_NAMESPACE : MyServerlessApplication POWERTOOLS_SERVICE_NAME : users Resources : UsersService : Type : AWS::Serverless::Function Properties : Handler : users.main.lambda_handler CodeUri : src Layers : # Latest version: https://awslabs.github.io/aws-lambda-powertools-python/latest/#lambda-layer - !Sub arn:aws:lambda:${AWS::Region}:017000801446:layer:AWSLambdaPowertoolsPython:4 Events : ByUser : Type : Api Properties : Path : /users/{name} Method : GET AllUsers : Type : Api Properties : Path : /users Method : GET HealthCheck : Type : Api Properties : Path : /status Method : GET Outputs : UsersApiEndpoint : Description : \"API Gateway endpoint URL for Prod environment for Users Function\" Value : !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod\" AllUsersURL : Description : \"URL to fetch all registered users\" Value : !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/users\" ByUserURL : Description : \"URL to retrieve details by user\" Value : !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/users/test\" UsersServiceFunctionArn : Description : \"Users Lambda Function ARN\" Value : !GetAtt UsersService.Arn src/users/main.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from typing import Dict from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.event_handler import ApiGatewayResolver from aws_lambda_powertools.event_handler.api_gateway import ProxyEventType from aws_lambda_powertools.logging.correlation_paths import APPLICATION_LOAD_BALANCER from aws_lambda_powertools.utilities.typing import LambdaContext from .routers import health , users tracer = Tracer () logger = Logger () app = ApiGatewayResolver ( proxy_type = ProxyEventType . APIGatewayProxyEvent ) app . include_router ( health . router ) app . include_router ( users . router ) @logger . inject_lambda_context ( correlation_id_path = API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event : Dict , context : LambdaContext ): return app . resolve ( event , context ) src/users/routers/health.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import Dict from aws_lambda_powertools import Logger from aws_lambda_powertools.event_handler.api_gateway import Router router = Router () logger = Logger ( child = True ) @router . get ( \"/status\" ) def health () -> Dict : logger . debug ( \"Health check called\" ) return { \"status\" : \"OK\" } tests/functional/test_users.py 1 2 3 4 5 6 7 8 9 10 11 import json from src.users import main # follows namespace package from root def test_lambda_handler ( apigw_event , lambda_context ): ret = main . lambda_handler ( apigw_event , lambda_context ) expected = json . dumps ({ \"message\" : \"hello universe\" }, separators = ( \",\" , \":\" )) assert ret [ \"statusCode\" ] == 200 assert ret [ \"body\" ] == expected Considerations \u00b6 This utility is optimized for fast startup, minimal feature set, and to quickly on-board customers familiar with frameworks like Flask \u2014 it's not meant to be a fully fledged framework. Event Handler naturally leads to a single Lambda function handling multiple routes for a given service, which can be eventually broken into multiple functions. Both single (monolithic) and multiple functions (micro) offer different set of trade-offs worth knowing. Tip TL;DR. Start with a monolithic function, add additional functions with new handlers, and possibly break into micro functions if necessary. Monolithic function \u00b6 A monolithic function means that your final code artifact will be deployed to a single function. This is generally the best approach to start. Benefits Code reuse . It's easier to reason about your service, modularize it and reuse code as it grows. Eventually, it can be turned into a standalone library. No custom tooling . Monolithic functions are treated just like normal Python packages; no upfront investment in tooling. Faster deployment and debugging . Whether you use all-at-once, linear, or canary deployments, a monolithic function is a single deployable unit. IDEs like PyCharm and VSCode have tooling to quickly profile, visualize, and step through debug any Python package. Downsides Cold starts . Frequent deployments and/or high load can diminish the benefit of monolithic functions depending on your latency requirements, due to Lambda scaling model . Always load test to pragmatically balance between your customer experience and development cognitive load. Granular security permissions . The micro function approach enables you to use fine-grained permissions & access controls, separate external dependencies & code signing at the function level. Conversely, you could have multiple functions while duplicating the final code artifact in a monolithic approach. Regardless, least privilege can be applied to either approaches. Higher risk per deployment . A misconfiguration or invalid import can cause disruption if not caught earlier in automated testing. Multiple functions can mitigate misconfigurations but they would still share the same code artifact. You can further minimize risks with multiple environments in your CI/CD pipeline. Micro function \u00b6 A micro function means that your final code artifact will be different to each function deployed. This is generally the approach to start if you're looking for fine-grain control and/or high load on certain parts of your service. Benefits Granular scaling . A micro function can benefit from the Lambda scaling model to scale differently depending on each part of your application. Concurrency controls and provisioned concurrency can also be used at a granular level for capacity management. Discoverability . Micro functions are easier do visualize when using distributed tracing. Their high-level architectures can be self-explanatory, and complexity is highly visible \u2014 assuming each function is named to the business purpose it serves. Package size . An independent function can be significant smaller (KB vs MB) depending on external dependencies it require to perform its purpose. Conversely, a monolithic approach can benefit from Lambda Layers to optimize builds for external dependencies. Downsides Upfront investment . Python ecosystem doesn't use a bundler \u2014 you need a custom build tooling to ensure each function only has what it needs and account for C bindings for runtime compatibility . Operations become more elaborate \u2014 you need to standardize tracing labels/annotations, structured logging, and metrics to pinpoint root causes. Engineering discipline is necessary for both approaches. Micro-function approach however requires further attention in consistency as the number of functions grow, just like any distributed system. Harder to share code . Shared code must be carefully evaluated to avoid unnecessary deployments when that changes. Equally, if shared code isn't a library, your development, building, deployment tooling need to accommodate the distinct layout. Slower safe deployments . Safely deploying multiple functions require coordination \u2014 AWS CodeDeploy deploys and verifies each function sequentially. This increases lead time substantially (minutes to hours) depending on the deployment strategy you choose. You can mitigate it by selectively enabling it in prod-like environments only, and where the risk profile is applicable. Automated testing, operational and security reviews are essential to stability in either approaches. Testing your code \u00b6 You can test your routes by passing a proxy event request where path and httpMethod . test_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from dataclasses import dataclass import pytest import app @pytest . fixture def lambda_context (): @dataclass class LambdaContext : function_name : str = \"test\" memory_limit_in_mb : int = 128 invoked_function_arn : str = \"arn:aws:lambda:eu-west-1:809313241:function:test\" aws_request_id : str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\" return LambdaContext () def test_lambda_handler ( lambda_context ): minimal_event = { \"path\" : \"/hello\" , \"httpMethod\" : \"GET\" \"requestContext\" : { # correlation ID \"requestId\" : \"c6af9ac6-7b61-11e6-9a41-93e8deadbeef\" } } app . lambda_handler ( minimal_event , lambda_context ) app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver logger = Logger () app = ApiGatewayResolver () # by default API Gateway REST API (v1) @app . get ( \"/hello\" ) def get_hello_universe (): return { \"message\" : \"hello universe\" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) def lambda_handler ( event , context ): return app . resolve ( event , context ) FAQ \u00b6 What's the difference between this utility and frameworks like Chalice? Chalice is a full featured microframework that manages application and infrastructure. This utility, however, is largely focused on routing to reduce boilerplate and expects you to setup and manage infrastructure with your framework of choice. That said, Chalice has native integration with Lambda Powertools if you're looking for a more opinionated and web framework feature set.","title":"REST API"},{"location":"core/event_handler/api_gateway/#key-features","text":"Lightweight routing to reduce boilerplate for API Gateway REST/HTTP API and ALB Seamless support for CORS, binary and Gzip compression Integrates with Data classes utilities to easily access event and identity information Built-in support for Decimals JSON encoding Support for dynamic path expressions Router to allow for splitting up the handler accross multiple files","title":"Key Features"},{"location":"core/event_handler/api_gateway/#getting-started","text":"","title":"Getting started"},{"location":"core/event_handler/api_gateway/#required-resources","text":"You must have an existing API Gateway Proxy integration or ALB configured to invoke your Lambda function. There is no additional permissions or dependencies required to use this utility. This is the sample infrastructure for API Gateway we are using for the examples in this documentation. AWS Serverless Application Model (SAM) example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 AWSTemplateFormatVersion : '2010-09-09' Transform : AWS::Serverless-2016-10-31 Description : Hello world event handler API Gateway Globals : Api : TracingEnabled : true Cors : # see CORS section AllowOrigin : \"'https://example.com'\" AllowHeaders : \"'Content-Type,Authorization,X-Amz-Date'\" MaxAge : \"'300'\" BinaryMediaTypes : # see Binary responses section - '*~1*' # converts to */* for any binary type Function : Timeout : 5 Runtime : python3.8 Tracing : Active Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_LOGGER_SAMPLE_RATE : 0.1 POWERTOOLS_LOGGER_LOG_EVENT : true POWERTOOLS_METRICS_NAMESPACE : MyServerlessApplication POWERTOOLS_SERVICE_NAME : my_api-service Resources : ApiFunction : Type : AWS::Serverless::Function Properties : Handler : app.lambda_handler CodeUri : api_handler/ Description : API handler function Events : ApiEvent : Type : Api Properties : Path : /{proxy+} # Send requests on any path to the lambda function Method : ANY # Send requests using any http method to the lambda function","title":"Required resources"},{"location":"core/event_handler/api_gateway/#api-gateway-decorator","text":"You can define your functions to match a path and HTTP method, when you use the decorator ApiGatewayResolver . Here's an example where we have two separate functions to resolve two paths: /hello . Info We automatically serialize Dict responses as JSON, trim whitespaces for compact responses, and set content-type to application/json . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver tracer = Tracer () logger = Logger () app = ApiGatewayResolver () # by default API Gateway REST API (v1) @app . get ( \"/hello\" ) @tracer . capture_method def get_hello_universe (): return { \"message\" : \"hello universe\" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) hello_event.json This utility uses path and httpMethod to route to the right function. This helps make unit tests and local invocation easier too. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 { \"body\" : \"hello\" , \"resource\" : \"/hello\" , \"path\" : \"/hello\" , \"httpMethod\" : \"GET\" , \"isBase64Encoded\" : false , \"queryStringParameters\" : { \"foo\" : \"bar\" }, \"multiValueQueryStringParameters\" : {}, \"pathParameters\" : { \"hello\" : \"/hello\" }, \"stageVariables\" : {}, \"headers\" : { \"Accept\" : \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\" , \"Accept-Encoding\" : \"gzip, deflate, sdch\" , \"Accept-Language\" : \"en-US,en;q=0.8\" , \"Cache-Control\" : \"max-age=0\" , \"CloudFront-Forwarded-Proto\" : \"https\" , \"CloudFront-Is-Desktop-Viewer\" : \"true\" , \"CloudFront-Is-Mobile-Viewer\" : \"false\" , \"CloudFront-Is-SmartTV-Viewer\" : \"false\" , \"CloudFront-Is-Tablet-Viewer\" : \"false\" , \"CloudFront-Viewer-Country\" : \"US\" , \"Host\" : \"1234567890.execute-api.us-east-1.amazonaws.com\" , \"Upgrade-Insecure-Requests\" : \"1\" , \"User-Agent\" : \"Custom User Agent String\" , \"Via\" : \"1.1 08f323deadbeefa7af34d5feb414ce27.cloudfront.net (CloudFront)\" , \"X-Amz-Cf-Id\" : \"cDehVQoZnx43VYQb9j2-nvCh-9z396Uhbp027Y2JvkCPNLmGJHqlaA==\" , \"X-Forwarded-For\" : \"127.0.0.1, 127.0.0.2\" , \"X-Forwarded-Port\" : \"443\" , \"X-Forwarded-Proto\" : \"https\" }, \"multiValueHeaders\" : {}, \"requestContext\" : { \"accountId\" : \"123456789012\" , \"resourceId\" : \"123456\" , \"stage\" : \"Prod\" , \"requestId\" : \"c6af9ac6-7b61-11e6-9a41-93e8deadbeef\" , \"requestTime\" : \"25/Jul/2020:12:34:56 +0000\" , \"requestTimeEpoch\" : 1428582896000 , \"identity\" : { \"cognitoIdentityPoolId\" : null , \"accountId\" : null , \"cognitoIdentityId\" : null , \"caller\" : null , \"accessKey\" : null , \"sourceIp\" : \"127.0.0.1\" , \"cognitoAuthenticationType\" : null , \"cognitoAuthenticationProvider\" : null , \"userArn\" : null , \"userAgent\" : \"Custom User Agent String\" , \"user\" : null }, \"path\" : \"/Prod/hello\" , \"resourcePath\" : \"/hello\" , \"httpMethod\" : \"POST\" , \"apiId\" : \"1234567890\" , \"protocol\" : \"HTTP/1.1\" } } response.json 1 2 3 4 5 6 7 8 { \"statusCode\" : 200 , \"headers\" : { \"Content-Type\" : \"application/json\" }, \"body\" : \"{\\\"message\\\":\\\"hello universe\\\"}\" , \"isBase64Encoded\" : false }","title":"API Gateway decorator"},{"location":"core/event_handler/api_gateway/#http-api","text":"When using API Gateway HTTP API to front your Lambda functions, you can instruct ApiGatewayResolver to conform with their contract via proxy_type param: Using HTTP API resolver 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver , ProxyEventType tracer = Tracer () logger = Logger () app = ApiGatewayResolver ( proxy_type = ProxyEventType . APIGatewayProxyEventV2 ) @app . get ( \"/hello\" ) @tracer . capture_method def get_hello_universe (): return { \"message\" : \"hello universe\" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_HTTP ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context )","title":"HTTP API"},{"location":"core/event_handler/api_gateway/#alb","text":"When using ALB to front your Lambda functions, you can instruct ApiGatewayResolver to conform with their contract via proxy_type param: Using ALB resolver 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver , ProxyEventType tracer = Tracer () logger = Logger () app = ApiGatewayResolver ( proxy_type = ProxyEventType . ALBEvent ) @app . get ( \"/hello\" ) @tracer . capture_method def get_hello_universe (): return { \"message\" : \"hello universe\" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPLICATION_LOAD_BALANCER ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context )","title":"ALB"},{"location":"core/event_handler/api_gateway/#dynamic-routes","text":"You can use /path/{dynamic_value} when configuring dynamic URL paths. This allows you to define such dynamic value as part of your function signature. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver tracer = Tracer () logger = Logger () app = ApiGatewayResolver () @app . get ( \"/hello/<name>\" ) @tracer . capture_method def get_hello_you ( name ): return { \"message\" : f \"hello { name } \" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) sample_request.json 1 2 3 4 5 6 { \"resource\" : \"/hello/{name}\" , \"path\" : \"/hello/lessa\" , \"httpMethod\" : \"GET\" , ... }","title":"Dynamic routes"},{"location":"core/event_handler/api_gateway/#nested-routes","text":"You can also nest paths as configured earlier in our sample infrastructure : /{message}/{name} . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver tracer = Tracer () logger = Logger () app = ApiGatewayResolver () @app . get ( \"/<message>/<name>\" ) @tracer . capture_method def get_message ( message , name ): return { \"message\" : f \" { message } , { name }} \" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) sample_request.json 1 2 3 4 5 6 { \"resource\" : \"/{message}/{name}\" , \"path\" : \"/hi/michael\" , \"httpMethod\" : \"GET\" , ... }","title":"Nested routes"},{"location":"core/event_handler/api_gateway/#catch-all-routes","text":"Note We recommend having explicit routes whenever possible; use catch-all routes sparingly. You can use a regex string to handle an arbitrary number of paths within a request, for example .+ . You can also combine nested paths with greedy regex to catch in between routes. Warning We will choose the more explicit registered route that match incoming event. app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver app = ApiGatewayResolver () @app . get ( \".+\" ) def catch_any_route_after_any (): return { \"path_received\" : app . current_event . path } def lambda_handler ( event , context ): return app . resolve ( event , context ) sample_request.json 1 2 3 4 5 6 { \"resource\" : \"/any/route/should/work\" , \"path\" : \"/any/route/should/work\" , \"httpMethod\" : \"GET\" , ... }","title":"Catch-all routes"},{"location":"core/event_handler/api_gateway/#http-methods","text":"You can use named decorators to specify the HTTP method that should be handled in your functions. As well as the get method already shown above, you can use post , put , patch , delete , and patch . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver tracer = Tracer () logger = Logger () app = ApiGatewayResolver () # Only POST HTTP requests to the path /hello will route to this function @app . post ( \"/hello\" ) @tracer . capture_method def get_hello_you (): name = app . current_event . json_body . get ( \"name\" ) return { \"message\" : f \"hello { name } \" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) sample_request.json 1 2 3 4 5 6 { \"resource\" : \"/hello/{name}\" , \"path\" : \"/hello/lessa\" , \"httpMethod\" : \"GET\" , ... } If you need to accept multiple HTTP methods in a single function, you can use the route method and pass a list of HTTP methods. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver tracer = Tracer () logger = Logger () app = ApiGatewayResolver () # PUT and POST HTTP requests to the path /hello will route to this function @app . route ( \"/hello\" , method = [ \"PUT\" , \"POST\" ]) @tracer . capture_method def get_hello_you (): name = app . current_event . json_body . get ( \"name\" ) return { \"message\" : f \"hello { name } \" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) sample_request.json 1 2 3 4 5 6 { \"resource\" : \"/hello/{name}\" , \"path\" : \"/hello/lessa\" , \"httpMethod\" : \"GET\" , ... } Note It is usually better to have separate functions for each HTTP method, as the functionality tends to differ depending on which method is used.","title":"HTTP Methods"},{"location":"core/event_handler/api_gateway/#accessing-request-details","text":"By integrating with Data classes utilities , you have access to request details, Lambda context and also some convenient methods. These are made available in the response returned when instantiating ApiGatewayResolver , for example app.current_event and app.lambda_context .","title":"Accessing request details"},{"location":"core/event_handler/api_gateway/#query-strings-and-payload","text":"Within app.current_event property, you can access query strings as dictionary via query_string_parameters , or by name via get_query_string_value method. You can access the raw payload via body property, or if it's a JSON string you can quickly deserialize it via json_body property. Accessing query strings, JSON payload, and raw payload 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver app = ApiGatewayResolver () @app . get ( \"/hello\" ) def get_hello_you (): query_strings_as_dict = app . current_event . query_string_parameters json_payload = app . current_event . json_body payload = app . current_event . body name = app . current_event . get_query_string_value ( name = \"name\" , default_value = \"\" ) return { \"message\" : f \"hello { name }} \" } def lambda_handler ( event , context ): return app . resolve ( event , context )","title":"Query strings and payload"},{"location":"core/event_handler/api_gateway/#headers","text":"Similarly to Query strings , you can access headers as dictionary via app.current_event.headers , or by name via get_header_value . Accessing HTTP Headers 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver app = ApiGatewayResolver () @app . get ( \"/hello\" ) def get_hello_you (): headers_as_dict = app . current_event . headers name = app . current_event . get_header_value ( name = \"X-Name\" , default_value = \"\" ) return { \"message\" : f \"hello { name }} \" } def lambda_handler ( event , context ): return app . resolve ( event , context )","title":"Headers"},{"location":"core/event_handler/api_gateway/#handling-not-found-routes","text":"By default, we return 404 for any unmatched route. You can use not_found decorator to override this behaviour, and return a custom Response . Handling not found 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler import content_types from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver , Response from aws_lambda_powertools.event_handler.exceptions import NotFoundError tracer = Tracer () logger = Logger () app = ApiGatewayResolver () @app . not_found @tracer . capture_method def handle_not_found_errors ( exc : NotFoundError ) -> Response : # Return 418 upon 404 errors logger . info ( f \"Not found route: { app . current_event . path } \" ) return Response ( status_code = 418 , content_type = content_types . TEXT_PLAIN , body = \"I'm a teapot!\" ) @app . get ( \"/catch/me/if/you/can\" ) @tracer . capture_method def catch_me_if_you_can (): return { \"message\" : \"oh hey\" } @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context )","title":"Handling not found routes"},{"location":"core/event_handler/api_gateway/#exception-handling","text":"You can use exception_handler decorator with any Python exception. This allows you to handle a common exception outside your route, for example validation errors. Exception handling 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler import content_types from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver , Response tracer = Tracer () logger = Logger () app = ApiGatewayResolver () @app . exception_handler ( ValueError ) def handle_value_error ( ex : ValueError ): metadata = { \"path\" : app . current_event . path } logger . error ( f \"Malformed request: { ex } \" , extra = metadata ) return Response ( status_code = 400 , content_type = content_types . TEXT_PLAIN , body = \"Invalid request\" , ) @app . get ( \"/hello\" ) @tracer . capture_method def hello_name (): name = app . current_event . get_query_string_value ( name = \"name\" ) if name is not None : raise ValueError ( \"name query string must be present\" ) return { \"message\" : f \"hello { name } \" } @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context )","title":"Exception handling"},{"location":"core/event_handler/api_gateway/#raising-http-errors","text":"You can easily raise any HTTP Error back to the client using ServiceError exception. Info If you need to send custom headers, use Response class instead. Additionally, we provide pre-defined errors for the most popular ones such as HTTP 400, 401, 404, 500. Raising common HTTP Status errors (4xx, 5xx) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver from aws_lambda_powertools.event_handler.exceptions import ( BadRequestError , InternalServerError , NotFoundError , ServiceError , UnauthorizedError , ) tracer = Tracer () logger = Logger () app = ApiGatewayResolver () @app . get ( rule = \"/bad-request-error\" ) def bad_request_error (): # HTTP 400 raise BadRequestError ( \"Missing required parameter\" ) @app . get ( rule = \"/unauthorized-error\" ) def unauthorized_error (): # HTTP 401 raise UnauthorizedError ( \"Unauthorized\" ) @app . get ( rule = \"/not-found-error\" ) def not_found_error (): # HTTP 404 raise NotFoundError @app . get ( rule = \"/internal-server-error\" ) def internal_server_error (): # HTTP 500 raise InternalServerError ( \"Internal server error\" ) @app . get ( rule = \"/service-error\" , cors = True ) def service_error (): raise ServiceError ( 502 , \"Something went wrong!\" ) # alternatively # from http import HTTPStatus # raise ServiceError(HTTPStatus.BAD_GATEWAY.value, \"Something went wrong) def handler ( event , context ): return app . resolve ( event , context )","title":"Raising HTTP errors"},{"location":"core/event_handler/api_gateway/#custom-domain-api-mappings","text":"When using Custom Domain API Mappings feature, you must use strip_prefixes param in the ApiGatewayResolver constructor. Scenario: You have a custom domain api.mydomain.dev and set an API Mapping payment to forward requests to your Payments API, the path argument will be /payment/<your_actual_path> . This will lead to a HTTP 404 despite having your Lambda configured correctly. See the example below on how to account for this change. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver tracer = Tracer () logger = Logger () app = ApiGatewayResolver ( strip_prefixes = [ \"/payment\" ]) @app . get ( \"/subscriptions/<subscription>\" ) @tracer . capture_method def get_subscription ( subscription ): return { \"subscription_id\" : subscription } @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) sample_request.json 1 2 3 4 5 6 { \"resource\" : \"/subscriptions/{subscription}\" , \"path\" : \"/payment/subscriptions/123\" , \"httpMethod\" : \"GET\" , ... } Note After removing a path prefix with strip_prefixes , the new root path will automatically be mapped to the path argument of / . For example, when using strip_prefixes value of /pay , there is no difference between a request path of /pay and /pay/ ; and the path argument would be defined as / .","title":"Custom Domain API Mappings"},{"location":"core/event_handler/api_gateway/#advanced","text":"","title":"Advanced"},{"location":"core/event_handler/api_gateway/#cors","text":"You can configure CORS at the ApiGatewayResolver constructor via cors parameter using the CORSConfig class. This will ensure that CORS headers are always returned as part of the response when your functions match the path invoked. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver , CORSConfig tracer = Tracer () logger = Logger () cors_config = CORSConfig ( allow_origin = \"https://example.com\" , max_age = 300 ) app = ApiGatewayResolver ( cors = cors_config ) @app . get ( \"/hello/<name>\" ) @tracer . capture_method def get_hello_you ( name ): return { \"message\" : f \"hello { name } \" } @app . get ( \"/hello\" , cors = False ) # optionally exclude CORS from response, if needed @tracer . capture_method def get_hello_no_cors_needed (): return { \"message\" : \"hello, no CORS needed for this path ;)\" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) response.json 1 2 3 4 5 6 7 8 9 10 { \"statusCode\" : 200 , \"headers\" : { \"Content-Type\" : \"application/json\" , \"Access-Control-Allow-Origin\" : \"https://www.example.com\" , \"Access-Control-Allow-Headers\" : \"Authorization,Content-Type,X-Amz-Date,X-Amz-Security-Token,X-Api-Key\" }, \"body\" : \"{\\\"message\\\":\\\"hello lessa\\\"}\" , \"isBase64Encoded\" : false } response_no_cors.json 1 2 3 4 5 6 7 8 { \"statusCode\" : 200 , \"headers\" : { \"Content-Type\" : \"application/json\" }, \"body\" : \"{\\\"message\\\":\\\"hello lessa\\\"}\" , \"isBase64Encoded\" : false } Tip Optionally disable CORS on a per path basis with cors=False parameter.","title":"CORS"},{"location":"core/event_handler/api_gateway/#pre-flight","text":"Pre-flight (OPTIONS) calls are typically handled at the API Gateway level as per our sample infrastructure , no Lambda integration necessary. However, ALB expects you to handle pre-flight requests. For convenience, we automatically handle that for you as long as you setup CORS in the constructor level .","title":"Pre-flight"},{"location":"core/event_handler/api_gateway/#defaults","text":"For convenience, these are the default values when using CORSConfig to enable CORS: Warning Always configure allow_origin when using in production. Key Value Note allow_origin : str * Only use the default value for development. Never use * for production unless your use case requires it allow_headers : List[str] [Authorization, Content-Type, X-Amz-Date, X-Api-Key, X-Amz-Security-Token] Additional headers will be appended to the default list for your convenience expose_headers : List[str] [] Any additional header beyond the safe listed by CORS specification . max_age : int `` Only for pre-flight requests if you choose to have your function to handle it instead of API Gateway allow_credentials : bool False Only necessary when you need to expose cookies, authorization headers or TLS client certificates.","title":"Defaults"},{"location":"core/event_handler/api_gateway/#fine-grained-responses","text":"You can use the Response class to have full control over the response, for example you might want to add additional headers or set a custom Content-type. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver , Response app = ApiGatewayResolver () @app . get ( \"/hello\" ) def get_hello_you (): payload = json . dumps ({ \"message\" : \"I'm a teapot\" }) custom_headers = { \"X-Custom\" : \"X-Value\" } return Response ( status_code = 418 , content_type = \"application/json\" , body = payload , headers = custom_headers ) def lambda_handler ( event , context ): return app . resolve ( event , context ) response.json ```json { \"body\": \"{\\\"message\\\":\\\"I\\'m a teapot\\\"}\", \"headers\": { \"Content-Type\": \"application/json\", \"X-Custom\": \"X-Value\" }, \"isBase64Encoded\": false, \"statusCode\": 418 }","title":"Fine grained responses"},{"location":"core/event_handler/api_gateway/#compress","text":"You can compress with gzip and base64 encode your responses via compress parameter. Warning The client must send the Accept-Encoding header, otherwise a normal response will be sent. app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver app = ApiGatewayResolver () @app . get ( \"/hello\" , compress = True ) def get_hello_you (): return { \"message\" : \"hello universe\" } def lambda_handler ( event , context ): return app . resolve ( event , context ) sample_request.json 1 2 3 4 5 6 7 8 { \"headers\" : { \"Accept-Encoding\" : \"gzip\" }, \"httpMethod\" : \"GET\" , \"path\" : \"/hello\" , ... } response.json 1 2 3 4 5 6 7 8 9 { \"body\" : \"H4sIAAAAAAACE6tWyk0tLk5MT1WyUspIzcnJVyjNyyxLLSpOVaoFANha8kEcAAAA\" , \"headers\" : { \"Content-Encoding\" : \"gzip\" , \"Content-Type\" : \"application/json\" }, \"isBase64Encoded\" : true , \"statusCode\" : 200 }","title":"Compress"},{"location":"core/event_handler/api_gateway/#binary-responses","text":"For convenience, we automatically base64 encode binary responses. You can also use in combination with compress parameter if your client supports gzip. Like compress feature, the client must send the Accept header with the correct media type. Warning This feature requires API Gateway to configure binary media types, see our sample infrastructure for reference. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import os from pathlib import Path from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver , Response app = ApiGatewayResolver () logo_file : bytes = Path ( os . getenv ( \"LAMBDA_TASK_ROOT\" ) + \"/logo.svg\" ) . read_bytes () @app . get ( \"/logo\" ) def get_logo (): return Response ( status_code = 200 , content_type = \"image/svg+xml\" , body = logo_file ) def lambda_handler ( event , context ): return app . resolve ( event , context ) logo.svg 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 <?xml version=\"1.0\" encoding=\"utf-8\"?> <!-- Generator: Adobe Illustrator 19.0.1, SVG Export Plug-In . SVG Version: 6.00 Build 0) --> <svg version= \"1.1\" id= \"Layer_1\" xmlns= \"http://www.w3.org/2000/svg\" xmlns:xlink= \"http://www.w3.org/1999/xlink\" x= \"0px\" y= \"0px\" viewBox= \"0 0 304 182\" style= \"enable-background:new 0 0 304 182;\" xml:space= \"preserve\" > <style type= \"text/css\" > .st0{fill:#FFFFFF;} .st1{fill-rule:evenodd;clip-rule:evenodd;fill:#FFFFFF;} </style> <g> <path class= \"st0\" d= \"M86.4,66.4c0,3.7,0.4,6.7,1.1,8.9c0.8,2.2,1.8,4.6,3.2,7.2c0.5,0.8,0.7,1.6,0.7,2.3c0,1-0.6,2-1.9,3l-6.3,4.2 c-0.9,0.6-1.8,0.9-2.6,0.9c-1,0-2-0.5-3-1.4C76.2,90,75,88.4,74,86.8c-1-1.7-2-3.6-3.1-5.9c-7.8,9.2-17.6,13.8-29.4,13.8 c-8.4,0-15.1-2.4-20-7.2c-4.9-4.8-7.4-11.2-7.4-19.2c0-8.5,3-15.4,9.1-20.6c6.1-5.2,14.2-7.8,24.5-7.8c3.4,0,6.9,0.3,10.6,0.8 c3.7,0.5,7.5,1.3,11.5,2.2v-7.3c0-7.6-1.6-12.9-4.7-16c-3.2-3.1-8.6-4.6-16.3-4.6c-3.5,0-7.1,0.4-10.8,1.3c-3.7,0.9-7.3,2-10.8,3.4 c-1.6,0.7-2.8,1.1-3.5,1.3c-0.7,0.2-1.2,0.3-1.6,0.3c-1.4,0-2.1-1-2.1-3.1v-4.9c0-1.6,0.2-2.8,0.7-3.5c0.5-0.7,1.4-1.4,2.8-2.1 c3.5-1.8,7.7-3.3,12.6-4.5c4.9-1.3,10.1-1.9,15.6-1.9c11.9,0,20.6,2.7,26.2,8.1c5.5,5.4,8.3,13.6,8.3,24.6V66.4z M45.8,81.6 c3.3,0,6.7-0.6,10.3-1.8c3.6-1.2,6.8-3.4,9.5-6.4c1.6-1.9,2.8-4,3.4-6.4c0.6-2.4,1-5.3,1-8.7v-4.2c-2.9-0.7-6-1.3-9.2-1.7 c-3.2-0.4-6.3-0.6-9.4-0.6c-6.7,0-11.6,1.3-14.9,4c-3.3,2.7-4.9,6.5-4.9,11.5c0,4.7,1.2,8.2,3.7,10.6 C37.7,80.4,41.2,81.6,45.8,81.6z M126.1,92.4c-1.8,0-3-0.3-3.8-1c-0.8-0.6-1.5-2-2.1-3.9L96.7,10.2c-0.6-2-0.9-3.3-0.9-4 c0-1.6,0.8-2.5,2.4-2.5h9.8c1.9,0,3.2,0.3,3.9,1c0.8,0.6,1.4,2,2,3.9l16.8,66.2l15.6-66.2c0.5-2,1.1-3.3,1.9-3.9c0.8-0.6,2.2-1,4-1 h8c1.9,0,3.2,0.3,4,1c0.8,0.6,1.5,2,1.9,3.9l15.8,67l17.3-67c0.6-2,1.3-3.3,2-3.9c0.8-0.6,2.1-1,3.9-1h9.3c1.6,0,2.5,0.8,2.5,2.5 c0,0.5-0.1,1-0.2,1.6c-0.1,0.6-0.3,1.4-0.7,2.5l-24.1,77.3c-0.6,2-1.3,3.3-2.1,3.9c-0.8,0.6-2.1,1-3.8,1h-8.6c-1.9,0-3.2-0.3-4-1 c-0.8-0.7-1.5-2-1.9-4L156,23l-15.4,64.4c-0.5,2-1.1,3.3-1.9,4c-0.8,0.7-2.2,1-4,1H126.1z M254.6,95.1c-5.2,0-10.4-0.6-15.4-1.8 c-5-1.2-8.9-2.5-11.5-4c-1.6-0.9-2.7-1.9-3.1-2.8c-0.4-0.9-0.6-1.9-0.6-2.8v-5.1c0-2.1,0.8-3.1,2.3-3.1c0.6,0,1.2,0.1,1.8,0.3 c0.6,0.2,1.5,0.6,2.5,1c3.4,1.5,7.1,2.7,11,3.5c4,0.8,7.9,1.2,11.9,1.2c6.3,0,11.2-1.1,14.6-3.3c3.4-2.2,5.2-5.4,5.2-9.5 c0-2.8-0.9-5.1-2.7-7c-1.8-1.9-5.2-3.6-10.1-5.2L246,52c-7.3-2.3-12.7-5.7-16-10.2c-3.3-4.4-5-9.3-5-14.5c0-4.2,0.9-7.9,2.7-11.1 c1.8-3.2,4.2-6,7.2-8.2c3-2.3,6.4-4,10.4-5.2c4-1.2,8.2-1.7,12.6-1.7c2.2,0,4.5,0.1,6.7,0.4c2.3,0.3,4.4,0.7,6.5,1.1 c2,0.5,3.9,1,5.7,1.6c1.8,0.6,3.2,1.2,4.2,1.8c1.4,0.8,2.4,1.6,3,2.5c0.6,0.8,0.9,1.9,0.9,3.3v4.7c0,2.1-0.8,3.2-2.3,3.2 c-0.8,0-2.1-0.4-3.8-1.2c-5.7-2.6-12.1-3.9-19.2-3.9c-5.7,0-10.2,0.9-13.3,2.8c-3.1,1.9-4.7,4.8-4.7,8.9c0,2.8,1,5.2,3,7.1 c2,1.9,5.7,3.8,11,5.5l14.2,4.5c7.2,2.3,12.4,5.5,15.5,9.6c3.1,4.1,4.6,8.8,4.6,14c0,4.3-0.9,8.2-2.6,11.6 c-1.8,3.4-4.2,6.4-7.3,8.8c-3.1,2.5-6.8,4.3-11.1,5.6C264.4,94.4,259.7,95.1,254.6,95.1z\" /> <g> <path class= \"st1\" d= \"M273.5,143.7c-32.9,24.3-80.7,37.2-121.8,37.2c-57.6,0-109.5-21.3-148.7-56.7c-3.1-2.8-0.3-6.6,3.4-4.4 c42.4,24.6,94.7,39.5,148.8,39.5c36.5,0,76.6-7.6,113.5-23.2C274.2,133.6,278.9,139.7,273.5,143.7z\" /> <path class= \"st1\" d= \"M287.2,128.1c-4.2-5.4-27.8-2.6-38.5-1.3c-3.2,0.4-3.7-2.4-0.8-4.5c18.8-13.2,49.7-9.4,53.3-5 c3.6,4.5-1,35.4-18.6,50.2c-2.7,2.3-5.3,1.1-4.1-1.9C282.5,155.7,291.4,133.4,287.2,128.1z\" /> </g> </g> </svg> sample_request.json 1 2 3 4 5 6 7 8 { \"headers\" : { \"Accept\" : \"image/svg+xml\" }, \"httpMethod\" : \"GET\" , \"path\" : \"/logo\" , ... } response.json 1 2 3 4 5 6 7 8 { \"body\" : \"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\" , \"headers\" : { \"Content-Type\" : \"image/svg+xml\" }, \"isBase64Encoded\" : true , \"statusCode\" : 200 }","title":"Binary responses"},{"location":"core/event_handler/api_gateway/#debug-mode","text":"You can enable debug mode via debug param, or via POWERTOOLS_EVENT_HANDLER_DEBUG environment variable . This will enable full tracebacks errors in the response, print request and responses, and set CORS in development mode. Danger This might reveal sensitive information in your logs and relax CORS restrictions, use it sparingly. Enabling debug mode 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver app = ApiGatewayResolver ( debug = True ) @app . get ( \"/hello\" ) def get_hello_universe (): return { \"message\" : \"hello universe\" } def lambda_handler ( event , context ): return app . resolve ( event , context )","title":"Debug mode"},{"location":"core/event_handler/api_gateway/#custom-serializer","text":"You can instruct API Gateway handler to use a custom serializer to best suit your needs, for example take into account Enums when serializing. Using a custom JSON serializer for responses 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import json from enum import Enum from json import JSONEncoder from typing import Dict class CustomEncoder ( JSONEncoder ): \"\"\"Your customer json encoder\"\"\" def default ( self , obj ): if isinstance ( obj , Enum ): return obj . value try : iterable = iter ( obj ) except TypeError : pass else : return sorted ( iterable ) return JSONEncoder . default ( self , obj ) def custom_serializer ( obj ) -> str : \"\"\"Your custom serializer function ApiGatewayResolver will use\"\"\" return json . dumps ( obj , cls = CustomEncoder ) # Assigning your custom serializer app = ApiGatewayResolver ( serializer = custom_serializer ) class Color ( Enum ): RED = 1 BLUE = 2 @app . get ( \"/colors\" ) def get_color () -> Dict : return { # Color.RED will be serialized to 1 as expected now \"color\" : Color . RED , \"variations\" : { \"light\" , \"dark\" }, }","title":"Custom serializer"},{"location":"core/event_handler/api_gateway/#split-routes-with-router","text":"As you grow the number of routes a given Lambda function should handle, it is natural to split routes into separate files to ease maintenance - That's where the Router feature is useful. Let's assume you have app.py as your Lambda function entrypoint and routes in users.py , this is how you'd use the Router feature. users.py We import Router instead of ApiGatewayResolver ; syntax wise is exactly the same. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import itertools from typing import Dict from aws_lambda_powertools import Logger from aws_lambda_powertools.event_handler.api_gateway import Router logger = Logger ( child = True ) router = Router () USERS = { \"user1\" : \"details_here\" , \"user2\" : \"details_here\" , \"user3\" : \"details_here\" } @router . get ( \"/users\" ) def get_users () -> Dict : # /users?limit=1 pagination_limit = router . current_event . get_query_string_value ( name = \"limit\" , default_value = 10 ) logger . info ( f \"Fetching the first { pagination_limit } users...\" ) ret = dict ( itertools . islice ( USERS . items (), int ( pagination_limit ))) return { \"items\" : [ ret ]} @router . get ( \"/users/<username>\" ) def get_user ( username : str ) -> Dict : logger . info ( f \"Fetching username { username } \" ) return { \"details\" : USERS . get ( username , {})} # many other related /users routing app.py We use include_router method and include all user routers registered in the router global object. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import Dict from aws_lambda_powertools import Logger from aws_lambda_powertools.event_handler import ApiGatewayResolver from aws_lambda_powertools.utilities.typing import LambdaContext import users logger = Logger () app = ApiGatewayResolver () app . include_router ( users . router ) def lambda_handler ( event : Dict , context : LambdaContext ): return app . resolve ( event , context )","title":"Split routes with Router"},{"location":"core/event_handler/api_gateway/#route-prefix","text":"In the previous example, users.py routes had a /users prefix. This might grow over time and become repetitive. When necessary, you can set a prefix when including a router object. This means you could remove /users prefix in users.py altogether. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import Dict from aws_lambda_powertools.event_handler import ApiGatewayResolver from aws_lambda_powertools.utilities.typing import LambdaContext import users app = ApiGatewayResolver () app . include_router ( users . router , prefix = \"/users\" ) # prefix '/users' to any route in `users.router` def lambda_handler ( event : Dict , context : LambdaContext ): return app . resolve ( event , context ) users.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from typing import Dict from aws_lambda_powertools import Logger from aws_lambda_powertools.event_handler.api_gateway import Router logger = Logger ( child = True ) router = Router () USERS = { \"user1\" : \"details\" , \"user2\" : \"details\" , \"user3\" : \"details\" } @router . get ( \"/\" ) # /users, when we set the prefix in app.py def get_users () -> Dict : ... @router . get ( \"/<username>\" ) def get_user ( username : str ) -> Dict : ... # many other related /users routing","title":"Route prefix"},{"location":"core/event_handler/api_gateway/#sample-layout","text":"This sample project contains a Users function with two distinct set of routes, /users and /health . The layout optimizes for code sharing, no custom build tooling, and it uses Lambda Layers to install Lambda Powertools. Project layout 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 . \u251c\u2500\u2500 Pipfile # project app & dev dependencies; poetry, pipenv, etc. \u251c\u2500\u2500 Pipfile . lock \u251c\u2500\u2500 README . md \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 __init__ . py \u2502 \u251c\u2500\u2500 requirements . txt # sam build detect it automatically due to CodeUri: src, e.g. pipenv lock -r > src/requirements.txt \u2502 \u2514\u2500\u2500 users \u2502 \u251c\u2500\u2500 __init__ . py \u2502 \u251c\u2500\u2500 main . py # this will be our users Lambda fn; it could be split in folders if we want separate fns same code base \u2502 \u2514\u2500\u2500 routers # routers module \u2502 \u251c\u2500\u2500 __init__ . py \u2502 \u251c\u2500\u2500 health . py # /users routes, e.g. from routers import users; users.router \u2502 \u2514\u2500\u2500 users . py # /users routes, e.g. from .routers import users; users.router \u251c\u2500\u2500 template . yml # SAM template.yml, CodeUri: src, Handler: users.main.lambda_handler \u2514\u2500\u2500 tests \u251c\u2500\u2500 __init__ . py \u251c\u2500\u2500 unit \u2502 \u251c\u2500\u2500 __init__ . py \u2502 \u2514\u2500\u2500 test_users . py # unit tests for the users router \u2502 \u2514\u2500\u2500 test_health . py # unit tests for the health router \u2514\u2500\u2500 functional \u251c\u2500\u2500 __init__ . py \u251c\u2500\u2500 conftest . py # pytest fixtures for the functional tests \u2514\u2500\u2500 test_main . py # functional tests for the main lambda handler template.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 AWSTemplateFormatVersion : '2010-09-09' Transform : AWS::Serverless-2016-10-31 Description : Example service with multiple routes Globals : Function : Timeout : 10 MemorySize : 512 Runtime : python3.9 Tracing : Active Architectures : - x86_64 Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_LOGGER_LOG_EVENT : true POWERTOOLS_METRICS_NAMESPACE : MyServerlessApplication POWERTOOLS_SERVICE_NAME : users Resources : UsersService : Type : AWS::Serverless::Function Properties : Handler : users.main.lambda_handler CodeUri : src Layers : # Latest version: https://awslabs.github.io/aws-lambda-powertools-python/latest/#lambda-layer - !Sub arn:aws:lambda:${AWS::Region}:017000801446:layer:AWSLambdaPowertoolsPython:4 Events : ByUser : Type : Api Properties : Path : /users/{name} Method : GET AllUsers : Type : Api Properties : Path : /users Method : GET HealthCheck : Type : Api Properties : Path : /status Method : GET Outputs : UsersApiEndpoint : Description : \"API Gateway endpoint URL for Prod environment for Users Function\" Value : !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod\" AllUsersURL : Description : \"URL to fetch all registered users\" Value : !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/users\" ByUserURL : Description : \"URL to retrieve details by user\" Value : !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/users/test\" UsersServiceFunctionArn : Description : \"Users Lambda Function ARN\" Value : !GetAtt UsersService.Arn src/users/main.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from typing import Dict from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.event_handler import ApiGatewayResolver from aws_lambda_powertools.event_handler.api_gateway import ProxyEventType from aws_lambda_powertools.logging.correlation_paths import APPLICATION_LOAD_BALANCER from aws_lambda_powertools.utilities.typing import LambdaContext from .routers import health , users tracer = Tracer () logger = Logger () app = ApiGatewayResolver ( proxy_type = ProxyEventType . APIGatewayProxyEvent ) app . include_router ( health . router ) app . include_router ( users . router ) @logger . inject_lambda_context ( correlation_id_path = API_GATEWAY_REST ) @tracer . capture_lambda_handler def lambda_handler ( event : Dict , context : LambdaContext ): return app . resolve ( event , context ) src/users/routers/health.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import Dict from aws_lambda_powertools import Logger from aws_lambda_powertools.event_handler.api_gateway import Router router = Router () logger = Logger ( child = True ) @router . get ( \"/status\" ) def health () -> Dict : logger . debug ( \"Health check called\" ) return { \"status\" : \"OK\" } tests/functional/test_users.py 1 2 3 4 5 6 7 8 9 10 11 import json from src.users import main # follows namespace package from root def test_lambda_handler ( apigw_event , lambda_context ): ret = main . lambda_handler ( apigw_event , lambda_context ) expected = json . dumps ({ \"message\" : \"hello universe\" }, separators = ( \",\" , \":\" )) assert ret [ \"statusCode\" ] == 200 assert ret [ \"body\" ] == expected","title":"Sample layout"},{"location":"core/event_handler/api_gateway/#considerations","text":"This utility is optimized for fast startup, minimal feature set, and to quickly on-board customers familiar with frameworks like Flask \u2014 it's not meant to be a fully fledged framework. Event Handler naturally leads to a single Lambda function handling multiple routes for a given service, which can be eventually broken into multiple functions. Both single (monolithic) and multiple functions (micro) offer different set of trade-offs worth knowing. Tip TL;DR. Start with a monolithic function, add additional functions with new handlers, and possibly break into micro functions if necessary.","title":"Considerations"},{"location":"core/event_handler/api_gateway/#monolithic-function","text":"A monolithic function means that your final code artifact will be deployed to a single function. This is generally the best approach to start. Benefits Code reuse . It's easier to reason about your service, modularize it and reuse code as it grows. Eventually, it can be turned into a standalone library. No custom tooling . Monolithic functions are treated just like normal Python packages; no upfront investment in tooling. Faster deployment and debugging . Whether you use all-at-once, linear, or canary deployments, a monolithic function is a single deployable unit. IDEs like PyCharm and VSCode have tooling to quickly profile, visualize, and step through debug any Python package. Downsides Cold starts . Frequent deployments and/or high load can diminish the benefit of monolithic functions depending on your latency requirements, due to Lambda scaling model . Always load test to pragmatically balance between your customer experience and development cognitive load. Granular security permissions . The micro function approach enables you to use fine-grained permissions & access controls, separate external dependencies & code signing at the function level. Conversely, you could have multiple functions while duplicating the final code artifact in a monolithic approach. Regardless, least privilege can be applied to either approaches. Higher risk per deployment . A misconfiguration or invalid import can cause disruption if not caught earlier in automated testing. Multiple functions can mitigate misconfigurations but they would still share the same code artifact. You can further minimize risks with multiple environments in your CI/CD pipeline.","title":"Monolithic function"},{"location":"core/event_handler/api_gateway/#micro-function","text":"A micro function means that your final code artifact will be different to each function deployed. This is generally the approach to start if you're looking for fine-grain control and/or high load on certain parts of your service. Benefits Granular scaling . A micro function can benefit from the Lambda scaling model to scale differently depending on each part of your application. Concurrency controls and provisioned concurrency can also be used at a granular level for capacity management. Discoverability . Micro functions are easier do visualize when using distributed tracing. Their high-level architectures can be self-explanatory, and complexity is highly visible \u2014 assuming each function is named to the business purpose it serves. Package size . An independent function can be significant smaller (KB vs MB) depending on external dependencies it require to perform its purpose. Conversely, a monolithic approach can benefit from Lambda Layers to optimize builds for external dependencies. Downsides Upfront investment . Python ecosystem doesn't use a bundler \u2014 you need a custom build tooling to ensure each function only has what it needs and account for C bindings for runtime compatibility . Operations become more elaborate \u2014 you need to standardize tracing labels/annotations, structured logging, and metrics to pinpoint root causes. Engineering discipline is necessary for both approaches. Micro-function approach however requires further attention in consistency as the number of functions grow, just like any distributed system. Harder to share code . Shared code must be carefully evaluated to avoid unnecessary deployments when that changes. Equally, if shared code isn't a library, your development, building, deployment tooling need to accommodate the distinct layout. Slower safe deployments . Safely deploying multiple functions require coordination \u2014 AWS CodeDeploy deploys and verifies each function sequentially. This increases lead time substantially (minutes to hours) depending on the deployment strategy you choose. You can mitigate it by selectively enabling it in prod-like environments only, and where the risk profile is applicable. Automated testing, operational and security reviews are essential to stability in either approaches.","title":"Micro function"},{"location":"core/event_handler/api_gateway/#testing-your-code","text":"You can test your routes by passing a proxy event request where path and httpMethod . test_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from dataclasses import dataclass import pytest import app @pytest . fixture def lambda_context (): @dataclass class LambdaContext : function_name : str = \"test\" memory_limit_in_mb : int = 128 invoked_function_arn : str = \"arn:aws:lambda:eu-west-1:809313241:function:test\" aws_request_id : str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\" return LambdaContext () def test_lambda_handler ( lambda_context ): minimal_event = { \"path\" : \"/hello\" , \"httpMethod\" : \"GET\" \"requestContext\" : { # correlation ID \"requestId\" : \"c6af9ac6-7b61-11e6-9a41-93e8deadbeef\" } } app . lambda_handler ( minimal_event , lambda_context ) app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver logger = Logger () app = ApiGatewayResolver () # by default API Gateway REST API (v1) @app . get ( \"/hello\" ) def get_hello_universe (): return { \"message\" : \"hello universe\" } # You can continue to use other utilities just as before @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) def lambda_handler ( event , context ): return app . resolve ( event , context )","title":"Testing your code"},{"location":"core/event_handler/api_gateway/#faq","text":"What's the difference between this utility and frameworks like Chalice? Chalice is a full featured microframework that manages application and infrastructure. This utility, however, is largely focused on routing to reduce boilerplate and expects you to setup and manage infrastructure with your framework of choice. That said, Chalice has native integration with Lambda Powertools if you're looking for a more opinionated and web framework feature set.","title":"FAQ"},{"location":"core/event_handler/appsync/","text":"Event handler for AWS AppSync Direct Lambda Resolver and Amplify GraphQL Transformer. Key Features \u00b6 Automatically parse API arguments to function arguments Choose between strictly match a GraphQL field name or all of them to a function Integrates with Data classes utilities to access resolver and identity information Works with both Direct Lambda Resolver and Amplify GraphQL Transformer @function directive Support async Python 3.8+ functions, and generators Terminology \u00b6 Direct Lambda Resolver . A custom AppSync Resolver to bypass the use of Apache Velocity Template (VTL) and automatically map your function's response to a GraphQL field. Amplify GraphQL Transformer . Custom GraphQL directives to define your application's data model using Schema Definition Language (SDL). Amplify CLI uses these directives to convert GraphQL SDL into full descriptive AWS CloudFormation templates. Getting started \u00b6 Required resources \u00b6 You must have an existing AppSync GraphQL API and IAM permissions to invoke your Lambda function. That said, there is no additional permissions to use this utility. This is the sample infrastructure we are using for the initial examples with a AppSync Direct Lambda Resolver. Tip: Designing GraphQL Schemas for the first time? Visit AWS AppSync schema documentation for understanding how to define types, nesting, and pagination. schema.graphql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 schema { query : Query } type Query { getTodo ( id : ID ! ) : Todo listTodos : [ Todo ] } type Todo { id : ID ! title : String description : String done : Boolean } template.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 AWSTemplateFormatVersion : '2010-09-09' Transform : AWS::Serverless-2016-10-31 Description : Hello world Direct Lambda Resolver Globals : Function : Timeout : 5 Runtime : python3.8 Tracing : Active Environment : Variables : # Powertools env vars: https://awslabs.github.io/aws-lambda-powertools-python/latest/#environment-variables LOG_LEVEL : INFO POWERTOOLS_LOGGER_SAMPLE_RATE : 0.1 POWERTOOLS_LOGGER_LOG_EVENT : true POWERTOOLS_SERVICE_NAME : sample_resolver Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Handler : app.lambda_handler CodeUri : hello_world Description : Sample Lambda Powertools Direct Lambda Resolver Tags : SOLUTION : LambdaPowertoolsPython # IAM Permissions and Roles AppSyncServiceRole : Type : \"AWS::IAM::Role\" Properties : AssumeRolePolicyDocument : Version : \"2012-10-17\" Statement : - Effect : \"Allow\" Principal : Service : - \"appsync.amazonaws.com\" Action : - \"sts:AssumeRole\" InvokeLambdaResolverPolicy : Type : \"AWS::IAM::Policy\" Properties : PolicyName : \"DirectAppSyncLambda\" PolicyDocument : Version : \"2012-10-17\" Statement : - Effect : \"Allow\" Action : \"lambda:invokeFunction\" Resource : - !GetAtt HelloWorldFunction.Arn Roles : - !Ref AppSyncServiceRole # GraphQL API HelloWorldApi : Type : \"AWS::AppSync::GraphQLApi\" Properties : Name : HelloWorldApi AuthenticationType : \"API_KEY\" XrayEnabled : true HelloWorldApiKey : Type : AWS::AppSync::ApiKey Properties : ApiId : !GetAtt HelloWorldApi.ApiId HelloWorldApiSchema : Type : \"AWS::AppSync::GraphQLSchema\" Properties : ApiId : !GetAtt HelloWorldApi.ApiId Definition : | schema { query:Query } type Query { getTodo(id: ID!): Todo listTodos: [Todo] } type Todo { id: ID! title: String description: String done: Boolean } # Lambda Direct Data Source and Resolver HelloWorldFunctionDataSource : Type : \"AWS::AppSync::DataSource\" Properties : ApiId : !GetAtt HelloWorldApi.ApiId Name : \"HelloWorldLambdaDirectResolver\" Type : \"AWS_LAMBDA\" ServiceRoleArn : !GetAtt AppSyncServiceRole.Arn LambdaConfig : LambdaFunctionArn : !GetAtt HelloWorldFunction.Arn ListTodosResolver : Type : \"AWS::AppSync::Resolver\" Properties : ApiId : !GetAtt HelloWorldApi.ApiId TypeName : \"Query\" FieldName : \"listTodos\" DataSourceName : !GetAtt HelloWorldFunctionDataSource.Name GetTodoResolver : Type : \"AWS::AppSync::Resolver\" Properties : ApiId : !GetAtt HelloWorldApi.ApiId TypeName : \"Query\" FieldName : \"getTodo\" DataSourceName : !GetAtt HelloWorldFunctionDataSource.Name Outputs : HelloWorldFunction : Description : \"Hello World Lambda Function ARN\" Value : !GetAtt HelloWorldFunction.Arn HelloWorldAPI : Value : !GetAtt HelloWorldApi.Arn Resolver decorator \u00b6 You can define your functions to match GraphQL types and fields with the app.resolver() decorator. Here's an example where we have two separate functions to resolve getTodo and listTodos fields within the Query type. For completion, we use Scalar type utilities to generate the right output based on our schema definition. Info GraphQL arguments are passed as function arguments. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler import AppSyncResolver from aws_lambda_powertools.utilities.data_classes.appsync import scalar_types_utils tracer = Tracer ( service = \"sample_resolver\" ) logger = Logger ( service = \"sample_resolver\" ) app = AppSyncResolver () # Note that `creation_time` isn't available in the schema # This utility also takes into account what info you make available at API level vs what's stored TODOS = [ { \"id\" : scalar_types_utils . make_id (), # type ID or String \"title\" : \"First task\" , \"description\" : \"String\" , \"done\" : False , \"creation_time\" : scalar_types_utils . aws_datetime (), # type AWSDateTime }, { \"id\" : scalar_types_utils . make_id (), \"title\" : \"Second task\" , \"description\" : \"String\" , \"done\" : True , \"creation_time\" : scalar_types_utils . aws_datetime (), }, ] @app . resolver ( type_name = \"Query\" , field_name = \"getTodo\" ) def get_todo ( id : str = \"\" ): logger . info ( f \"Fetching Todo { id } \" ) todo = [ todo for todo in TODOS if todo [ \"id\" ] == id ] return todo @app . resolver ( type_name = \"Query\" , field_name = \"listTodos\" ) def list_todos (): return TODOS @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPSYNC_RESOLVER ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) schema.graphql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 schema { query : Query } type Query { getTodo ( id : ID ! ) : Todo listTodos : [ Todo ] } type Todo { id : ID ! title : String description : String done : Boolean } getTodo_event.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 { \"arguments\" : { \"id\" : \"7e362732-c8cd-4405-b090-144ac9b38960\" }, \"identity\" : null , \"source\" : null , \"request\" : { \"headers\" : { \"x-forwarded-for\" : \"1.2.3.4, 5.6.7.8\" , \"accept-encoding\" : \"gzip, deflate, br\" , \"cloudfront-viewer-country\" : \"NL\" , \"cloudfront-is-tablet-viewer\" : \"false\" , \"referer\" : \"https://eu-west-1.console.aws.amazon.com/appsync/home?region=eu-west-1\" , \"via\" : \"2.0 9fce949f3749407c8e6a75087e168b47.cloudfront.net (CloudFront)\" , \"cloudfront-forwarded-proto\" : \"https\" , \"origin\" : \"https://eu-west-1.console.aws.amazon.com\" , \"x-api-key\" : \"da1-c33ullkbkze3jg5hf5ddgcs4fq\" , \"content-type\" : \"application/json\" , \"x-amzn-trace-id\" : \"Root=1-606eb2f2-1babc433453a332c43fb4494\" , \"x-amz-cf-id\" : \"SJw16ZOPuMZMINx5Xcxa9pB84oMPSGCzNOfrbJLvd80sPa0waCXzYQ==\" , \"content-length\" : \"114\" , \"x-amz-user-agent\" : \"AWS-Console-AppSync/\" , \"x-forwarded-proto\" : \"https\" , \"host\" : \"ldcvmkdnd5az3lm3gnf5ixvcyy.appsync-api.eu-west-1.amazonaws.com\" , \"accept-language\" : \"en-US,en;q=0.5\" , \"user-agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\" , \"cloudfront-is-desktop-viewer\" : \"true\" , \"cloudfront-is-mobile-viewer\" : \"false\" , \"accept\" : \"*/*\" , \"x-forwarded-port\" : \"443\" , \"cloudfront-is-smarttv-viewer\" : \"false\" } }, \"prev\" : null , \"info\" : { \"parentTypeName\" : \"Query\" , \"selectionSetList\" : [ \"title\" , \"id\" ], \"selectionSetGraphQL\" : \"{\\n title\\n id\\n}\" , \"fieldName\" : \"getTodo\" , \"variables\" : {} }, \"stash\" : {} } listTodos_event.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 { \"arguments\" : {}, \"identity\" : null , \"source\" : null , \"request\" : { \"headers\" : { \"x-forwarded-for\" : \"1.2.3.4, 5.6.7.8\" , \"accept-encoding\" : \"gzip, deflate, br\" , \"cloudfront-viewer-country\" : \"NL\" , \"cloudfront-is-tablet-viewer\" : \"false\" , \"referer\" : \"https://eu-west-1.console.aws.amazon.com/appsync/home?region=eu-west-1\" , \"via\" : \"2.0 9fce949f3749407c8e6a75087e168b47.cloudfront.net (CloudFront)\" , \"cloudfront-forwarded-proto\" : \"https\" , \"origin\" : \"https://eu-west-1.console.aws.amazon.com\" , \"x-api-key\" : \"da1-c33ullkbkze3jg5hf5ddgcs4fq\" , \"content-type\" : \"application/json\" , \"x-amzn-trace-id\" : \"Root=1-606eb2f2-1babc433453a332c43fb4494\" , \"x-amz-cf-id\" : \"SJw16ZOPuMZMINx5Xcxa9pB84oMPSGCzNOfrbJLvd80sPa0waCXzYQ==\" , \"content-length\" : \"114\" , \"x-amz-user-agent\" : \"AWS-Console-AppSync/\" , \"x-forwarded-proto\" : \"https\" , \"host\" : \"ldcvmkdnd5az3lm3gnf5ixvcyy.appsync-api.eu-west-1.amazonaws.com\" , \"accept-language\" : \"en-US,en;q=0.5\" , \"user-agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\" , \"cloudfront-is-desktop-viewer\" : \"true\" , \"cloudfront-is-mobile-viewer\" : \"false\" , \"accept\" : \"*/*\" , \"x-forwarded-port\" : \"443\" , \"cloudfront-is-smarttv-viewer\" : \"false\" } }, \"prev\" : null , \"info\" : { \"parentTypeName\" : \"Query\" , \"selectionSetList\" : [ \"id\" , \"title\" ], \"selectionSetGraphQL\" : \"{\\n id\\n title\\n}\" , \"fieldName\" : \"listTodos\" , \"variables\" : {} }, \"stash\" : {} } Advanced \u00b6 Nested mappings \u00b6 You can nest app.resolver() decorator multiple times when resolving fields with the same return. nested_mappings.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler import AppSyncResolver tracer = Tracer ( service = \"sample_resolver\" ) logger = Logger ( service = \"sample_resolver\" ) app = AppSyncResolver () @app . resolver ( field_name = \"listLocations\" ) @app . resolver ( field_name = \"locations\" ) def get_locations ( name : str , description : str = \"\" ): return name + description @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPSYNC_RESOLVER ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) schema.graphql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 schema { query : Query } type Query { listLocations : [ Todo ] } type Location { id : ID ! name : String ! description : String address : String } type Merchant { id : String ! name : String ! description : String locations : [ Location ] } Async functions \u00b6 For Lambda Python3.8+ runtime, this utility supports async functions when you use in conjunction with asyncio.run . Resolving GraphQL resolvers async 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler import AppSyncResolver tracer = Tracer ( service = \"sample_resolver\" ) logger = Logger ( service = \"sample_resolver\" ) app = AppSyncResolver () @app . resolver ( type_name = \"Query\" , field_name = \"listTodos\" ) async def list_todos (): todos = await some_async_io_call () return todos @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPSYNC_RESOLVER ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): result = app . resolve ( event , context ) return asyncio . run ( result ) Amplify GraphQL Transformer \u00b6 Assuming you have Amplify CLI installed , create a new API using amplify add api and use the following GraphQL Schema. Example GraphQL Schema 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @model type Merchant { id : String ! name : String ! description : String # Resolves to `common_field` commonField : String @function ( name : \"merchantInfo-${env}\" ) } type Location { id : ID ! name : String ! address : String # Resolves to `common_field` commonField : String @function ( name : \"merchantInfo-${env}\" ) } type Query { # List of locations resolves to `list_locations` listLocations ( page : Int , size : Int ) : [ Location ] @function ( name : \"merchantInfo-${env}\" ) # List of locations resolves to `list_locations` findMerchant ( search : str ) : [ Merchant ] @function ( name : \"searchMerchant-${env}\" ) } Create two new basic Python functions via amplify add function . Note Amplify CLI generated functions use Pipenv as a dependency manager. Your function source code is located at amplify/backend/function/your-function-name . Within your function's folder, add Lambda Powertools as a dependency with pipenv install aws-lambda-powertools . Use the following code for merchantInfo and searchMerchant functions respectively. merchantInfo/src/app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler import AppSyncResolver from aws_lambda_powertools.utilities.data_classes.appsync import scalar_types_utils tracer = Tracer ( service = \"sample_graphql_transformer_resolver\" ) logger = Logger ( service = \"sample_graphql_transformer_resolver\" ) app = AppSyncResolver () @app . resolver ( type_name = \"Query\" , field_name = \"listLocations\" ) def list_locations ( page : int = 0 , size : int = 10 ): return [{ \"id\" : 100 , \"name\" : \"Smooth Grooves\" }] @app . resolver ( field_name = \"commonField\" ) def common_field (): # Would match all fieldNames matching 'commonField' return scalar_types_utils . make_id () @tracer . capture_lambda_handler @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPSYNC_RESOLVER ) def lambda_handler ( event , context ): app . resolve ( event , context ) searchMerchant/src/app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from aws_lambda_powertools.event_handler import AppSyncResolver from aws_lambda_powertools.utilities.data_classes.appsync import scalar_types_utils app = AppSyncResolver () @app . resolver ( type_name = \"Query\" , field_name = \"findMerchant\" ) def find_merchant ( search : str ): return [ { \"id\" : scalar_types_utils . make_id (), \"name\" : \"Brewer Brewing\" , \"description\" : \"Mike Brewer's IPA brewing place\" }, { \"id\" : scalar_types_utils . make_id (), \"name\" : \"Serverlessa's Bakery\" , \"description\" : \"Lessa's sourdough place\" }, ] Example AppSync GraphQL Transformer Function resolver events Query.listLocations event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"typeName\" : \"Query\" , \"fieldName\" : \"listLocations\" , \"arguments\" : { \"page\" : 2 , \"size\" : 1 }, \"identity\" : { \"claims\" : { \"iat\" : 1615366261 ... }, \"username\" : \"mike\" , ... }, \"request\" : { \"headers\" : { \"x-amzn-trace-id\" : \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\" , \"x-forwarded-for\" : \"127.0.0.1\" ... } }, ... } *.commonField event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \"typeName\" : \"Merchant\" , \"fieldName\" : \"commonField\" , \"arguments\" : { }, \"identity\" : { \"claims\" : { \"iat\" : 1615366261 ... }, \"username\" : \"mike\" , ... }, \"request\" : { \"headers\" : { \"x-amzn-trace-id\" : \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\" , \"x-forwarded-for\" : \"127.0.0.1\" ... } }, ... } Query.findMerchant event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"typeName\" : \"Query\" , \"fieldName\" : \"findMerchant\" , \"arguments\" : { \"search\" : \"Brewers Coffee\" }, \"identity\" : { \"claims\" : { \"iat\" : 1615366261 ... }, \"username\" : \"mike\" , ... }, \"request\" : { \"headers\" : { \"x-amzn-trace-id\" : \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\" , \"x-forwarded-for\" : \"127.0.0.1\" ... } }, ... } Custom data models \u00b6 You can subclass AppSyncResolverEvent to bring your own set of methods to handle incoming events, by using data_model param in the resolve method. custom_model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler import AppSyncResolver tracer = Tracer ( service = \"sample_resolver\" ) logger = Logger ( service = \"sample_resolver\" ) app = AppSyncResolver () class MyCustomModel ( AppSyncResolverEvent ): @property def country_viewer ( self ) -> str : return self . request_headers . get ( \"cloudfront-viewer-country\" ) @app . resolver ( field_name = \"listLocations\" ) @app . resolver ( field_name = \"locations\" ) def get_locations ( name : str , description : str = \"\" ): if app . current_event . country_viewer == \"US\" : ... return name + description @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPSYNC_RESOLVER ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context , data_model = MyCustomModel ) schema.graphql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 schema { query : Query } type Query { listLocations : [ Location ] } type Location { id : ID ! name : String ! description : String address : String } type Merchant { id : String ! name : String ! description : String locations : [ Location ] } listLocations_event.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 { \"arguments\" : {}, \"identity\" : null , \"source\" : null , \"request\" : { \"headers\" : { \"x-forwarded-for\" : \"1.2.3.4, 5.6.7.8\" , \"accept-encoding\" : \"gzip, deflate, br\" , \"cloudfront-viewer-country\" : \"NL\" , \"cloudfront-is-tablet-viewer\" : \"false\" , \"referer\" : \"https://eu-west-1.console.aws.amazon.com/appsync/home?region=eu-west-1\" , \"via\" : \"2.0 9fce949f3749407c8e6a75087e168b47.cloudfront.net (CloudFront)\" , \"cloudfront-forwarded-proto\" : \"https\" , \"origin\" : \"https://eu-west-1.console.aws.amazon.com\" , \"x-api-key\" : \"da1-c33ullkbkze3jg5hf5ddgcs4fq\" , \"content-type\" : \"application/json\" , \"x-amzn-trace-id\" : \"Root=1-606eb2f2-1babc433453a332c43fb4494\" , \"x-amz-cf-id\" : \"SJw16ZOPuMZMINx5Xcxa9pB84oMPSGCzNOfrbJLvd80sPa0waCXzYQ==\" , \"content-length\" : \"114\" , \"x-amz-user-agent\" : \"AWS-Console-AppSync/\" , \"x-forwarded-proto\" : \"https\" , \"host\" : \"ldcvmkdnd5az3lm3gnf5ixvcyy.appsync-api.eu-west-1.amazonaws.com\" , \"accept-language\" : \"en-US,en;q=0.5\" , \"user-agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\" , \"cloudfront-is-desktop-viewer\" : \"true\" , \"cloudfront-is-mobile-viewer\" : \"false\" , \"accept\" : \"*/*\" , \"x-forwarded-port\" : \"443\" , \"cloudfront-is-smarttv-viewer\" : \"false\" } }, \"prev\" : null , \"info\" : { \"parentTypeName\" : \"Query\" , \"selectionSetList\" : [ \"id\" , \"name\" , \"description\" ], \"selectionSetGraphQL\" : \"{\\n id\\n name\\n description\\n}\" , \"fieldName\" : \"listLocations\" , \"variables\" : {} }, \"stash\" : {} } Split operations with Router \u00b6 Tip Read the considerations section for trade-offs between monolithic and micro functions , as it's also applicable here. As you grow the number of related GraphQL operations a given Lambda function should handle, it is natural to split them into separate files to ease maintenance - That's where the Router feature is useful. Let's assume you have app.py as your Lambda function entrypoint and routes in location.py , this is how you'd use the Router feature. resolvers/location.py We import Router instead of AppSyncResolver ; syntax wise is exactly the same. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from typing import Any , Dict , List from aws_lambda_powertools import Logger from aws_lambda_powertools.event_handler.appsync import Router logger = Logger ( child = True ) router = Router () @router . resolver ( type_name = \"Query\" , field_name = \"listLocations\" ) def list_locations ( merchant_id : str ) -> List [ Dict [ str , Any ]]: return [{ \"name\" : \"Location name\" , \"merchant_id\" : merchant_id }] @router . resolver ( type_name = \"Location\" , field_name = \"status\" ) def resolve_status ( merchant_id : str ) -> str : logger . debug ( f \"Resolve status for merchant_id: { merchant_id } \" ) return \"FOO\" app.py We use include_router method and include all location operations registered in the router global object. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from typing import Dict from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.event_handler import AppSyncResolver from aws_lambda_powertools.logging.correlation_paths import APPSYNC_RESOLVER from aws_lambda_powertools.utilities.typing import LambdaContext from resolvers import location tracer = Tracer () logger = Logger () app = AppSyncResolver () app . include_router ( location . router ) @tracer . capture_lambda_handler @logger . inject_lambda_context ( correlation_id_path = APPSYNC_RESOLVER ) def lambda_handler ( event : Dict , context : LambdaContext ): app . resolve ( event , context ) Testing your code \u00b6 You can test your resolvers by passing a mocked or actual AppSync Lambda event that you're expecting. You can use either app.resolve(event, context) or simply app(event, context) . Here's an example from our internal functional test. test_direct_resolver.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def test_direct_resolver (): # Check whether we can handle an example appsync direct resolver # load_event primarily deserialize the JSON event into a dict mock_event = load_event ( \"appSyncDirectResolver.json\" ) app = AppSyncResolver () @app . resolver ( field_name = \"createSomething\" ) def create_something ( id : str ): assert app . lambda_context == {} return id # Call the implicit handler result = app ( mock_event , {}) assert result == \"my identifier\" appSyncDirectResolver.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 { \"arguments\" : { \"id\" : \"my identifier\" }, \"identity\" : { \"claims\" : { \"sub\" : \"192879fc-a240-4bf1-ab5a-d6a00f3063f9\" , \"email_verified\" : true , \"iss\" : \"https://cognito-idp.us-west-2.amazonaws.com/us-west-xxxxxxxxxxx\" , \"phone_number_verified\" : false , \"cognito:username\" : \"jdoe\" , \"aud\" : \"7471s60os7h0uu77i1tk27sp9n\" , \"event_id\" : \"bc334ed8-a938-4474-b644-9547e304e606\" , \"token_use\" : \"id\" , \"auth_time\" : 1599154213 , \"phone_number\" : \"+19999999999\" , \"exp\" : 1599157813 , \"iat\" : 1599154213 , \"email\" : \"jdoe@email.com\" }, \"defaultAuthStrategy\" : \"ALLOW\" , \"groups\" : null , \"issuer\" : \"https://cognito-idp.us-west-2.amazonaws.com/us-west-xxxxxxxxxxx\" , \"sourceIp\" : [ \"1.1.1.1\" ], \"sub\" : \"192879fc-a240-4bf1-ab5a-d6a00f3063f9\" , \"username\" : \"jdoe\" }, \"source\" : null , \"request\" : { \"headers\" : { \"x-forwarded-for\" : \"1.1.1.1, 2.2.2.2\" , \"cloudfront-viewer-country\" : \"US\" , \"cloudfront-is-tablet-viewer\" : \"false\" , \"via\" : \"2.0 xxxxxxxxxxxxxxxx.cloudfront.net (CloudFront)\" , \"cloudfront-forwarded-proto\" : \"https\" , \"origin\" : \"https://us-west-1.console.aws.amazon.com\" , \"content-length\" : \"217\" , \"accept-language\" : \"en-US,en;q=0.9\" , \"host\" : \"xxxxxxxxxxxxxxxx.appsync-api.us-west-1.amazonaws.com\" , \"x-forwarded-proto\" : \"https\" , \"user-agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36\" , \"accept\" : \"*/*\" , \"cloudfront-is-mobile-viewer\" : \"false\" , \"cloudfront-is-smarttv-viewer\" : \"false\" , \"accept-encoding\" : \"gzip, deflate, br\" , \"referer\" : \"https://us-west-1.console.aws.amazon.com/appsync/home?region=us-west-1\" , \"content-type\" : \"application/json\" , \"sec-fetch-mode\" : \"cors\" , \"x-amz-cf-id\" : \"3aykhqlUwQeANU-HGY7E_guV5EkNeMMtwyOgiA==\" , \"x-amzn-trace-id\" : \"Root=1-5f512f51-fac632066c5e848ae714\" , \"authorization\" : \"eyJraWQiOiJScWFCSlJqYVJlM0hrSnBTUFpIcVRXazNOW...\" , \"sec-fetch-dest\" : \"empty\" , \"x-amz-user-agent\" : \"AWS-Console-AppSync/\" , \"cloudfront-is-desktop-viewer\" : \"true\" , \"sec-fetch-site\" : \"cross-site\" , \"x-forwarded-port\" : \"443\" } }, \"prev\" : null , \"info\" : { \"selectionSetList\" : [ \"id\" , \"field1\" , \"field2\" ], \"selectionSetGraphQL\" : \"{\\n id\\n field1\\n field2\\n}\" , \"parentTypeName\" : \"Mutation\" , \"fieldName\" : \"createSomething\" , \"variables\" : {} }, \"stash\" : {} }","title":"GraphQL API"},{"location":"core/event_handler/appsync/#key-features","text":"Automatically parse API arguments to function arguments Choose between strictly match a GraphQL field name or all of them to a function Integrates with Data classes utilities to access resolver and identity information Works with both Direct Lambda Resolver and Amplify GraphQL Transformer @function directive Support async Python 3.8+ functions, and generators","title":"Key Features"},{"location":"core/event_handler/appsync/#terminology","text":"Direct Lambda Resolver . A custom AppSync Resolver to bypass the use of Apache Velocity Template (VTL) and automatically map your function's response to a GraphQL field. Amplify GraphQL Transformer . Custom GraphQL directives to define your application's data model using Schema Definition Language (SDL). Amplify CLI uses these directives to convert GraphQL SDL into full descriptive AWS CloudFormation templates.","title":"Terminology"},{"location":"core/event_handler/appsync/#getting-started","text":"","title":"Getting started"},{"location":"core/event_handler/appsync/#required-resources","text":"You must have an existing AppSync GraphQL API and IAM permissions to invoke your Lambda function. That said, there is no additional permissions to use this utility. This is the sample infrastructure we are using for the initial examples with a AppSync Direct Lambda Resolver. Tip: Designing GraphQL Schemas for the first time? Visit AWS AppSync schema documentation for understanding how to define types, nesting, and pagination. schema.graphql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 schema { query : Query } type Query { getTodo ( id : ID ! ) : Todo listTodos : [ Todo ] } type Todo { id : ID ! title : String description : String done : Boolean } template.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 AWSTemplateFormatVersion : '2010-09-09' Transform : AWS::Serverless-2016-10-31 Description : Hello world Direct Lambda Resolver Globals : Function : Timeout : 5 Runtime : python3.8 Tracing : Active Environment : Variables : # Powertools env vars: https://awslabs.github.io/aws-lambda-powertools-python/latest/#environment-variables LOG_LEVEL : INFO POWERTOOLS_LOGGER_SAMPLE_RATE : 0.1 POWERTOOLS_LOGGER_LOG_EVENT : true POWERTOOLS_SERVICE_NAME : sample_resolver Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Handler : app.lambda_handler CodeUri : hello_world Description : Sample Lambda Powertools Direct Lambda Resolver Tags : SOLUTION : LambdaPowertoolsPython # IAM Permissions and Roles AppSyncServiceRole : Type : \"AWS::IAM::Role\" Properties : AssumeRolePolicyDocument : Version : \"2012-10-17\" Statement : - Effect : \"Allow\" Principal : Service : - \"appsync.amazonaws.com\" Action : - \"sts:AssumeRole\" InvokeLambdaResolverPolicy : Type : \"AWS::IAM::Policy\" Properties : PolicyName : \"DirectAppSyncLambda\" PolicyDocument : Version : \"2012-10-17\" Statement : - Effect : \"Allow\" Action : \"lambda:invokeFunction\" Resource : - !GetAtt HelloWorldFunction.Arn Roles : - !Ref AppSyncServiceRole # GraphQL API HelloWorldApi : Type : \"AWS::AppSync::GraphQLApi\" Properties : Name : HelloWorldApi AuthenticationType : \"API_KEY\" XrayEnabled : true HelloWorldApiKey : Type : AWS::AppSync::ApiKey Properties : ApiId : !GetAtt HelloWorldApi.ApiId HelloWorldApiSchema : Type : \"AWS::AppSync::GraphQLSchema\" Properties : ApiId : !GetAtt HelloWorldApi.ApiId Definition : | schema { query:Query } type Query { getTodo(id: ID!): Todo listTodos: [Todo] } type Todo { id: ID! title: String description: String done: Boolean } # Lambda Direct Data Source and Resolver HelloWorldFunctionDataSource : Type : \"AWS::AppSync::DataSource\" Properties : ApiId : !GetAtt HelloWorldApi.ApiId Name : \"HelloWorldLambdaDirectResolver\" Type : \"AWS_LAMBDA\" ServiceRoleArn : !GetAtt AppSyncServiceRole.Arn LambdaConfig : LambdaFunctionArn : !GetAtt HelloWorldFunction.Arn ListTodosResolver : Type : \"AWS::AppSync::Resolver\" Properties : ApiId : !GetAtt HelloWorldApi.ApiId TypeName : \"Query\" FieldName : \"listTodos\" DataSourceName : !GetAtt HelloWorldFunctionDataSource.Name GetTodoResolver : Type : \"AWS::AppSync::Resolver\" Properties : ApiId : !GetAtt HelloWorldApi.ApiId TypeName : \"Query\" FieldName : \"getTodo\" DataSourceName : !GetAtt HelloWorldFunctionDataSource.Name Outputs : HelloWorldFunction : Description : \"Hello World Lambda Function ARN\" Value : !GetAtt HelloWorldFunction.Arn HelloWorldAPI : Value : !GetAtt HelloWorldApi.Arn","title":"Required resources"},{"location":"core/event_handler/appsync/#resolver-decorator","text":"You can define your functions to match GraphQL types and fields with the app.resolver() decorator. Here's an example where we have two separate functions to resolve getTodo and listTodos fields within the Query type. For completion, we use Scalar type utilities to generate the right output based on our schema definition. Info GraphQL arguments are passed as function arguments. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler import AppSyncResolver from aws_lambda_powertools.utilities.data_classes.appsync import scalar_types_utils tracer = Tracer ( service = \"sample_resolver\" ) logger = Logger ( service = \"sample_resolver\" ) app = AppSyncResolver () # Note that `creation_time` isn't available in the schema # This utility also takes into account what info you make available at API level vs what's stored TODOS = [ { \"id\" : scalar_types_utils . make_id (), # type ID or String \"title\" : \"First task\" , \"description\" : \"String\" , \"done\" : False , \"creation_time\" : scalar_types_utils . aws_datetime (), # type AWSDateTime }, { \"id\" : scalar_types_utils . make_id (), \"title\" : \"Second task\" , \"description\" : \"String\" , \"done\" : True , \"creation_time\" : scalar_types_utils . aws_datetime (), }, ] @app . resolver ( type_name = \"Query\" , field_name = \"getTodo\" ) def get_todo ( id : str = \"\" ): logger . info ( f \"Fetching Todo { id } \" ) todo = [ todo for todo in TODOS if todo [ \"id\" ] == id ] return todo @app . resolver ( type_name = \"Query\" , field_name = \"listTodos\" ) def list_todos (): return TODOS @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPSYNC_RESOLVER ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) schema.graphql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 schema { query : Query } type Query { getTodo ( id : ID ! ) : Todo listTodos : [ Todo ] } type Todo { id : ID ! title : String description : String done : Boolean } getTodo_event.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 { \"arguments\" : { \"id\" : \"7e362732-c8cd-4405-b090-144ac9b38960\" }, \"identity\" : null , \"source\" : null , \"request\" : { \"headers\" : { \"x-forwarded-for\" : \"1.2.3.4, 5.6.7.8\" , \"accept-encoding\" : \"gzip, deflate, br\" , \"cloudfront-viewer-country\" : \"NL\" , \"cloudfront-is-tablet-viewer\" : \"false\" , \"referer\" : \"https://eu-west-1.console.aws.amazon.com/appsync/home?region=eu-west-1\" , \"via\" : \"2.0 9fce949f3749407c8e6a75087e168b47.cloudfront.net (CloudFront)\" , \"cloudfront-forwarded-proto\" : \"https\" , \"origin\" : \"https://eu-west-1.console.aws.amazon.com\" , \"x-api-key\" : \"da1-c33ullkbkze3jg5hf5ddgcs4fq\" , \"content-type\" : \"application/json\" , \"x-amzn-trace-id\" : \"Root=1-606eb2f2-1babc433453a332c43fb4494\" , \"x-amz-cf-id\" : \"SJw16ZOPuMZMINx5Xcxa9pB84oMPSGCzNOfrbJLvd80sPa0waCXzYQ==\" , \"content-length\" : \"114\" , \"x-amz-user-agent\" : \"AWS-Console-AppSync/\" , \"x-forwarded-proto\" : \"https\" , \"host\" : \"ldcvmkdnd5az3lm3gnf5ixvcyy.appsync-api.eu-west-1.amazonaws.com\" , \"accept-language\" : \"en-US,en;q=0.5\" , \"user-agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\" , \"cloudfront-is-desktop-viewer\" : \"true\" , \"cloudfront-is-mobile-viewer\" : \"false\" , \"accept\" : \"*/*\" , \"x-forwarded-port\" : \"443\" , \"cloudfront-is-smarttv-viewer\" : \"false\" } }, \"prev\" : null , \"info\" : { \"parentTypeName\" : \"Query\" , \"selectionSetList\" : [ \"title\" , \"id\" ], \"selectionSetGraphQL\" : \"{\\n title\\n id\\n}\" , \"fieldName\" : \"getTodo\" , \"variables\" : {} }, \"stash\" : {} } listTodos_event.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 { \"arguments\" : {}, \"identity\" : null , \"source\" : null , \"request\" : { \"headers\" : { \"x-forwarded-for\" : \"1.2.3.4, 5.6.7.8\" , \"accept-encoding\" : \"gzip, deflate, br\" , \"cloudfront-viewer-country\" : \"NL\" , \"cloudfront-is-tablet-viewer\" : \"false\" , \"referer\" : \"https://eu-west-1.console.aws.amazon.com/appsync/home?region=eu-west-1\" , \"via\" : \"2.0 9fce949f3749407c8e6a75087e168b47.cloudfront.net (CloudFront)\" , \"cloudfront-forwarded-proto\" : \"https\" , \"origin\" : \"https://eu-west-1.console.aws.amazon.com\" , \"x-api-key\" : \"da1-c33ullkbkze3jg5hf5ddgcs4fq\" , \"content-type\" : \"application/json\" , \"x-amzn-trace-id\" : \"Root=1-606eb2f2-1babc433453a332c43fb4494\" , \"x-amz-cf-id\" : \"SJw16ZOPuMZMINx5Xcxa9pB84oMPSGCzNOfrbJLvd80sPa0waCXzYQ==\" , \"content-length\" : \"114\" , \"x-amz-user-agent\" : \"AWS-Console-AppSync/\" , \"x-forwarded-proto\" : \"https\" , \"host\" : \"ldcvmkdnd5az3lm3gnf5ixvcyy.appsync-api.eu-west-1.amazonaws.com\" , \"accept-language\" : \"en-US,en;q=0.5\" , \"user-agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\" , \"cloudfront-is-desktop-viewer\" : \"true\" , \"cloudfront-is-mobile-viewer\" : \"false\" , \"accept\" : \"*/*\" , \"x-forwarded-port\" : \"443\" , \"cloudfront-is-smarttv-viewer\" : \"false\" } }, \"prev\" : null , \"info\" : { \"parentTypeName\" : \"Query\" , \"selectionSetList\" : [ \"id\" , \"title\" ], \"selectionSetGraphQL\" : \"{\\n id\\n title\\n}\" , \"fieldName\" : \"listTodos\" , \"variables\" : {} }, \"stash\" : {} }","title":"Resolver decorator"},{"location":"core/event_handler/appsync/#advanced","text":"","title":"Advanced"},{"location":"core/event_handler/appsync/#nested-mappings","text":"You can nest app.resolver() decorator multiple times when resolving fields with the same return. nested_mappings.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler import AppSyncResolver tracer = Tracer ( service = \"sample_resolver\" ) logger = Logger ( service = \"sample_resolver\" ) app = AppSyncResolver () @app . resolver ( field_name = \"listLocations\" ) @app . resolver ( field_name = \"locations\" ) def get_locations ( name : str , description : str = \"\" ): return name + description @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPSYNC_RESOLVER ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) schema.graphql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 schema { query : Query } type Query { listLocations : [ Todo ] } type Location { id : ID ! name : String ! description : String address : String } type Merchant { id : String ! name : String ! description : String locations : [ Location ] }","title":"Nested mappings"},{"location":"core/event_handler/appsync/#async-functions","text":"For Lambda Python3.8+ runtime, this utility supports async functions when you use in conjunction with asyncio.run . Resolving GraphQL resolvers async 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler import AppSyncResolver tracer = Tracer ( service = \"sample_resolver\" ) logger = Logger ( service = \"sample_resolver\" ) app = AppSyncResolver () @app . resolver ( type_name = \"Query\" , field_name = \"listTodos\" ) async def list_todos (): todos = await some_async_io_call () return todos @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPSYNC_RESOLVER ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): result = app . resolve ( event , context ) return asyncio . run ( result )","title":"Async functions"},{"location":"core/event_handler/appsync/#amplify-graphql-transformer","text":"Assuming you have Amplify CLI installed , create a new API using amplify add api and use the following GraphQL Schema. Example GraphQL Schema 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @model type Merchant { id : String ! name : String ! description : String # Resolves to `common_field` commonField : String @function ( name : \"merchantInfo-${env}\" ) } type Location { id : ID ! name : String ! address : String # Resolves to `common_field` commonField : String @function ( name : \"merchantInfo-${env}\" ) } type Query { # List of locations resolves to `list_locations` listLocations ( page : Int , size : Int ) : [ Location ] @function ( name : \"merchantInfo-${env}\" ) # List of locations resolves to `list_locations` findMerchant ( search : str ) : [ Merchant ] @function ( name : \"searchMerchant-${env}\" ) } Create two new basic Python functions via amplify add function . Note Amplify CLI generated functions use Pipenv as a dependency manager. Your function source code is located at amplify/backend/function/your-function-name . Within your function's folder, add Lambda Powertools as a dependency with pipenv install aws-lambda-powertools . Use the following code for merchantInfo and searchMerchant functions respectively. merchantInfo/src/app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler import AppSyncResolver from aws_lambda_powertools.utilities.data_classes.appsync import scalar_types_utils tracer = Tracer ( service = \"sample_graphql_transformer_resolver\" ) logger = Logger ( service = \"sample_graphql_transformer_resolver\" ) app = AppSyncResolver () @app . resolver ( type_name = \"Query\" , field_name = \"listLocations\" ) def list_locations ( page : int = 0 , size : int = 10 ): return [{ \"id\" : 100 , \"name\" : \"Smooth Grooves\" }] @app . resolver ( field_name = \"commonField\" ) def common_field (): # Would match all fieldNames matching 'commonField' return scalar_types_utils . make_id () @tracer . capture_lambda_handler @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPSYNC_RESOLVER ) def lambda_handler ( event , context ): app . resolve ( event , context ) searchMerchant/src/app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from aws_lambda_powertools.event_handler import AppSyncResolver from aws_lambda_powertools.utilities.data_classes.appsync import scalar_types_utils app = AppSyncResolver () @app . resolver ( type_name = \"Query\" , field_name = \"findMerchant\" ) def find_merchant ( search : str ): return [ { \"id\" : scalar_types_utils . make_id (), \"name\" : \"Brewer Brewing\" , \"description\" : \"Mike Brewer's IPA brewing place\" }, { \"id\" : scalar_types_utils . make_id (), \"name\" : \"Serverlessa's Bakery\" , \"description\" : \"Lessa's sourdough place\" }, ] Example AppSync GraphQL Transformer Function resolver events Query.listLocations event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"typeName\" : \"Query\" , \"fieldName\" : \"listLocations\" , \"arguments\" : { \"page\" : 2 , \"size\" : 1 }, \"identity\" : { \"claims\" : { \"iat\" : 1615366261 ... }, \"username\" : \"mike\" , ... }, \"request\" : { \"headers\" : { \"x-amzn-trace-id\" : \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\" , \"x-forwarded-for\" : \"127.0.0.1\" ... } }, ... } *.commonField event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \"typeName\" : \"Merchant\" , \"fieldName\" : \"commonField\" , \"arguments\" : { }, \"identity\" : { \"claims\" : { \"iat\" : 1615366261 ... }, \"username\" : \"mike\" , ... }, \"request\" : { \"headers\" : { \"x-amzn-trace-id\" : \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\" , \"x-forwarded-for\" : \"127.0.0.1\" ... } }, ... } Query.findMerchant event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"typeName\" : \"Query\" , \"fieldName\" : \"findMerchant\" , \"arguments\" : { \"search\" : \"Brewers Coffee\" }, \"identity\" : { \"claims\" : { \"iat\" : 1615366261 ... }, \"username\" : \"mike\" , ... }, \"request\" : { \"headers\" : { \"x-amzn-trace-id\" : \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\" , \"x-forwarded-for\" : \"127.0.0.1\" ... } }, ... }","title":"Amplify GraphQL Transformer"},{"location":"core/event_handler/appsync/#custom-data-models","text":"You can subclass AppSyncResolverEvent to bring your own set of methods to handle incoming events, by using data_model param in the resolve method. custom_model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.event_handler import AppSyncResolver tracer = Tracer ( service = \"sample_resolver\" ) logger = Logger ( service = \"sample_resolver\" ) app = AppSyncResolver () class MyCustomModel ( AppSyncResolverEvent ): @property def country_viewer ( self ) -> str : return self . request_headers . get ( \"cloudfront-viewer-country\" ) @app . resolver ( field_name = \"listLocations\" ) @app . resolver ( field_name = \"locations\" ) def get_locations ( name : str , description : str = \"\" ): if app . current_event . country_viewer == \"US\" : ... return name + description @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPSYNC_RESOLVER ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context , data_model = MyCustomModel ) schema.graphql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 schema { query : Query } type Query { listLocations : [ Location ] } type Location { id : ID ! name : String ! description : String address : String } type Merchant { id : String ! name : String ! description : String locations : [ Location ] } listLocations_event.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 { \"arguments\" : {}, \"identity\" : null , \"source\" : null , \"request\" : { \"headers\" : { \"x-forwarded-for\" : \"1.2.3.4, 5.6.7.8\" , \"accept-encoding\" : \"gzip, deflate, br\" , \"cloudfront-viewer-country\" : \"NL\" , \"cloudfront-is-tablet-viewer\" : \"false\" , \"referer\" : \"https://eu-west-1.console.aws.amazon.com/appsync/home?region=eu-west-1\" , \"via\" : \"2.0 9fce949f3749407c8e6a75087e168b47.cloudfront.net (CloudFront)\" , \"cloudfront-forwarded-proto\" : \"https\" , \"origin\" : \"https://eu-west-1.console.aws.amazon.com\" , \"x-api-key\" : \"da1-c33ullkbkze3jg5hf5ddgcs4fq\" , \"content-type\" : \"application/json\" , \"x-amzn-trace-id\" : \"Root=1-606eb2f2-1babc433453a332c43fb4494\" , \"x-amz-cf-id\" : \"SJw16ZOPuMZMINx5Xcxa9pB84oMPSGCzNOfrbJLvd80sPa0waCXzYQ==\" , \"content-length\" : \"114\" , \"x-amz-user-agent\" : \"AWS-Console-AppSync/\" , \"x-forwarded-proto\" : \"https\" , \"host\" : \"ldcvmkdnd5az3lm3gnf5ixvcyy.appsync-api.eu-west-1.amazonaws.com\" , \"accept-language\" : \"en-US,en;q=0.5\" , \"user-agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\" , \"cloudfront-is-desktop-viewer\" : \"true\" , \"cloudfront-is-mobile-viewer\" : \"false\" , \"accept\" : \"*/*\" , \"x-forwarded-port\" : \"443\" , \"cloudfront-is-smarttv-viewer\" : \"false\" } }, \"prev\" : null , \"info\" : { \"parentTypeName\" : \"Query\" , \"selectionSetList\" : [ \"id\" , \"name\" , \"description\" ], \"selectionSetGraphQL\" : \"{\\n id\\n name\\n description\\n}\" , \"fieldName\" : \"listLocations\" , \"variables\" : {} }, \"stash\" : {} }","title":"Custom data models"},{"location":"core/event_handler/appsync/#split-operations-with-router","text":"Tip Read the considerations section for trade-offs between monolithic and micro functions , as it's also applicable here. As you grow the number of related GraphQL operations a given Lambda function should handle, it is natural to split them into separate files to ease maintenance - That's where the Router feature is useful. Let's assume you have app.py as your Lambda function entrypoint and routes in location.py , this is how you'd use the Router feature. resolvers/location.py We import Router instead of AppSyncResolver ; syntax wise is exactly the same. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from typing import Any , Dict , List from aws_lambda_powertools import Logger from aws_lambda_powertools.event_handler.appsync import Router logger = Logger ( child = True ) router = Router () @router . resolver ( type_name = \"Query\" , field_name = \"listLocations\" ) def list_locations ( merchant_id : str ) -> List [ Dict [ str , Any ]]: return [{ \"name\" : \"Location name\" , \"merchant_id\" : merchant_id }] @router . resolver ( type_name = \"Location\" , field_name = \"status\" ) def resolve_status ( merchant_id : str ) -> str : logger . debug ( f \"Resolve status for merchant_id: { merchant_id } \" ) return \"FOO\" app.py We use include_router method and include all location operations registered in the router global object. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from typing import Dict from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.event_handler import AppSyncResolver from aws_lambda_powertools.logging.correlation_paths import APPSYNC_RESOLVER from aws_lambda_powertools.utilities.typing import LambdaContext from resolvers import location tracer = Tracer () logger = Logger () app = AppSyncResolver () app . include_router ( location . router ) @tracer . capture_lambda_handler @logger . inject_lambda_context ( correlation_id_path = APPSYNC_RESOLVER ) def lambda_handler ( event : Dict , context : LambdaContext ): app . resolve ( event , context )","title":"Split operations with Router"},{"location":"core/event_handler/appsync/#testing-your-code","text":"You can test your resolvers by passing a mocked or actual AppSync Lambda event that you're expecting. You can use either app.resolve(event, context) or simply app(event, context) . Here's an example from our internal functional test. test_direct_resolver.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def test_direct_resolver (): # Check whether we can handle an example appsync direct resolver # load_event primarily deserialize the JSON event into a dict mock_event = load_event ( \"appSyncDirectResolver.json\" ) app = AppSyncResolver () @app . resolver ( field_name = \"createSomething\" ) def create_something ( id : str ): assert app . lambda_context == {} return id # Call the implicit handler result = app ( mock_event , {}) assert result == \"my identifier\" appSyncDirectResolver.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 { \"arguments\" : { \"id\" : \"my identifier\" }, \"identity\" : { \"claims\" : { \"sub\" : \"192879fc-a240-4bf1-ab5a-d6a00f3063f9\" , \"email_verified\" : true , \"iss\" : \"https://cognito-idp.us-west-2.amazonaws.com/us-west-xxxxxxxxxxx\" , \"phone_number_verified\" : false , \"cognito:username\" : \"jdoe\" , \"aud\" : \"7471s60os7h0uu77i1tk27sp9n\" , \"event_id\" : \"bc334ed8-a938-4474-b644-9547e304e606\" , \"token_use\" : \"id\" , \"auth_time\" : 1599154213 , \"phone_number\" : \"+19999999999\" , \"exp\" : 1599157813 , \"iat\" : 1599154213 , \"email\" : \"jdoe@email.com\" }, \"defaultAuthStrategy\" : \"ALLOW\" , \"groups\" : null , \"issuer\" : \"https://cognito-idp.us-west-2.amazonaws.com/us-west-xxxxxxxxxxx\" , \"sourceIp\" : [ \"1.1.1.1\" ], \"sub\" : \"192879fc-a240-4bf1-ab5a-d6a00f3063f9\" , \"username\" : \"jdoe\" }, \"source\" : null , \"request\" : { \"headers\" : { \"x-forwarded-for\" : \"1.1.1.1, 2.2.2.2\" , \"cloudfront-viewer-country\" : \"US\" , \"cloudfront-is-tablet-viewer\" : \"false\" , \"via\" : \"2.0 xxxxxxxxxxxxxxxx.cloudfront.net (CloudFront)\" , \"cloudfront-forwarded-proto\" : \"https\" , \"origin\" : \"https://us-west-1.console.aws.amazon.com\" , \"content-length\" : \"217\" , \"accept-language\" : \"en-US,en;q=0.9\" , \"host\" : \"xxxxxxxxxxxxxxxx.appsync-api.us-west-1.amazonaws.com\" , \"x-forwarded-proto\" : \"https\" , \"user-agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36\" , \"accept\" : \"*/*\" , \"cloudfront-is-mobile-viewer\" : \"false\" , \"cloudfront-is-smarttv-viewer\" : \"false\" , \"accept-encoding\" : \"gzip, deflate, br\" , \"referer\" : \"https://us-west-1.console.aws.amazon.com/appsync/home?region=us-west-1\" , \"content-type\" : \"application/json\" , \"sec-fetch-mode\" : \"cors\" , \"x-amz-cf-id\" : \"3aykhqlUwQeANU-HGY7E_guV5EkNeMMtwyOgiA==\" , \"x-amzn-trace-id\" : \"Root=1-5f512f51-fac632066c5e848ae714\" , \"authorization\" : \"eyJraWQiOiJScWFCSlJqYVJlM0hrSnBTUFpIcVRXazNOW...\" , \"sec-fetch-dest\" : \"empty\" , \"x-amz-user-agent\" : \"AWS-Console-AppSync/\" , \"cloudfront-is-desktop-viewer\" : \"true\" , \"sec-fetch-site\" : \"cross-site\" , \"x-forwarded-port\" : \"443\" } }, \"prev\" : null , \"info\" : { \"selectionSetList\" : [ \"id\" , \"field1\" , \"field2\" ], \"selectionSetGraphQL\" : \"{\\n id\\n field1\\n field2\\n}\" , \"parentTypeName\" : \"Mutation\" , \"fieldName\" : \"createSomething\" , \"variables\" : {} }, \"stash\" : {} }","title":"Testing your code"},{"location":"tutorial/","text":"This tutorial progressively introduces Lambda Powertools core utilities by using one feature at a time. Requirements \u00b6 AWS CLI and configured with your credentials . AWS SAM CLI installed. Getting started \u00b6 Let's clone our sample project before we add one feature at a time. Tip: Want to skip to the final project? Bootstrap directly via SAM CLI: sam init --location https://github.com/aws-samples/cookiecutter-aws-sam-python Use SAM CLI to initialize the sample project 1 sam init --runtime python3.9 --dependency-manager pip --app-template hello-world --name powertools-quickstart Project structure \u00b6 As we move forward, we will modify the following files within the powertools-quickstart folder: app.py - Application code. template.yaml - AWS infrastructure configuration using SAM. requirements.txt - List of extra Python packages needed. Code example \u00b6 Let's configure our base application to look like the following code snippet. app.py 1 2 3 4 5 6 7 8 9 import json def hello (): return { \"statusCode\" : 200 , \"body\" : json . dumps ({ \"message\" : \"hello unknown!\" })} def lambda_handler ( event , context ): return hello () template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 AWSTemplateFormatVersion : '2010-09-09' Transform : AWS::Serverless-2016-10-31 Description : Sample SAM Template for powertools-quickstart Globals : Function : Timeout : 3 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : CodeUri : hello_world/ Handler : app.lambda_handler Runtime : python3.9 Architectures : - x86_64 Events : HelloWorld : Type : Api Properties : Path : /hello Method : get Outputs : HelloWorldApi : Description : \"API Gateway endpoint URL for Prod stage for Hello World function\" Value : !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\" Our Lambda code consists of an entry point function named lambda_handler , and a hello function. When API Gateway receives a HTTP GET request on /hello route, Lambda will call our lambda_handler function, subsequently calling the hello function. API Gateway will use this response to return the correct HTTP Status Code and payload back to the caller. Warning For simplicity, we do not set up authentication and authorization! You can find more information on how to implement it on AWS SAM documentation . Run your code \u00b6 At each point, you have two ways to run your code: locally and within your AWS account. Local test \u00b6 AWS SAM allows you to execute a serverless application locally by running sam build && sam local start-api in your preferred shell. Build and run API Gateway locally 1 2 3 > sam build && sam local start-api ... 2021 -11-26 17 :43:08 * Running on http://127.0.0.1:3000/ ( Press CTRL+C to quit ) As a result, a local API endpoint will be exposed and you can invoke it using your browser, or your preferred HTTP API client e.g., Postman , httpie , etc. Invoking our function locally via curl 1 2 > curl http://127.0.0.1:3000/hello { \"message\" : \"hello unknown!\" } Info To learn more about local testing, please visit the AWS SAM CLI local testing documentation. Live test \u00b6 First, you need to deploy your application into your AWS Account by issuing sam build && sam deploy --guided command. This command builds a ZIP package of your source code, and deploy it to your AWS Account. Build and deploy your serverless application 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 > sam build && sam deploy --guided ... CloudFormation outputs from deployed stack ------------------------------------------------------------------------------------------------------------------------------------------ Outputs ------------------------------------------------------------------------------------------------------------------------------------------ Key HelloWorldFunctionIamRole Description Implicit IAM Role created for Hello World function Value arn:aws:iam::123456789012:role/sam-app-HelloWorldFunctionRole-1T2W3H9LZHGGV Key HelloWorldApi Description API Gateway endpoint URL for Prod stage for Hello World function Value https://1234567890.execute-api.eu-central-1.amazonaws.com/Prod/hello/ Key HelloWorldFunction Description Hello World Lambda Function ARN Value arn:aws:lambda:eu-central-1:123456789012:function:sam-app-HelloWorldFunction-dOcfAtYoEiGo ------------------------------------------------------------------------------------------------------------------------------------------ Successfully created/updated stack - sam-app in eu-central-1 At the end of the deployment, you will find the API endpoint URL within Outputs section. You can use this URL to test your serverless application. Invoking our application via API endpoint 1 2 > curl https://1234567890.execute-api.eu-central-1.amazonaws.com/Prod/hello { \"message\" : \"hello unknown!\" } % Info For more details on AWS SAM deployment mechanism, see SAM Deploy reference docs . Routing \u00b6 Adding a new route \u00b6 Let's expand our application with a new route - /hello/{name} . It will accept an username as a path input and return it in the response. For this to work, we could create a new Lambda function to handle incoming requests for /hello/{name} - It'd look like this: hello_by_name.py 1 2 3 4 5 6 7 8 9 10 import json def hello_name ( name ): return { \"statusCode\" : 200 , \"body\" : json . dumps ({ \"message\" : f \"hello { name } !\" })} def lambda_handler ( event , context ): name = event [ \"pathParameters\" ][ \"name\" ] return hello_name ( name ) template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 AWSTemplateFormatVersion : \"2010-09-09\" Transform : AWS::Serverless-2016-10-31 Description : Sample SAM Template for powertools-quickstart Globals : Function : Timeout : 3 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : CodeUri : hello_world/ Handler : app.lambda_handler Runtime : python3.9 Events : HelloWorld : Type : Api Properties : Path : /hello Method : get HelloWorldByNameFunctionName : Type : AWS::Serverless::Function Properties : CodeUri : hello_world/ Handler : hello_by_name.lambda_handler Runtime : python3.9 Events : HelloWorldName : Type : Api Properties : Path : /hello/{name} Method : get Outputs : HelloWorldApi : Description : \"API Gateway endpoint URL for Prod stage for Hello World function\" Value : !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\" Question But what happens if your application gets bigger and we need to cover numerous URL paths and HTTP methods for them? This would quickly become non-trivial to maintain . Adding new Lambda function for each path, or multiple if/else to handle several routes & HTTP Methods can be error prone. Creating our own router \u00b6 Question What if we create a simple router to reduce boilerplate? We could group similar routes and intents, separate read and write operations resulting in fewer functions. It doesn't address the boilerplate routing code, but maybe it will be easier to add additional URLs. Info: You might be already asking yourself about mono vs micro-functions If you want a more detailed explanation of these two approaches, head over to the trade-offs on each approach later. A first attempt at the routing logic might look similar to the following code snippet. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import json def hello_name ( event , ** kargs ): username = event [ \"pathParameters\" ][ \"name\" ] return { \"statusCode\" : 200 , \"body\" : json . dumps ({ \"message\" : f \"hello { username } !\" })} def hello ( ** kargs ): return { \"statusCode\" : 200 , \"body\" : json . dumps ({ \"message\" : \"hello unknown!\" })} class Router : def __init__ ( self ): self . routes = {} def set ( self , path , method , handler ): self . routes [ f \" { path } - { method } \" ] = handler def get ( self , path , method ): try : route = self . routes [ f \" { path } - { method } \" ] except KeyError : raise RuntimeError ( f \"Cannot route request to the correct method. path= { path } , method= { method } \" ) return route router = Router () router . set ( path = \"/hello\" , method = \"GET\" , handler = hello ) router . set ( path = \"/hello/ {name} \" , method = \"GET\" , handler = hello_name ) def lambda_handler ( event , context ): path = event [ \"resource\" ] http_method = event [ \"httpMethod\" ] method = router . get ( path = path , method = http_method ) return method ( event = event ) template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 AWSTemplateFormatVersion : \"2010-09-09\" Transform : AWS::Serverless-2016-10-31 Description : Sample SAM Template for powertools-quickstart Globals : Function : Timeout : 3 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : CodeUri : hello_world/ Handler : app.lambda_handler Runtime : python3.9 Events : HelloWorld : Type : Api Properties : Path : /hello Method : get HelloWorldName : Type : Api Properties : Path : /hello/{name} Method : get Outputs : HelloWorldApi : Description : \"API Gateway endpoint URL for Prod stage for Hello World function\" Value : !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\" Let's break this down: L4,9 : We defined two hello_name and hello functions to handle /hello/{name} and /hello routes. L13: We added a Router class to map a path, a method, and the function to call. L27-29 : We create a Router instance and map both /hello and /hello/{name} . L35: We use Router's get method to retrieve a reference to the processing method ( hello or hello_name ). L36: Finally, we run this method and send the results back to API Gateway. This approach simplifies the configuration of our infrastructure since we have added all API Gateway paths in the HelloWorldFunction event section. However, it forces us to understand the internal structure of the API Gateway request events, responses, and it could lead to other errors such as CORS not being handled properly, error handling, etc. Simplifying with Event Handler \u00b6 We can massively simplify cross-cutting concerns while keeping it lightweight by using Event Handler . Tip This is available for both REST API (API Gateway, ALB) and GraphQL API (AppSync) . Let's include Lambda Powertools as a dependency in requirement.txt , and use Event Handler to refactor our previous example. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver app = ApiGatewayResolver () @app . get ( \"/hello/<name>\" ) def hello_name ( name ): return { \"message\" : f \"hello { name } !\" } @app . get ( \"/hello\" ) def hello (): return { \"message\" : \"hello unknown!\" } def lambda_handler ( event , context ): return app . resolve ( event , context ) requirements.txt 1 aws-lambda-powertools Use sam build && sam local start-api and try run it locally again. Note If you're coming from Flask , you will be familiar with this experience already. Event Handler for API Gateway uses ApiGatewayResolver to give a Flask-like experience while staying true to our tenet Keep it lean . We have added the route annotation as the decorator for our methods. It enables us to use the parameters passed in the request directly, and our responses are simply dictionaries. Lastly, we used return app.resolve(event, context) so Event Handler can resolve routes, inject the current request, handle serialization, route validation, etc. From here, we could handle 404 routes , error handling , access query strings, payload , etc. Tip If you'd like to learn how python decorators work under the hood, you can follow Real Python 's article. Structured Logging \u00b6 Over time, you realize that searching logs as text results in poor observability, it's hard to create metrics from, enumerate common exceptions, etc. Then, you decided to propose production quality logging capabilities to your Lambda code. You found out that by having logs as JSON you can structure them , so that you can use any Log Analytics tool out there to quickly analyze them. This helps not only in searching, but produces consistent logs containing enough context and data to ask arbitrary questions on the status of your system. We can take advantage of CloudWatch Logs and Cloudwatch Insight for this purpose. JSON as output \u00b6 The first option could be to use the standard Python Logger, and use a specialized library like pythonjsonlogger to create a JSON Formatter. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import logging import os from pythonjsonlogger import jsonlogger from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver logger = logging . getLogger ( \"APP\" ) logHandler = logging . StreamHandler () formatter = jsonlogger . JsonFormatter ( fmt = \" %(asctime)s %(levelname)s %(name)s %(message)s \" ) logHandler . setFormatter ( formatter ) logger . addHandler ( logHandler ) logger . setLevel ( os . getenv ( \"LOG_LEVEL\" , \"INFO\" )) app = ApiGatewayResolver () @app . get ( \"/hello/<name>\" ) def hello_name ( name ): logger . info ( f \"Request from { name } received\" ) return { \"message\" : f \"hello { name } !\" } @app . get ( \"/hello\" ) def hello (): logger . info ( \"Request from unknown received\" ) return { \"message\" : \"hello unknown!\" } def lambda_handler ( event , context ): logger . debug ( event ) return app . resolve ( event , context ) requirements.txt 1 2 aws-lambda-powertools python-json-logger With just a few lines our logs will now output to JSON format. We've taken the following steps to make that work: L7 : Creates an application logger named APP . L8-11 : Configures handler and formatter. L12 : Sets the logging level set in the LOG_LEVEL environment variable, or INFO as a sentinel value. After that, we use this logger in our application code to record the required information. We see logs structured as follows: JSON output 1 2 3 4 5 6 { \"asctime\" : \"2021-11-22 15:32:02,145\" , \"levelname\" : \"INFO\" , \"name\" : \"APP\" , \"message\" : \"Request from unknown received\" } Normal output 1 [ INFO ] 2021 - 11 - 22 T15 : 32 : 02.145 Z ba3bea3d - fe3a - 45 db - a2ce - 72e813 d55b91 Request from unknown received So far, so good! We can take a step further now by adding additional context to the logs. We could start by creating a dictionary with Lambda context information or something from the incoming event, which should always be logged. Additional attributes could be added on every logger.info using extra keyword like in any standard Python logger. Simplifying with Logger \u00b6 Surely this could be easier, right? Yes! Powertools Logger to the rescue :-) As we already have Lambda Powertools as a dependency, we can simply import Logger . Refactoring with Lambda Powertools Logger 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from aws_lambda_powertools import Logger from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver from aws_lambda_powertools.logging import correlation_paths logger = Logger ( service = \"APP\" ) app = ApiGatewayResolver () @app . get ( \"/hello/<name>\" ) def hello_name ( name ): logger . info ( f \"Request from { name } received\" ) return { \"message\" : f \"hello { name } !\" } @app . get ( \"/hello\" ) def hello (): logger . info ( \"Request from unknown received\" ) return { \"message\" : \"hello unknown!\" } @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST , log_event = True ) def lambda_handler ( event , context ): return app . resolve ( event , context ) Let's break this down: L5 : We add Lambda Powertools Logger; the boilerplate is now done for you. By default, we set INFO as the logging level if LOG_LEVEL env var isn't set. L22 : We use logger.inject_lambda_context decorator to inject key information from Lambda context into every log. L22 : We also instruct Logger to use the incoming API Gateway Request ID as a correlation id automatically. L22 : Since we're in dev, we also use log_event=True to automatically log each incoming request for debugging. This can be also set via environment variables . This is how the logs would look like now: Our logs are now structured consistently 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"hello:17\" , \"message\" : \"Request from unknown received\" , \"timestamp\" : \"2021-10-22 16:29:58,367+0000\" , \"service\" : \"APP\" , \"cold_start\" : true , \"function_name\" : \"HelloWorldFunction\" , \"function_memory_size\" : \"256\" , \"function_arn\" : \"arn:aws:lambda:us-east-1:123456789012:function:HelloWorldFunction\" , \"function_request_id\" : \"d50bb07a-7712-4b2d-9f5d-c837302221a2\" , \"correlation_id\" : \"bf9b584c-e5d9-4ad5-af3d-db953f2b10dc\" } We can now search our logs by the request ID to find a specific operation. Additionally, we can also search our logs for function name, Lambda request ID, Lambda function ARN, find out whether an operation was a cold start, etc. From here, we could set specific keys to add additional contextual information about a given operation, log exceptions to easily enumerate them later, sample debug logs , etc. By having structured logs like this, we can easily search and analyse them in CloudWatch Logs Insight . CloudWatch Logs Insight Example Tracing \u00b6 Note You won't see any traces in AWS X-Ray when executing your function locally. The next improvement is to add distributed tracing to your stack. Traces help you visualize end-to-end transactions or parts of it to easily debug upstream/downstream anomalies. Combined with structured logs, it is an important step to be able to observe how your application runs in production. Generating traces \u00b6 AWS X-Ray is the distributed tracing service we're going to use. But how do we generate application traces in the first place? It's a two-step process : Enable tracing in your Lambda function. Instrument your application code. Let's explore how we can instrument our code with AWS X-Ray SDK , and then simplify it with Lambda Powertools Tracer feature. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from aws_xray_sdk.core import xray_recorder from aws_lambda_powertools import Logger from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver from aws_lambda_powertools.logging import correlation_paths logger = Logger ( service = \"APP\" ) app = ApiGatewayResolver () @app . get ( \"/hello/<name>\" ) @xray_recorder . capture ( 'hello_name' ) def hello_name ( name ): logger . info ( f \"Request from { name } received\" ) return { \"message\" : f \"hello { name } !\" } @app . get ( \"/hello\" ) @xray_recorder . capture ( 'hello' ) def hello (): logger . info ( \"Request from unknown received\" ) return { \"message\" : \"hello unknown!\" } @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST , log_event = True ) @xray_recorder . capture ( 'handler' ) def lambda_handler ( event , context ): return app . resolve ( event , context ) template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 AWSTemplateFormatVersion : \"2010-09-09\" Transform : AWS::Serverless-2016-10-31 Description : Sample SAM Template for powertools-quickstart Globals : Function : Timeout : 3 Api : TracingEnabled : true Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : CodeUri : hello_world/ Handler : app.lambda_handler Runtime : python3.9 Tracing : Active Events : HelloWorld : Type : Api Properties : Path : /hello Method : get HelloWorldName : Type : Api Properties : Path : /hello/{name} Method : get Outputs : HelloWorldApi : Description : \"API Gateway endpoint URL for Prod stage for Hello World function\" Value : !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\" requirements.txt 1 2 aws-lambda-powertools aws-xray-sdk Let's break it down: L1 : First, we import AWS X-Ray SDK. xray_recorder records blocks of code being traced ( subsegment ). It also sends generated traces to the AWS X-Ray daemon running in the Lambda service who subsequently forwards them to AWS X-Ray service. L13,20,27 : We decorate our function so the SDK traces the end-to-end execution, and the argument names the generated block being traced. Question But how do I enable tracing for the Lambda function and what permissions do I need? We've made the following changes in template.yaml for this to work seamless: L7-8 : Enables tracing for Amazon API Gateway. L16 : Enables tracing for our Serverless Function. This will also add a managed IAM Policy named AWSXRayDaemonWriteAccess to allow Lambda to send traces to AWS X-Ray. You can now build and deploy our updates with sam build && sam deploy . Once deployed, try invoking the application via the API endpoint, and visit AWS X-Ray Console to see how much progress we've made so far!! Enriching our generated traces \u00b6 What we've done helps bring an initial visibility, but we can do so much more. Question You're probably asking yourself at least the following questions: What if I want to search traces by customer name? What about grouping traces with cold starts? Better yet, what if we want to include the request or response of our functions as part of the trace? Within AWS X-Ray, we can answer these questions by using two features: tracing Annotations and Metadata . Annotations are simple key-value pairs that are indexed for use with filter expressions . Metadata are key-value pairs with values of any type, including objects and lists, but that are not indexed. Let's put them into action. Enriching traces with annotations and metadata 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 from aws_xray_sdk.core import patch_all , xray_recorder from aws_lambda_powertools import Logger from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver from aws_lambda_powertools.logging import correlation_paths logger = Logger ( service = \"APP\" ) app = ApiGatewayResolver () cold_start = True patch_all () @app . get ( \"/hello/<name>\" ) @xray_recorder . capture ( 'hello_name' ) def hello_name ( name ): subsegment = xray_recorder . current_subsegment () subsegment . put_annotation ( key = \"User\" , value = name ) logger . info ( f \"Request from { name } received\" ) return { \"message\" : f \"hello { name } !\" } @app . get ( \"/hello\" ) @xray_recorder . capture ( 'hello' ) def hello (): subsegment = xray_recorder . current_subsegment () subsegment . put_annotation ( key = \"User\" , value = \"unknown\" ) logger . info ( \"Request from unknown received\" ) return { \"message\" : \"hello unknown!\" } @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST , log_event = True ) @xray_recorder . capture ( 'handler' ) def lambda_handler ( event , context ): global cold_start subsegment = xray_recorder . current_subsegment () if cold_start : subsegment . put_annotation ( key = \"ColdStart\" , value = cold_start ) cold_start = False else : subsegment . put_annotation ( key = \"ColdStart\" , value = cold_start ) result = app . resolve ( event , context ) subsegment . put_metadata ( \"response\" , result ) return result Let's break it down: L10 : We track Lambda cold start by setting global variable outside the handler; this is executed once per sandbox Lambda creates. This information provides an overview of how often the sandbox is reused by Lambda, which directly impacts the performance of each transaction. L17-18 : We use AWS X-Ray SDK to add User annotation on hello_name subsegment. This will allow us to filter traces using the User value. L26-27 : We repeat what we did in L17-18 except we use the value unknown since we don't have that information. L35 : We use global to modify our global variable defined in the outer scope. 37-42 : We add ColdStart annotation and flip the value of cold_start variable, so that subsequent requests annotates the value false when the sandbox is reused. L45 : We include the final response under response key as part of the handler subsegment. Info If you want to understand how the Lambda execution environment (sandbox) works and why cold starts can occur, see this blog series on Lambda performance . Repeat the process of building, deploying, and invoking your application via the API endpoint. Within the AWS X-Ray Console , you should now be able to group traces by the User and ColdStart annotation. If you choose any of the traces available, try opening the handler subsegment and you should see the response of your Lambda function under the Metadata tab. Simplifying with Tracer \u00b6 Cross-cutting concerns like filtering traces by Cold Start, including response as well as exceptions as tracing metadata can take a considerable amount of boilerplate. We can simplify our previous patterns by using Lambda Powertools Tracer ; a thin wrapper on top of X-Ray SDK. Note You can now safely remove aws-xray-sdk from requirements.txt ; keep aws-lambda-powertools only. Refactoring with Lambda Powertools Tracer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver from aws_lambda_powertools.logging import correlation_paths logger = Logger ( service = \"APP\" ) tracer = Tracer ( service = \"APP\" ) app = ApiGatewayResolver () @app . get ( \"/hello/<name>\" ) @tracer . capture_method def hello_name ( name ): tracer . put_annotation ( key = \"User\" , value = name ) logger . info ( f \"Request from { name } received\" ) return { \"message\" : f \"hello { name } !\" } @app . get ( \"/hello\" ) @tracer . capture_method def hello (): tracer . put_annotation ( key = \"User\" , value = \"unknown\" ) logger . info ( \"Request from unknown received\" ) return { \"message\" : \"hello unknown!\" } @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST , log_event = True ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) Decorators, annotations and metadata are largely the same, except we now have a much cleaner code as the boilerplate is gone. Here's what's changed compared to AWS X-Ray SDK approach: L6 : We initialize Tracer and define the name of our service ( APP ). We automatically run patch_all from AWS X-Ray SDK on your behalf. Any previously patched or non-imported library is simply ignored. L11 : We use @tracer.capture_method decorator instead of xray_recorder.capture . We automatically create a subsegment named after the function name ( ## hello_name ), and add the response/exception as tracing metadata. L13 : Putting annotations remain exactly the same UX. L27 : We use @tracer.lambda_handler so we automatically add ColdStart annotation within Tracer itself. We also add a new Service annotation using the value of Tracer(service=\"APP\") , so that you can filter traces by the service your function(s) represent. Another subtle difference is that you can now run your Lambda functions and unit test them locally without having to explicitly disable Tracer. Lambda Powertools optimizes for Lambda compute environment. As such, we add these and other common approaches to accelerate your development, so you don't worry about implementing every cross-cutting concern. Tip You can opt-out some of these behaviours like disabling response capturing, explicitly patching only X modules, etc. Repeat the process of building, deploying, and invoking your application via the API endpoint. Within the AWS X-Ray Console , you should see a similar view: Tip Consider using Amazon CloudWatch ServiceLens view as it aggregates AWS X-Ray traces and CloudWatch metrics and logs in one view. From here, you can browse to specific logs in CloudWatch Logs Insight, Metrics Dashboard or AWS X-Ray traces. Info For more information on Amazon CloudWatch ServiceLens, please visit link . Custom Metrics \u00b6 Creating metrics \u00b6 Let's add custom metrics to better understand our application and business behavior (e.g. number of reservations, etc.). Out of the box, AWS Lambda adds invocation, performance, and concurrency metrics . Amazon API Gateway also adds general metrics at the aggregate level such as latency, number of requests received, etc. Tip You can optionally enable detailed metrics per each API route, stage, and method in API Gateway. Let's expand our application with custom metrics using AWS SDK to see how it works, then let's upgrade it with Lambda Powertools :-) app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 import os import boto3 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver from aws_lambda_powertools.logging import correlation_paths cold_start = True metric_namespace = \"MyApp\" logger = Logger ( service = \"APP\" ) tracer = Tracer ( service = \"APP\" ) metrics = boto3 . client ( \"cloudwatch\" ) app = ApiGatewayResolver () @tracer . capture_method def add_greeting_metric ( service : str = \"APP\" ): function_name = os . getenv ( \"AWS_LAMBDA_FUNCTION_NAME\" , \"undefined\" ) service_dimension = { \"Name\" : \"service\" , \"Value\" : service } function_dimension = { \"Name\" : \"function_name\" : \"Value\" : function_name } is_cold_start = True global cold_start if cold_start : cold_start = False else : is_cold_start = False return metrics . put_metric_data ( MetricData = [ { \"MetricName\" : \"SuccessfulGreetings\" , \"Dimensions\" : [ service_dimension ], \"Unit\" : \"Count\" , \"Value\" : 1 , }, { \"MetricName\" : \"ColdStart\" , \"Dimensions\" : [ service_dimension , function_dimension ], \"Unit\" : \"Count\" , \"Value\" : int ( is_cold_start ) } ], Namespace = metric_namespace , ) @app . get ( \"/hello/<name>\" ) @tracer . capture_method def hello_name ( name ): tracer . put_annotation ( key = \"User\" , value = name ) logger . info ( f \"Request from { name } received\" ) add_greeting_metric () return { \"message\" : f \"hello { name } !\" } @app . get ( \"/hello\" ) @tracer . capture_method def hello (): tracer . put_annotation ( key = \"User\" , value = \"unknown\" ) logger . info ( \"Request from unknown received\" ) add_greeting_metric () return { \"message\" : \"hello unknown!\" } @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST , log_event = True ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 AWSTemplateFormatVersion : \"2010-09-09\" Transform : AWS::Serverless-2016-10-31 Description : Sample SAM Template for powertools-quickstart Globals : Function : Timeout : 3 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : CodeUri : hello_world/ Handler : app.lambda_handler Runtime : python3.9 Tracing : Active Events : HelloWorld : Type : Api Properties : Path : /hello Method : get HelloWorldName : Type : Api Properties : Path : /hello/{name} Method : get Policies : - CloudWatchPutMetricPolicy : {} Outputs : HelloWorldApi : Description : \"API Gateway endpoint URL for Prod stage for Hello World function\" Value : !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\" There's a lot going on, let's break this down: L10 : We define a container where all of our application metrics will live MyApp , a.k.a Metrics Namespace . L14 : We initialize a CloudWatch client to send metrics later. L19-47 : We create a custom function to prepare and send ColdStart and SuccessfulGreetings metrics using CloudWatch expected data structure. We also set dimensions of these metrics - Think of them as metadata to define to slice and dice them later; an unique metric is a combination of metric name + metric dimension(s). L55,64 : We call our custom function to create metrics for every greeting received. Question But what permissions do I need to send metrics to CloudWatch? Within template.yaml , we add CloudWatchPutMetricPolicy policy in SAM. Adding metrics via AWS SDK gives a lot of flexibility at a cost put_metric_data is a synchronous call to CloudWatch Metrics API. This means establishing a connection to CloudWatch endpoint, sending metrics payload, and waiting from a response. It will be visible in your AWS X-RAY traces as additional external call. Given your architecture scale, this approach might lead to disadvantages such as increased cost of measuring data collection and increased Lambda latency. Simplifying with Metrics \u00b6 Lambda Powertools Metrics uses Amazon CloudWatch Embedded Metric Format (EMF) to create custom metrics asynchronously via a native integration with Lambda. In general terms, EMF is a specification that expects metrics in a JSON payload within CloudWatch Logs. Lambda ingests all logs emitted by a given function into CloudWatch Logs. CloudWatch automatically looks up for log entries that follow the EMF format and transforms them into a CloudWatch metric. Info If you are interested in the details of the EMF mechanism, follow blog post . Let's implement that using Metrics : Refactoring with Lambda Powertools Metrics 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 from aws_lambda_powertools import Logger , Tracer , Metrics from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.metrics import MetricUnit logger = Logger ( service = \"APP\" ) tracer = Tracer ( service = \"APP\" ) metrics = Metrics ( namespace = \"MyApp\" , service = \"APP\" ) app = ApiGatewayResolver () @app . get ( \"/hello/<name>\" ) @tracer . capture_method def hello_name ( name ): tracer . put_annotation ( key = \"User\" , value = name ) logger . info ( f \"Request from { name } received\" ) metrics . add_metric ( name = \"SuccessfulGreetings\" , unit = MetricUnit . Count , value = 1 ) return { \"message\" : f \"hello { name } !\" } @app . get ( \"/hello\" ) @tracer . capture_method def hello (): tracer . put_annotation ( key = \"User\" , value = \"unknown\" ) logger . info ( \"Request from unknown received\" ) metrics . add_metric ( name = \"SuccessfulGreetings\" , unit = MetricUnit . Count , value = 1 ) return { \"message\" : \"hello unknown!\" } @tracer . capture_lambda_handler @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST , log_event = True ) @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( event , context ): try : return app . resolve ( event , context ) except Exception as e : logger . exception ( e ) raise That's a lot less boilerplate code! Let's break this down: L9 : We initialize Metrics with our service name ( APP ) and metrics namespace ( MyApp ), reducing the need to add the service dimension for every metric and setting the namespace later L18, 27 : We use add_metric similarly to our custom function, except we now have an enum MetricCount to help us understand which Metric Units we have at our disposal L33 : We use @metrics.log_metrics decorator to ensure that our metrics are aligned with the EMF output and validated before-hand, like in case we forget to set namespace, or accidentally use a metric unit as a string that doesn't exist in CloudWatch. L33 : We also use capture_cold_start_metric=True so we don't have to handle that logic either. Note that Metrics does not publish a warm invocation metric (ColdStart=0) for cost reasons. As such, treat the absence (sparse metric) as a non-cold start invocation. Repeat the process of building, deploying, and invoking your application via the API endpoint a few times to generate metrics - Artillery and K6.io are quick ways to generate some load. Within CloudWatch Metrics view , you should see MyApp custom namespace with your custom metrics there and SuccessfulGreetings available to graph. If you're curious about how the EMF portion of your function logs look like, you can quickly go to CloudWatch ServiceLens view , choose your function and open logs. You will see a similar entry that looks like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"_aws\" : { \"Timestamp\" : 1638115724269 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"CustomMetrics\" , \"Dimensions\" : [ [ \"method\" , \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"AppMethodsInvocations\" , \"Unit\" : \"Count\" } ] } ] }, \"method\" : \"/hello/<name>\" , \"service\" : \"APP\" , \"AppMethodsInvocations\" : [ 1 ] } Final considerations \u00b6 We covered a lot of ground here and we only scratched the surface of the feature set available within Lambda Powertools. When it comes to the observability features ( Tracer , Metrics , Logging ), don't stop there! The goal here is to ensure you can ask arbitrary questions to assess your system's health; these features are only part of the wider story! This requires a change in mindset to ensure operational excellence is part of the software development lifecycle. Tip You can find more details on other leading practices described in the Well-Architected Serverless Lens . Lambda Powertools is largely designed to make some of these practices easier to adopt from day 1. Have ideas for other tutorials? You can open up a documentation issue , or connect with us on the AWS Developers Slack at lambda-powertools channel, or via e-mail aws-lambda-powertools-feedback@amazon.com .","title":"Tutorial"},{"location":"tutorial/#requirements","text":"AWS CLI and configured with your credentials . AWS SAM CLI installed.","title":"Requirements"},{"location":"tutorial/#getting-started","text":"Let's clone our sample project before we add one feature at a time. Tip: Want to skip to the final project? Bootstrap directly via SAM CLI: sam init --location https://github.com/aws-samples/cookiecutter-aws-sam-python Use SAM CLI to initialize the sample project 1 sam init --runtime python3.9 --dependency-manager pip --app-template hello-world --name powertools-quickstart","title":"Getting started"},{"location":"tutorial/#project-structure","text":"As we move forward, we will modify the following files within the powertools-quickstart folder: app.py - Application code. template.yaml - AWS infrastructure configuration using SAM. requirements.txt - List of extra Python packages needed.","title":"Project structure"},{"location":"tutorial/#code-example","text":"Let's configure our base application to look like the following code snippet. app.py 1 2 3 4 5 6 7 8 9 import json def hello (): return { \"statusCode\" : 200 , \"body\" : json . dumps ({ \"message\" : \"hello unknown!\" })} def lambda_handler ( event , context ): return hello () template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 AWSTemplateFormatVersion : '2010-09-09' Transform : AWS::Serverless-2016-10-31 Description : Sample SAM Template for powertools-quickstart Globals : Function : Timeout : 3 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : CodeUri : hello_world/ Handler : app.lambda_handler Runtime : python3.9 Architectures : - x86_64 Events : HelloWorld : Type : Api Properties : Path : /hello Method : get Outputs : HelloWorldApi : Description : \"API Gateway endpoint URL for Prod stage for Hello World function\" Value : !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\" Our Lambda code consists of an entry point function named lambda_handler , and a hello function. When API Gateway receives a HTTP GET request on /hello route, Lambda will call our lambda_handler function, subsequently calling the hello function. API Gateway will use this response to return the correct HTTP Status Code and payload back to the caller. Warning For simplicity, we do not set up authentication and authorization! You can find more information on how to implement it on AWS SAM documentation .","title":"Code example"},{"location":"tutorial/#run-your-code","text":"At each point, you have two ways to run your code: locally and within your AWS account.","title":"Run your code"},{"location":"tutorial/#local-test","text":"AWS SAM allows you to execute a serverless application locally by running sam build && sam local start-api in your preferred shell. Build and run API Gateway locally 1 2 3 > sam build && sam local start-api ... 2021 -11-26 17 :43:08 * Running on http://127.0.0.1:3000/ ( Press CTRL+C to quit ) As a result, a local API endpoint will be exposed and you can invoke it using your browser, or your preferred HTTP API client e.g., Postman , httpie , etc. Invoking our function locally via curl 1 2 > curl http://127.0.0.1:3000/hello { \"message\" : \"hello unknown!\" } Info To learn more about local testing, please visit the AWS SAM CLI local testing documentation.","title":"Local test"},{"location":"tutorial/#live-test","text":"First, you need to deploy your application into your AWS Account by issuing sam build && sam deploy --guided command. This command builds a ZIP package of your source code, and deploy it to your AWS Account. Build and deploy your serverless application 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 > sam build && sam deploy --guided ... CloudFormation outputs from deployed stack ------------------------------------------------------------------------------------------------------------------------------------------ Outputs ------------------------------------------------------------------------------------------------------------------------------------------ Key HelloWorldFunctionIamRole Description Implicit IAM Role created for Hello World function Value arn:aws:iam::123456789012:role/sam-app-HelloWorldFunctionRole-1T2W3H9LZHGGV Key HelloWorldApi Description API Gateway endpoint URL for Prod stage for Hello World function Value https://1234567890.execute-api.eu-central-1.amazonaws.com/Prod/hello/ Key HelloWorldFunction Description Hello World Lambda Function ARN Value arn:aws:lambda:eu-central-1:123456789012:function:sam-app-HelloWorldFunction-dOcfAtYoEiGo ------------------------------------------------------------------------------------------------------------------------------------------ Successfully created/updated stack - sam-app in eu-central-1 At the end of the deployment, you will find the API endpoint URL within Outputs section. You can use this URL to test your serverless application. Invoking our application via API endpoint 1 2 > curl https://1234567890.execute-api.eu-central-1.amazonaws.com/Prod/hello { \"message\" : \"hello unknown!\" } % Info For more details on AWS SAM deployment mechanism, see SAM Deploy reference docs .","title":"Live test"},{"location":"tutorial/#routing","text":"","title":"Routing"},{"location":"tutorial/#adding-a-new-route","text":"Let's expand our application with a new route - /hello/{name} . It will accept an username as a path input and return it in the response. For this to work, we could create a new Lambda function to handle incoming requests for /hello/{name} - It'd look like this: hello_by_name.py 1 2 3 4 5 6 7 8 9 10 import json def hello_name ( name ): return { \"statusCode\" : 200 , \"body\" : json . dumps ({ \"message\" : f \"hello { name } !\" })} def lambda_handler ( event , context ): name = event [ \"pathParameters\" ][ \"name\" ] return hello_name ( name ) template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 AWSTemplateFormatVersion : \"2010-09-09\" Transform : AWS::Serverless-2016-10-31 Description : Sample SAM Template for powertools-quickstart Globals : Function : Timeout : 3 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : CodeUri : hello_world/ Handler : app.lambda_handler Runtime : python3.9 Events : HelloWorld : Type : Api Properties : Path : /hello Method : get HelloWorldByNameFunctionName : Type : AWS::Serverless::Function Properties : CodeUri : hello_world/ Handler : hello_by_name.lambda_handler Runtime : python3.9 Events : HelloWorldName : Type : Api Properties : Path : /hello/{name} Method : get Outputs : HelloWorldApi : Description : \"API Gateway endpoint URL for Prod stage for Hello World function\" Value : !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\" Question But what happens if your application gets bigger and we need to cover numerous URL paths and HTTP methods for them? This would quickly become non-trivial to maintain . Adding new Lambda function for each path, or multiple if/else to handle several routes & HTTP Methods can be error prone.","title":"Adding a new route"},{"location":"tutorial/#creating-our-own-router","text":"Question What if we create a simple router to reduce boilerplate? We could group similar routes and intents, separate read and write operations resulting in fewer functions. It doesn't address the boilerplate routing code, but maybe it will be easier to add additional URLs. Info: You might be already asking yourself about mono vs micro-functions If you want a more detailed explanation of these two approaches, head over to the trade-offs on each approach later. A first attempt at the routing logic might look similar to the following code snippet. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import json def hello_name ( event , ** kargs ): username = event [ \"pathParameters\" ][ \"name\" ] return { \"statusCode\" : 200 , \"body\" : json . dumps ({ \"message\" : f \"hello { username } !\" })} def hello ( ** kargs ): return { \"statusCode\" : 200 , \"body\" : json . dumps ({ \"message\" : \"hello unknown!\" })} class Router : def __init__ ( self ): self . routes = {} def set ( self , path , method , handler ): self . routes [ f \" { path } - { method } \" ] = handler def get ( self , path , method ): try : route = self . routes [ f \" { path } - { method } \" ] except KeyError : raise RuntimeError ( f \"Cannot route request to the correct method. path= { path } , method= { method } \" ) return route router = Router () router . set ( path = \"/hello\" , method = \"GET\" , handler = hello ) router . set ( path = \"/hello/ {name} \" , method = \"GET\" , handler = hello_name ) def lambda_handler ( event , context ): path = event [ \"resource\" ] http_method = event [ \"httpMethod\" ] method = router . get ( path = path , method = http_method ) return method ( event = event ) template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 AWSTemplateFormatVersion : \"2010-09-09\" Transform : AWS::Serverless-2016-10-31 Description : Sample SAM Template for powertools-quickstart Globals : Function : Timeout : 3 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : CodeUri : hello_world/ Handler : app.lambda_handler Runtime : python3.9 Events : HelloWorld : Type : Api Properties : Path : /hello Method : get HelloWorldName : Type : Api Properties : Path : /hello/{name} Method : get Outputs : HelloWorldApi : Description : \"API Gateway endpoint URL for Prod stage for Hello World function\" Value : !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\" Let's break this down: L4,9 : We defined two hello_name and hello functions to handle /hello/{name} and /hello routes. L13: We added a Router class to map a path, a method, and the function to call. L27-29 : We create a Router instance and map both /hello and /hello/{name} . L35: We use Router's get method to retrieve a reference to the processing method ( hello or hello_name ). L36: Finally, we run this method and send the results back to API Gateway. This approach simplifies the configuration of our infrastructure since we have added all API Gateway paths in the HelloWorldFunction event section. However, it forces us to understand the internal structure of the API Gateway request events, responses, and it could lead to other errors such as CORS not being handled properly, error handling, etc.","title":"Creating our own router"},{"location":"tutorial/#simplifying-with-event-handler","text":"We can massively simplify cross-cutting concerns while keeping it lightweight by using Event Handler . Tip This is available for both REST API (API Gateway, ALB) and GraphQL API (AppSync) . Let's include Lambda Powertools as a dependency in requirement.txt , and use Event Handler to refactor our previous example. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver app = ApiGatewayResolver () @app . get ( \"/hello/<name>\" ) def hello_name ( name ): return { \"message\" : f \"hello { name } !\" } @app . get ( \"/hello\" ) def hello (): return { \"message\" : \"hello unknown!\" } def lambda_handler ( event , context ): return app . resolve ( event , context ) requirements.txt 1 aws-lambda-powertools Use sam build && sam local start-api and try run it locally again. Note If you're coming from Flask , you will be familiar with this experience already. Event Handler for API Gateway uses ApiGatewayResolver to give a Flask-like experience while staying true to our tenet Keep it lean . We have added the route annotation as the decorator for our methods. It enables us to use the parameters passed in the request directly, and our responses are simply dictionaries. Lastly, we used return app.resolve(event, context) so Event Handler can resolve routes, inject the current request, handle serialization, route validation, etc. From here, we could handle 404 routes , error handling , access query strings, payload , etc. Tip If you'd like to learn how python decorators work under the hood, you can follow Real Python 's article.","title":"Simplifying with Event Handler"},{"location":"tutorial/#structured-logging","text":"Over time, you realize that searching logs as text results in poor observability, it's hard to create metrics from, enumerate common exceptions, etc. Then, you decided to propose production quality logging capabilities to your Lambda code. You found out that by having logs as JSON you can structure them , so that you can use any Log Analytics tool out there to quickly analyze them. This helps not only in searching, but produces consistent logs containing enough context and data to ask arbitrary questions on the status of your system. We can take advantage of CloudWatch Logs and Cloudwatch Insight for this purpose.","title":"Structured Logging"},{"location":"tutorial/#json-as-output","text":"The first option could be to use the standard Python Logger, and use a specialized library like pythonjsonlogger to create a JSON Formatter. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import logging import os from pythonjsonlogger import jsonlogger from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver logger = logging . getLogger ( \"APP\" ) logHandler = logging . StreamHandler () formatter = jsonlogger . JsonFormatter ( fmt = \" %(asctime)s %(levelname)s %(name)s %(message)s \" ) logHandler . setFormatter ( formatter ) logger . addHandler ( logHandler ) logger . setLevel ( os . getenv ( \"LOG_LEVEL\" , \"INFO\" )) app = ApiGatewayResolver () @app . get ( \"/hello/<name>\" ) def hello_name ( name ): logger . info ( f \"Request from { name } received\" ) return { \"message\" : f \"hello { name } !\" } @app . get ( \"/hello\" ) def hello (): logger . info ( \"Request from unknown received\" ) return { \"message\" : \"hello unknown!\" } def lambda_handler ( event , context ): logger . debug ( event ) return app . resolve ( event , context ) requirements.txt 1 2 aws-lambda-powertools python-json-logger With just a few lines our logs will now output to JSON format. We've taken the following steps to make that work: L7 : Creates an application logger named APP . L8-11 : Configures handler and formatter. L12 : Sets the logging level set in the LOG_LEVEL environment variable, or INFO as a sentinel value. After that, we use this logger in our application code to record the required information. We see logs structured as follows: JSON output 1 2 3 4 5 6 { \"asctime\" : \"2021-11-22 15:32:02,145\" , \"levelname\" : \"INFO\" , \"name\" : \"APP\" , \"message\" : \"Request from unknown received\" } Normal output 1 [ INFO ] 2021 - 11 - 22 T15 : 32 : 02.145 Z ba3bea3d - fe3a - 45 db - a2ce - 72e813 d55b91 Request from unknown received So far, so good! We can take a step further now by adding additional context to the logs. We could start by creating a dictionary with Lambda context information or something from the incoming event, which should always be logged. Additional attributes could be added on every logger.info using extra keyword like in any standard Python logger.","title":"JSON as output"},{"location":"tutorial/#simplifying-with-logger","text":"Surely this could be easier, right? Yes! Powertools Logger to the rescue :-) As we already have Lambda Powertools as a dependency, we can simply import Logger . Refactoring with Lambda Powertools Logger 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from aws_lambda_powertools import Logger from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver from aws_lambda_powertools.logging import correlation_paths logger = Logger ( service = \"APP\" ) app = ApiGatewayResolver () @app . get ( \"/hello/<name>\" ) def hello_name ( name ): logger . info ( f \"Request from { name } received\" ) return { \"message\" : f \"hello { name } !\" } @app . get ( \"/hello\" ) def hello (): logger . info ( \"Request from unknown received\" ) return { \"message\" : \"hello unknown!\" } @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST , log_event = True ) def lambda_handler ( event , context ): return app . resolve ( event , context ) Let's break this down: L5 : We add Lambda Powertools Logger; the boilerplate is now done for you. By default, we set INFO as the logging level if LOG_LEVEL env var isn't set. L22 : We use logger.inject_lambda_context decorator to inject key information from Lambda context into every log. L22 : We also instruct Logger to use the incoming API Gateway Request ID as a correlation id automatically. L22 : Since we're in dev, we also use log_event=True to automatically log each incoming request for debugging. This can be also set via environment variables . This is how the logs would look like now: Our logs are now structured consistently 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"hello:17\" , \"message\" : \"Request from unknown received\" , \"timestamp\" : \"2021-10-22 16:29:58,367+0000\" , \"service\" : \"APP\" , \"cold_start\" : true , \"function_name\" : \"HelloWorldFunction\" , \"function_memory_size\" : \"256\" , \"function_arn\" : \"arn:aws:lambda:us-east-1:123456789012:function:HelloWorldFunction\" , \"function_request_id\" : \"d50bb07a-7712-4b2d-9f5d-c837302221a2\" , \"correlation_id\" : \"bf9b584c-e5d9-4ad5-af3d-db953f2b10dc\" } We can now search our logs by the request ID to find a specific operation. Additionally, we can also search our logs for function name, Lambda request ID, Lambda function ARN, find out whether an operation was a cold start, etc. From here, we could set specific keys to add additional contextual information about a given operation, log exceptions to easily enumerate them later, sample debug logs , etc. By having structured logs like this, we can easily search and analyse them in CloudWatch Logs Insight . CloudWatch Logs Insight Example","title":"Simplifying with Logger"},{"location":"tutorial/#tracing","text":"Note You won't see any traces in AWS X-Ray when executing your function locally. The next improvement is to add distributed tracing to your stack. Traces help you visualize end-to-end transactions or parts of it to easily debug upstream/downstream anomalies. Combined with structured logs, it is an important step to be able to observe how your application runs in production.","title":"Tracing"},{"location":"tutorial/#generating-traces","text":"AWS X-Ray is the distributed tracing service we're going to use. But how do we generate application traces in the first place? It's a two-step process : Enable tracing in your Lambda function. Instrument your application code. Let's explore how we can instrument our code with AWS X-Ray SDK , and then simplify it with Lambda Powertools Tracer feature. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from aws_xray_sdk.core import xray_recorder from aws_lambda_powertools import Logger from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver from aws_lambda_powertools.logging import correlation_paths logger = Logger ( service = \"APP\" ) app = ApiGatewayResolver () @app . get ( \"/hello/<name>\" ) @xray_recorder . capture ( 'hello_name' ) def hello_name ( name ): logger . info ( f \"Request from { name } received\" ) return { \"message\" : f \"hello { name } !\" } @app . get ( \"/hello\" ) @xray_recorder . capture ( 'hello' ) def hello (): logger . info ( \"Request from unknown received\" ) return { \"message\" : \"hello unknown!\" } @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST , log_event = True ) @xray_recorder . capture ( 'handler' ) def lambda_handler ( event , context ): return app . resolve ( event , context ) template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 AWSTemplateFormatVersion : \"2010-09-09\" Transform : AWS::Serverless-2016-10-31 Description : Sample SAM Template for powertools-quickstart Globals : Function : Timeout : 3 Api : TracingEnabled : true Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : CodeUri : hello_world/ Handler : app.lambda_handler Runtime : python3.9 Tracing : Active Events : HelloWorld : Type : Api Properties : Path : /hello Method : get HelloWorldName : Type : Api Properties : Path : /hello/{name} Method : get Outputs : HelloWorldApi : Description : \"API Gateway endpoint URL for Prod stage for Hello World function\" Value : !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\" requirements.txt 1 2 aws-lambda-powertools aws-xray-sdk Let's break it down: L1 : First, we import AWS X-Ray SDK. xray_recorder records blocks of code being traced ( subsegment ). It also sends generated traces to the AWS X-Ray daemon running in the Lambda service who subsequently forwards them to AWS X-Ray service. L13,20,27 : We decorate our function so the SDK traces the end-to-end execution, and the argument names the generated block being traced. Question But how do I enable tracing for the Lambda function and what permissions do I need? We've made the following changes in template.yaml for this to work seamless: L7-8 : Enables tracing for Amazon API Gateway. L16 : Enables tracing for our Serverless Function. This will also add a managed IAM Policy named AWSXRayDaemonWriteAccess to allow Lambda to send traces to AWS X-Ray. You can now build and deploy our updates with sam build && sam deploy . Once deployed, try invoking the application via the API endpoint, and visit AWS X-Ray Console to see how much progress we've made so far!!","title":"Generating traces"},{"location":"tutorial/#enriching-our-generated-traces","text":"What we've done helps bring an initial visibility, but we can do so much more. Question You're probably asking yourself at least the following questions: What if I want to search traces by customer name? What about grouping traces with cold starts? Better yet, what if we want to include the request or response of our functions as part of the trace? Within AWS X-Ray, we can answer these questions by using two features: tracing Annotations and Metadata . Annotations are simple key-value pairs that are indexed for use with filter expressions . Metadata are key-value pairs with values of any type, including objects and lists, but that are not indexed. Let's put them into action. Enriching traces with annotations and metadata 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 from aws_xray_sdk.core import patch_all , xray_recorder from aws_lambda_powertools import Logger from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver from aws_lambda_powertools.logging import correlation_paths logger = Logger ( service = \"APP\" ) app = ApiGatewayResolver () cold_start = True patch_all () @app . get ( \"/hello/<name>\" ) @xray_recorder . capture ( 'hello_name' ) def hello_name ( name ): subsegment = xray_recorder . current_subsegment () subsegment . put_annotation ( key = \"User\" , value = name ) logger . info ( f \"Request from { name } received\" ) return { \"message\" : f \"hello { name } !\" } @app . get ( \"/hello\" ) @xray_recorder . capture ( 'hello' ) def hello (): subsegment = xray_recorder . current_subsegment () subsegment . put_annotation ( key = \"User\" , value = \"unknown\" ) logger . info ( \"Request from unknown received\" ) return { \"message\" : \"hello unknown!\" } @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST , log_event = True ) @xray_recorder . capture ( 'handler' ) def lambda_handler ( event , context ): global cold_start subsegment = xray_recorder . current_subsegment () if cold_start : subsegment . put_annotation ( key = \"ColdStart\" , value = cold_start ) cold_start = False else : subsegment . put_annotation ( key = \"ColdStart\" , value = cold_start ) result = app . resolve ( event , context ) subsegment . put_metadata ( \"response\" , result ) return result Let's break it down: L10 : We track Lambda cold start by setting global variable outside the handler; this is executed once per sandbox Lambda creates. This information provides an overview of how often the sandbox is reused by Lambda, which directly impacts the performance of each transaction. L17-18 : We use AWS X-Ray SDK to add User annotation on hello_name subsegment. This will allow us to filter traces using the User value. L26-27 : We repeat what we did in L17-18 except we use the value unknown since we don't have that information. L35 : We use global to modify our global variable defined in the outer scope. 37-42 : We add ColdStart annotation and flip the value of cold_start variable, so that subsequent requests annotates the value false when the sandbox is reused. L45 : We include the final response under response key as part of the handler subsegment. Info If you want to understand how the Lambda execution environment (sandbox) works and why cold starts can occur, see this blog series on Lambda performance . Repeat the process of building, deploying, and invoking your application via the API endpoint. Within the AWS X-Ray Console , you should now be able to group traces by the User and ColdStart annotation. If you choose any of the traces available, try opening the handler subsegment and you should see the response of your Lambda function under the Metadata tab.","title":"Enriching our generated traces"},{"location":"tutorial/#simplifying-with-tracer","text":"Cross-cutting concerns like filtering traces by Cold Start, including response as well as exceptions as tracing metadata can take a considerable amount of boilerplate. We can simplify our previous patterns by using Lambda Powertools Tracer ; a thin wrapper on top of X-Ray SDK. Note You can now safely remove aws-xray-sdk from requirements.txt ; keep aws-lambda-powertools only. Refactoring with Lambda Powertools Tracer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver from aws_lambda_powertools.logging import correlation_paths logger = Logger ( service = \"APP\" ) tracer = Tracer ( service = \"APP\" ) app = ApiGatewayResolver () @app . get ( \"/hello/<name>\" ) @tracer . capture_method def hello_name ( name ): tracer . put_annotation ( key = \"User\" , value = name ) logger . info ( f \"Request from { name } received\" ) return { \"message\" : f \"hello { name } !\" } @app . get ( \"/hello\" ) @tracer . capture_method def hello (): tracer . put_annotation ( key = \"User\" , value = \"unknown\" ) logger . info ( \"Request from unknown received\" ) return { \"message\" : \"hello unknown!\" } @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST , log_event = True ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) Decorators, annotations and metadata are largely the same, except we now have a much cleaner code as the boilerplate is gone. Here's what's changed compared to AWS X-Ray SDK approach: L6 : We initialize Tracer and define the name of our service ( APP ). We automatically run patch_all from AWS X-Ray SDK on your behalf. Any previously patched or non-imported library is simply ignored. L11 : We use @tracer.capture_method decorator instead of xray_recorder.capture . We automatically create a subsegment named after the function name ( ## hello_name ), and add the response/exception as tracing metadata. L13 : Putting annotations remain exactly the same UX. L27 : We use @tracer.lambda_handler so we automatically add ColdStart annotation within Tracer itself. We also add a new Service annotation using the value of Tracer(service=\"APP\") , so that you can filter traces by the service your function(s) represent. Another subtle difference is that you can now run your Lambda functions and unit test them locally without having to explicitly disable Tracer. Lambda Powertools optimizes for Lambda compute environment. As such, we add these and other common approaches to accelerate your development, so you don't worry about implementing every cross-cutting concern. Tip You can opt-out some of these behaviours like disabling response capturing, explicitly patching only X modules, etc. Repeat the process of building, deploying, and invoking your application via the API endpoint. Within the AWS X-Ray Console , you should see a similar view: Tip Consider using Amazon CloudWatch ServiceLens view as it aggregates AWS X-Ray traces and CloudWatch metrics and logs in one view. From here, you can browse to specific logs in CloudWatch Logs Insight, Metrics Dashboard or AWS X-Ray traces. Info For more information on Amazon CloudWatch ServiceLens, please visit link .","title":"Simplifying with Tracer"},{"location":"tutorial/#custom-metrics","text":"","title":"Custom Metrics"},{"location":"tutorial/#creating-metrics","text":"Let's add custom metrics to better understand our application and business behavior (e.g. number of reservations, etc.). Out of the box, AWS Lambda adds invocation, performance, and concurrency metrics . Amazon API Gateway also adds general metrics at the aggregate level such as latency, number of requests received, etc. Tip You can optionally enable detailed metrics per each API route, stage, and method in API Gateway. Let's expand our application with custom metrics using AWS SDK to see how it works, then let's upgrade it with Lambda Powertools :-) app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 import os import boto3 from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver from aws_lambda_powertools.logging import correlation_paths cold_start = True metric_namespace = \"MyApp\" logger = Logger ( service = \"APP\" ) tracer = Tracer ( service = \"APP\" ) metrics = boto3 . client ( \"cloudwatch\" ) app = ApiGatewayResolver () @tracer . capture_method def add_greeting_metric ( service : str = \"APP\" ): function_name = os . getenv ( \"AWS_LAMBDA_FUNCTION_NAME\" , \"undefined\" ) service_dimension = { \"Name\" : \"service\" , \"Value\" : service } function_dimension = { \"Name\" : \"function_name\" : \"Value\" : function_name } is_cold_start = True global cold_start if cold_start : cold_start = False else : is_cold_start = False return metrics . put_metric_data ( MetricData = [ { \"MetricName\" : \"SuccessfulGreetings\" , \"Dimensions\" : [ service_dimension ], \"Unit\" : \"Count\" , \"Value\" : 1 , }, { \"MetricName\" : \"ColdStart\" , \"Dimensions\" : [ service_dimension , function_dimension ], \"Unit\" : \"Count\" , \"Value\" : int ( is_cold_start ) } ], Namespace = metric_namespace , ) @app . get ( \"/hello/<name>\" ) @tracer . capture_method def hello_name ( name ): tracer . put_annotation ( key = \"User\" , value = name ) logger . info ( f \"Request from { name } received\" ) add_greeting_metric () return { \"message\" : f \"hello { name } !\" } @app . get ( \"/hello\" ) @tracer . capture_method def hello (): tracer . put_annotation ( key = \"User\" , value = \"unknown\" ) logger . info ( \"Request from unknown received\" ) add_greeting_metric () return { \"message\" : \"hello unknown!\" } @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST , log_event = True ) @tracer . capture_lambda_handler def lambda_handler ( event , context ): return app . resolve ( event , context ) template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 AWSTemplateFormatVersion : \"2010-09-09\" Transform : AWS::Serverless-2016-10-31 Description : Sample SAM Template for powertools-quickstart Globals : Function : Timeout : 3 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : CodeUri : hello_world/ Handler : app.lambda_handler Runtime : python3.9 Tracing : Active Events : HelloWorld : Type : Api Properties : Path : /hello Method : get HelloWorldName : Type : Api Properties : Path : /hello/{name} Method : get Policies : - CloudWatchPutMetricPolicy : {} Outputs : HelloWorldApi : Description : \"API Gateway endpoint URL for Prod stage for Hello World function\" Value : !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\" There's a lot going on, let's break this down: L10 : We define a container where all of our application metrics will live MyApp , a.k.a Metrics Namespace . L14 : We initialize a CloudWatch client to send metrics later. L19-47 : We create a custom function to prepare and send ColdStart and SuccessfulGreetings metrics using CloudWatch expected data structure. We also set dimensions of these metrics - Think of them as metadata to define to slice and dice them later; an unique metric is a combination of metric name + metric dimension(s). L55,64 : We call our custom function to create metrics for every greeting received. Question But what permissions do I need to send metrics to CloudWatch? Within template.yaml , we add CloudWatchPutMetricPolicy policy in SAM. Adding metrics via AWS SDK gives a lot of flexibility at a cost put_metric_data is a synchronous call to CloudWatch Metrics API. This means establishing a connection to CloudWatch endpoint, sending metrics payload, and waiting from a response. It will be visible in your AWS X-RAY traces as additional external call. Given your architecture scale, this approach might lead to disadvantages such as increased cost of measuring data collection and increased Lambda latency.","title":"Creating metrics"},{"location":"tutorial/#simplifying-with-metrics","text":"Lambda Powertools Metrics uses Amazon CloudWatch Embedded Metric Format (EMF) to create custom metrics asynchronously via a native integration with Lambda. In general terms, EMF is a specification that expects metrics in a JSON payload within CloudWatch Logs. Lambda ingests all logs emitted by a given function into CloudWatch Logs. CloudWatch automatically looks up for log entries that follow the EMF format and transforms them into a CloudWatch metric. Info If you are interested in the details of the EMF mechanism, follow blog post . Let's implement that using Metrics : Refactoring with Lambda Powertools Metrics 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 from aws_lambda_powertools import Logger , Tracer , Metrics from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.metrics import MetricUnit logger = Logger ( service = \"APP\" ) tracer = Tracer ( service = \"APP\" ) metrics = Metrics ( namespace = \"MyApp\" , service = \"APP\" ) app = ApiGatewayResolver () @app . get ( \"/hello/<name>\" ) @tracer . capture_method def hello_name ( name ): tracer . put_annotation ( key = \"User\" , value = name ) logger . info ( f \"Request from { name } received\" ) metrics . add_metric ( name = \"SuccessfulGreetings\" , unit = MetricUnit . Count , value = 1 ) return { \"message\" : f \"hello { name } !\" } @app . get ( \"/hello\" ) @tracer . capture_method def hello (): tracer . put_annotation ( key = \"User\" , value = \"unknown\" ) logger . info ( \"Request from unknown received\" ) metrics . add_metric ( name = \"SuccessfulGreetings\" , unit = MetricUnit . Count , value = 1 ) return { \"message\" : \"hello unknown!\" } @tracer . capture_lambda_handler @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST , log_event = True ) @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( event , context ): try : return app . resolve ( event , context ) except Exception as e : logger . exception ( e ) raise That's a lot less boilerplate code! Let's break this down: L9 : We initialize Metrics with our service name ( APP ) and metrics namespace ( MyApp ), reducing the need to add the service dimension for every metric and setting the namespace later L18, 27 : We use add_metric similarly to our custom function, except we now have an enum MetricCount to help us understand which Metric Units we have at our disposal L33 : We use @metrics.log_metrics decorator to ensure that our metrics are aligned with the EMF output and validated before-hand, like in case we forget to set namespace, or accidentally use a metric unit as a string that doesn't exist in CloudWatch. L33 : We also use capture_cold_start_metric=True so we don't have to handle that logic either. Note that Metrics does not publish a warm invocation metric (ColdStart=0) for cost reasons. As such, treat the absence (sparse metric) as a non-cold start invocation. Repeat the process of building, deploying, and invoking your application via the API endpoint a few times to generate metrics - Artillery and K6.io are quick ways to generate some load. Within CloudWatch Metrics view , you should see MyApp custom namespace with your custom metrics there and SuccessfulGreetings available to graph. If you're curious about how the EMF portion of your function logs look like, you can quickly go to CloudWatch ServiceLens view , choose your function and open logs. You will see a similar entry that looks like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"_aws\" : { \"Timestamp\" : 1638115724269 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"CustomMetrics\" , \"Dimensions\" : [ [ \"method\" , \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"AppMethodsInvocations\" , \"Unit\" : \"Count\" } ] } ] }, \"method\" : \"/hello/<name>\" , \"service\" : \"APP\" , \"AppMethodsInvocations\" : [ 1 ] }","title":"Simplifying with Metrics"},{"location":"tutorial/#final-considerations","text":"We covered a lot of ground here and we only scratched the surface of the feature set available within Lambda Powertools. When it comes to the observability features ( Tracer , Metrics , Logging ), don't stop there! The goal here is to ensure you can ask arbitrary questions to assess your system's health; these features are only part of the wider story! This requires a change in mindset to ensure operational excellence is part of the software development lifecycle. Tip You can find more details on other leading practices described in the Well-Architected Serverless Lens . Lambda Powertools is largely designed to make some of these practices easier to adopt from day 1. Have ideas for other tutorials? You can open up a documentation issue , or connect with us on the AWS Developers Slack at lambda-powertools channel, or via e-mail aws-lambda-powertools-feedback@amazon.com .","title":"Final considerations"},{"location":"tutorial/idempotency/","text":"The idempotency utility provides a simple solution to convert your Lambda functions into idempotent operations which are safe to retry. Terminology \u00b6 The property of idempotency means that an operation does not cause additional side effects if it is called more than once with the same input parameters. Idempotent operations will return the same result when they are called multiple times with the same parameters . This makes idempotent operations safe to retry. Idempotency key is a hash representation of either the entire event or a specific configured subset of the event, and invocation results are JSON serialized and stored in your persistence storage layer. Key features \u00b6 Prevent Lambda handler from executing more than once on the same event payload during a time window Ensure Lambda handler returns the same result when called with the same payload Select a subset of the event as the idempotency key using JMESPath expressions Set a time window in which records with the same payload should be considered duplicates Getting started \u00b6 Required resources \u00b6 Before getting started, you need to create a persistent storage layer where the idempotency utility can store its state - your lambda functions will need read and write access to it. As of now, Amazon DynamoDB is the only supported persistent storage layer, so you'll need to create a table first. Default table configuration If you're not changing the default configuration for the DynamoDB persistence layer , this is the expected default configuration: Configuration Value Notes Partition key id TTL attribute name expiration This can only be configured after your table is created if you're using AWS Console Tip: You can share a single state table for all functions You can reuse the same DynamoDB table to store idempotency state. We add your function_name in addition to the idempotency key as a hash key. AWS Serverless Application Model (SAM) example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Resources : IdempotencyTable : Type : AWS::DynamoDB::Table Properties : AttributeDefinitions : - AttributeName : id AttributeType : S KeySchema : - AttributeName : id KeyType : HASH TimeToLiveSpecification : AttributeName : expiration Enabled : true BillingMode : PAY_PER_REQUEST HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 ... Policies : - DynamoDBCrudPolicy : TableName : !Ref IdempotencyTable Warning: Large responses with DynamoDB persistence layer When using this utility with DynamoDB, your function's responses must be smaller than 400KB . Larger items cannot be written to DynamoDB and will cause exceptions. Info: DynamoDB Each function invocation will generally make 2 requests to DynamoDB. If the result returned by your Lambda is less than 1kb, you can expect 2 WCUs per invocation. For retried invocations, you will see 1WCU and 1RCU. Review the DynamoDB pricing documentation to estimate the cost. Idempotent decorator \u00b6 You can quickly start by initializing the DynamoDBPersistenceLayer class and using it with the idempotent decorator on your lambda handler. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): payment = create_subscription_payment ( user = event [ 'user' ], product = event [ 'product_id' ] ) ... return { \"payment_id\" : payment . id , \"message\" : \"success\" , \"statusCode\" : 200 , } Example event 1 2 3 4 { \"username\" : \"xyz\" , \"product_id\" : \"123456789\" } Idempotent_function decorator \u00b6 Similar to idempotent decorator , you can use idempotent_function decorator for any synchronous Python function. When using idempotent_function , you must tell us which keyword parameter in your function signature has the data we should use via data_keyword_argument . We support JSON serializable data, Python Dataclasses , Parser/Pydantic Models , and our Event Source Data Classes . Warning Make sure to call your decorated function using keyword arguments batch_sample.py This example also demonstrates how you can integrate with Batch utility , so you can process each record in an idempotent manner. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from aws_lambda_powertools.utilities.batch import ( BatchProcessor , EventType , batch_processor ) from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , IdempotencyConfig , idempotent_function ) processor = BatchProcessor ( event_type = EventType . SQS ) dynamodb = DynamoDBPersistenceLayer ( table_name = \"idem\" ) config = IdempotencyConfig ( event_key_jmespath = \"messageId\" , # see Choosing a payload subset section use_local_cache = True , ) @idempotent_function ( data_keyword_argument = \"record\" , config = config , persistence_store = dynamodb ) def record_handler ( record : SQSRecord ): return { \"message\" : record [ \"body\" ]} @idempotent_function ( data_keyword_argument = \"data\" , config = config , persistence_store = dynamodb ) def dummy ( arg_one , arg_two , data : dict , ** kwargs ): return { \"data\" : data } @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context ): # `data` parameter must be called as a keyword argument to work dummy ( \"hello\" , \"universe\" , data = \"test\" ) return processor . response () Batch event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \"Records\" : [ { \"messageId\" : \"059f36b4-87a3-44ab-83d2-661975830a7d\" , \"receiptHandle\" : \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...\" , \"body\" : \"Test message.\" , \"attributes\" : { \"ApproximateReceiveCount\" : \"1\" , \"SentTimestamp\" : \"1545082649183\" , \"SenderId\" : \"AIDAIENQZJOLO23YVJ4VO\" , \"ApproximateFirstReceiveTimestamp\" : \"1545082649185\" }, \"messageAttributes\" : { \"testAttr\" : { \"stringValue\" : \"100\" , \"binaryValue\" : \"base64Str\" , \"dataType\" : \"Number\" } }, \"md5OfBody\" : \"e4e68fb7bd0e697a0ae8f1bb342846b3\" , \"eventSource\" : \"aws:sqs\" , \"eventSourceARN\" : \"arn:aws:sqs:us-east-2:123456789012:my-queue\" , \"awsRegion\" : \"us-east-2\" } ] } dataclass_sample.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from dataclasses import dataclass from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , IdempotencyConfig , idempotent_function ) dynamodb = DynamoDBPersistenceLayer ( table_name = \"idem\" ) config = IdempotencyConfig ( event_key_jmespath = \"order_id\" , # see Choosing a payload subset section use_local_cache = True , ) @dataclass class OrderItem : sku : str description : str @dataclass class Order : item : OrderItem order_id : int @idempotent_function ( data_keyword_argument = \"order\" , config = config , persistence_store = dynamodb ) def process_order ( order : Order ): return f \"processed order { order . order_id } \" order_item = OrderItem ( sku = \"fake\" , description = \"sample\" ) order = Order ( item = order_item , order_id = \"fake-id\" ) # `order` parameter must be called as a keyword argument to work process_order ( order = order ) parser_pydantic_sample.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , IdempotencyConfig , idempotent_function ) from aws_lambda_powertools.utilities.parser import BaseModel dynamodb = DynamoDBPersistenceLayer ( table_name = \"idem\" ) config = IdempotencyConfig ( event_key_jmespath = \"order_id\" , # see Choosing a payload subset section use_local_cache = True , ) class OrderItem ( BaseModel ): sku : str description : str class Order ( BaseModel ): item : OrderItem order_id : int @idempotent_function ( data_keyword_argument = \"order\" , config = config , persistence_store = dynamodb ) def process_order ( order : Order ): return f \"processed order { order . order_id } \" order_item = OrderItem ( sku = \"fake\" , description = \"sample\" ) order = Order ( item = order_item , order_id = \"fake-id\" ) # `order` parameter must be called as a keyword argument to work process_order ( order = order ) Choosing a payload subset for idempotency \u00b6 Tip: Dealing with always changing payloads When dealing with a more elaborate payload, where parts of the payload always change, you should use event_key_jmespath parameter. Use IdempotencyConfig to instruct the idempotent decorator to only use a portion of your payload to verify whether a request is idempotent, and therefore it should not be retried. Payment scenario In this example, we have a Lambda handler that creates a payment for a user subscribing to a product. We want to ensure that we don't accidentally charge our customer by subscribing them more than once. Imagine the function executes successfully, but the client never receives the response due to a connection issue. It is safe to retry in this instance, as the idempotent decorator will return a previously saved response. Warning: Idempotency for JSON payloads The payload extracted by the event_key_jmespath is treated as a string by default, so will be sensitive to differences in whitespace even when the JSON payload itself is identical. To alter this behaviour, we can use the JMESPath built-in function powertools_json() to treat the payload as a JSON object (dict) rather than a string. payment.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import json from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) # Treat everything under the \"body\" key # in the event json object as our payload config = IdempotencyConfig ( event_key_jmespath = \"powertools_json(body)\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): body = json . loads ( event [ 'body' ]) payment = create_subscription_payment ( user = body [ 'user' ], product = body [ 'product_id' ] ) ... return { \"payment_id\" : payment . id , \"message\" : \"success\" , \"statusCode\" : 200 } Example event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { \"version\" : \"2.0\" , \"routeKey\" : \"ANY /createpayment\" , \"rawPath\" : \"/createpayment\" , \"rawQueryString\" : \"\" , \"headers\" : { \"Header1\" : \"value1\" , \"Header2\" : \"value2\" }, \"requestContext\" :{ \"accountId\" : \"123456789012\" , \"apiId\" : \"api-id\" , \"domainName\" : \"id.execute-api.us-east-1.amazonaws.com\" , \"domainPrefix\" : \"id\" , \"http\" :{ \"method\" : \"POST\" , \"path\" : \"/createpayment\" , \"protocol\" : \"HTTP/1.1\" , \"sourceIp\" : \"ip\" , \"userAgent\" : \"agent\" }, \"requestId\" : \"id\" , \"routeKey\" : \"ANY /createpayment\" , \"stage\" : \"$default\" , \"time\" : \"10/Feb/2021:13:40:43 +0000\" , \"timeEpoch\" : 1612964443723 }, \"body\" : \"{\\\"user\\\":\\\"xyz\\\",\\\"product_id\\\":\\\"123456789\\\"}\" , \"isBase64Encoded\" : false } Idempotency request flow \u00b6 This sequence diagram shows an example flow of what happens in the payment scenario: The client was successful in receiving the result after the retry. Since the Lambda handler was only executed once, our customer hasn't been charged twice. Note Bear in mind that the entire Lambda handler is treated as a single idempotent operation. If your Lambda handler can cause multiple side effects, consider splitting it into separate functions. Handling exceptions \u00b6 If you are using the idempotent decorator on your Lambda handler, any unhandled exceptions that are raised during the code execution will cause the record in the persistence layer to be deleted . This means that new invocations will execute your code again despite having the same payload. If you don't want the record to be deleted, you need to catch exceptions within the idempotent function and return a successful response. If you are using idempotent_function , any unhandled exceptions that are raised inside the decorated function will cause the record in the persistence layer to be deleted, and allow the function to be executed again if retried. If an Exception is raised outside the scope of the decorated function and after your function has been called, the persistent record will not be affected. In this case, idempotency will be maintained for your decorated function. Example: Exception not affecting idempotency record sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def lambda_handler ( event , context ): # If an exception is raised here, no idempotent record will ever get created as the # idempotent function does not get called do_some_stuff () result = call_external_service ( data = { \"user\" : \"user1\" , \"id\" : 5 }) # This exception will not cause the idempotent record to be deleted, since it # happens after the decorated function has been successfully called raise Exception @idempotent_function ( data_keyword_argument = \"data\" , config = config , persistence_store = dynamodb ) def call_external_service ( data : dict , ** kwargs ): result = requests . post ( 'http://example.com' , json = { \"user\" : data [ 'user' ], \"transaction_id\" : data [ 'id' ]} return result . json () Warning We will raise IdempotencyPersistenceLayerError if any of the calls to the persistence layer fail unexpectedly. As this happens outside the scope of your decorated function, you are not able to catch it if you're using the idempotent decorator on your Lambda handler. Persistence layers \u00b6 DynamoDBPersistenceLayer \u00b6 This persistence layer is built-in, and you can either use an existing DynamoDB table or create a new one dedicated for idempotency state (recommended). Customizing DynamoDBPersistenceLayer to suit your table structure 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.idempotency import DynamoDBPersistenceLayer persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , key_attr = \"idempotency_key\" , expiry_attr = \"expires_at\" , status_attr = \"current_status\" , data_attr = \"result_data\" , validation_key_attr = \"validation_key\" , ) When using DynamoDB as a persistence layer, you can alter the attribute names by passing these parameters when initializing the persistence layer: Parameter Required Default Description table_name Table name to store state key_attr id Partition key of the table. Hashed representation of the payload (unless sort_key_attr is specified) expiry_attr expiration Unix timestamp of when record expires status_attr status Stores status of the lambda execution during and after invocation data_attr data Stores results of successfully executed Lambda handlers validation_key_attr validation Hashed representation of the parts of the event used for validation sort_key_attr Sort key of the table (if table is configured with a sort key). static_pk_value idempotency#{LAMBDA_FUNCTION_NAME} Static value to use as the partition key. Only used when sort_key_attr is set. Advanced \u00b6 Customizing the default behavior \u00b6 Idempotent decorator can be further configured with IdempotencyConfig as seen in the previous example. These are the available options for further configuration Parameter Default Description event_key_jmespath \"\" JMESPath expression to extract the idempotency key from the event record using built-in functions payload_validation_jmespath \"\" JMESPath expression to validate whether certain parameters have changed in the event while the event payload raise_on_no_idempotency_key False Raise exception if no idempotency key was found in the request expires_after_seconds 3600 The number of seconds to wait before a record is expired use_local_cache False Whether to locally cache idempotency results local_cache_max_items 256 Max number of items to store in local cache hash_function md5 Function to use for calculating hashes, as provided by hashlib in the standard library. Handling concurrent executions with the same payload \u00b6 This utility will raise an IdempotencyAlreadyInProgressError exception if you receive multiple invocations with the same payload while the first invocation hasn't completed yet . Info If you receive IdempotencyAlreadyInProgressError , you can safely retry the operation. This is a locking mechanism for correctness. Since we don't know the result from the first invocation yet, we can't safely allow another concurrent execution. Using in-memory cache \u00b6 By default, in-memory local caching is disabled , since we don't know how much memory you consume per invocation compared to the maximum configured in your Lambda function. Note: This in-memory cache is local to each Lambda execution environment This means it will be effective in cases where your function's concurrency is low in comparison to the number of \"retry\" invocations with the same payload, because cache might be empty. You can enable in-memory caching with the use_local_cache parameter: Caching idempotent transactions in-memory to prevent multiple calls to storage 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) config = IdempotencyConfig ( event_key_jmespath = \"body\" , use_local_cache = True , ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... When enabled, the default is to cache a maximum of 256 records in each Lambda execution environment - You can change it with the local_cache_max_items parameter. Expiring idempotency records \u00b6 Note By default, we expire idempotency records after an hour (3600 seconds). In most cases, it is not desirable to store the idempotency records forever. Rather, you want to guarantee that the same payload won't be executed within a period of time. You can change this window with the expires_after_seconds parameter: Adjusting cache TTL 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) config = IdempotencyConfig ( event_key_jmespath = \"body\" , expires_after_seconds = 5 * 60 , # 5 minutes ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... This will mark any records older than 5 minutes as expired, and the lambda handler will be executed as normal if it is invoked with a matching payload. Note: DynamoDB time-to-live field This utility uses expiration as the TTL field in DynamoDB, as demonstrated in the SAM example earlier . Payload validation \u00b6 Question: What if your function is invoked with the same payload except some outer parameters have changed? Example: A payment transaction for a given productID was requested twice for the same customer, however the amount to be paid has changed in the second transaction . By default, we will return the same result as it returned before, however in this instance it may be misleading; we provide a fail fast payload validation to address this edge case. With payload_validation_jmespath , you can provide an additional JMESPath expression to specify which part of the event body should be validated against previous idempotent invocations app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"[userDetail, productId]\" , payload_validation_jmespath = \"amount\" ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): # Creating a subscription payment is a side # effect of calling this function! payment = create_subscription_payment ( user = event [ 'userDetail' ][ 'username' ], product = event [ 'product_id' ], amount = event [ 'amount' ] ) ... return { \"message\" : \"success\" , \"statusCode\" : 200 , \"payment_id\" : payment . id , \"amount\" : payment . amount } Example Event 1 1 2 3 4 5 6 7 8 9 { \"userDetail\" : { \"username\" : \"User1\" , \"user_email\" : \"user@example.com\" }, \"productId\" : 1500 , \"charge_type\" : \"subscription\" , \"amount\" : 500 } Example Event 2 1 2 3 4 5 6 7 8 9 { \"userDetail\" : { \"username\" : \"User1\" , \"user_email\" : \"user@example.com\" }, \"productId\" : 1500 , \"charge_type\" : \"subscription\" , \"amount\" : 1 } In this example, the userDetail and productId keys are used as the payload to generate the idempotency key, as per event_key_jmespath parameter. Note If we try to send the same request but with a different amount, we will raise IdempotencyValidationError . Without payload validation, we would have returned the same result as we did for the initial request. Since we're also returning an amount in the response, this could be quite confusing for the client. By using payload_validation_jmespath=\"amount\" , we prevent this potentially confusing behavior and instead raise an Exception. Making idempotency key required \u00b6 If you want to enforce that an idempotency key is required, you can set raise_on_no_idempotency_key to True . This means that we will raise IdempotencyKeyError if the evaluation of event_key_jmespath is None . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) # Requires \"user\".\"uid\" and \"order_id\" to be present config = IdempotencyConfig ( event_key_jmespath = \"[user.uid, order_id]\" , raise_on_no_idempotency_key = True , ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): pass Success Event 1 2 3 4 5 6 7 { \"user\" : { \"uid\" : \"BB0D045C-8878-40C8-889E-38B3CB0A61B1\" , \"name\" : \"Foo\" }, \"order_id\" : 10000 } Failure Event Notice that order_id is now accidentally within user key 1 2 3 4 5 6 7 { \"user\" : { \"uid\" : \"DE0D000E-1234-10D1-991E-EAC1DD1D52C8\" , \"name\" : \"Joe Bloggs\" , \"order_id\" : 10000 }, } Customizing boto configuration \u00b6 The boto_config and boto3_session parameters enable you to pass in a custom botocore config object or a custom boto3 session when constructing the persistence store. Custom session 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import boto3 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) boto3_session = boto3 . session . Session () persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , boto3_session = boto3_session ) config = IdempotencyConfig ( event_key_jmespath = \"body\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... Custom config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from botocore.config import Config from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"body\" ) boto_config = Config () persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , boto_config = boto_config ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... Using a DynamoDB table with a composite primary key \u00b6 When using a composite primary key table (hash+range key), use sort_key_attr parameter when initializing your persistence layer. With this setting, we will save the idempotency key in the sort key instead of the primary key. By default, the primary key will now be set to idempotency#{LAMBDA_FUNCTION_NAME} . You can optionally set a static value for the partition key using the static_pk_value parameter. Reusing a DynamoDB table that uses a composite primary key 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.idempotency import DynamoDBPersistenceLayer , idempotent persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , sort_key_attr = 'sort_key' ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): return { \"message\" : \"success\" : \"id\" : event [ 'body' ][ 'id]} The example function above would cause data to be stored in DynamoDB like this: id sort_key expiration status data idempotency#MyLambdaFunction 1e956ef7da78d0cb890be999aecc0c9e 1636549553 COMPLETED {\"id\": 12391, \"message\": \"success\"} idempotency#MyLambdaFunction 2b2cdb5f86361e97b4383087c1ffdf27 1636549571 COMPLETED {\"id\": 527212, \"message\": \"success\"} idempotency#MyLambdaFunction f091d2527ad1c78f05d54cc3f363be80 1636549585 IN_PROGRESS Bring your own persistent store \u00b6 This utility provides an abstract base class (ABC), so that you can implement your choice of persistent storage layer. You can inherit from the BasePersistenceLayer class and implement the abstract methods _get_record , _put_record , _update_record and _delete_record . Excerpt DynamoDB Persisntence Layer implementation for reference 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 import datetime import logging from typing import Any , Dict , Optional import boto3 from botocore.config import Config from aws_lambda_powertools.utilities.idempotency import BasePersistenceLayer from aws_lambda_powertools.utilities.idempotency.exceptions import ( IdempotencyItemAlreadyExistsError , IdempotencyItemNotFoundError , ) from aws_lambda_powertools.utilities.idempotency.persistence.base import DataRecord logger = logging . getLogger ( __name__ ) class DynamoDBPersistenceLayer ( BasePersistenceLayer ): def __init__ ( self , table_name : str , key_attr : str = \"id\" , expiry_attr : str = \"expiration\" , status_attr : str = \"status\" , data_attr : str = \"data\" , validation_key_attr : str = \"validation\" , boto_config : Optional [ Config ] = None , boto3_session : Optional [ boto3 . session . Session ] = None , ): boto_config = boto_config or Config () session = boto3_session or boto3 . session . Session () self . _ddb_resource = session . resource ( \"dynamodb\" , config = boto_config ) self . table_name = table_name self . table = self . _ddb_resource . Table ( self . table_name ) self . key_attr = key_attr self . expiry_attr = expiry_attr self . status_attr = status_attr self . data_attr = data_attr self . validation_key_attr = validation_key_attr super ( DynamoDBPersistenceLayer , self ) . __init__ () def _item_to_data_record ( self , item : Dict [ str , Any ]) -> DataRecord : \"\"\" Translate raw item records from DynamoDB to DataRecord Parameters ---------- item: Dict[str, Union[str, int]] Item format from dynamodb response Returns ------- DataRecord representation of item \"\"\" return DataRecord ( idempotency_key = item [ self . key_attr ], status = item [ self . status_attr ], expiry_timestamp = item [ self . expiry_attr ], response_data = item . get ( self . data_attr ), payload_hash = item . get ( self . validation_key_attr ), ) def _get_record ( self , idempotency_key ) -> DataRecord : response = self . table . get_item ( Key = { self . key_attr : idempotency_key }, ConsistentRead = True ) try : item = response [ \"Item\" ] except KeyError : raise IdempotencyItemNotFoundError return self . _item_to_data_record ( item ) def _put_record ( self , data_record : DataRecord ) -> None : item = { self . key_attr : data_record . idempotency_key , self . expiry_attr : data_record . expiry_timestamp , self . status_attr : data_record . status , } if self . payload_validation_enabled : item [ self . validation_key_attr ] = data_record . payload_hash now = datetime . datetime . now () try : logger . debug ( f \"Putting record for idempotency key: { data_record . idempotency_key } \" ) self . table . put_item ( Item = item , ConditionExpression = f \"attribute_not_exists( { self . key_attr } ) OR { self . expiry_attr } < :now\" , ExpressionAttributeValues = { \":now\" : int ( now . timestamp ())}, ) except self . _ddb_resource . meta . client . exceptions . ConditionalCheckFailedException : logger . debug ( f \"Failed to put record for already existing idempotency key: { data_record . idempotency_key } \" ) raise IdempotencyItemAlreadyExistsError def _update_record ( self , data_record : DataRecord ): logger . debug ( f \"Updating record for idempotency key: { data_record . idempotency_key } \" ) update_expression = \"SET #response_data = :response_data, #expiry = :expiry, #status = :status\" expression_attr_values = { \":expiry\" : data_record . expiry_timestamp , \":response_data\" : data_record . response_data , \":status\" : data_record . status , } expression_attr_names = { \"#response_data\" : self . data_attr , \"#expiry\" : self . expiry_attr , \"#status\" : self . status_attr , } if self . payload_validation_enabled : update_expression += \", #validation_key = :validation_key\" expression_attr_values [ \":validation_key\" ] = data_record . payload_hash expression_attr_names [ \"#validation_key\" ] = self . validation_key_attr kwargs = { \"Key\" : { self . key_attr : data_record . idempotency_key }, \"UpdateExpression\" : update_expression , \"ExpressionAttributeValues\" : expression_attr_values , \"ExpressionAttributeNames\" : expression_attr_names , } self . table . update_item ( ** kwargs ) def _delete_record ( self , data_record : DataRecord ) -> None : logger . debug ( f \"Deleting record for idempotency key: { data_record . idempotency_key } \" ) self . table . delete_item ( Key = { self . key_attr : data_record . idempotency_key },) Danger Pay attention to the documentation for each - you may need to perform additional checks inside these methods to ensure the idempotency guarantees remain intact. For example, the _put_record method needs to raise an exception if a non-expired record already exists in the data store with a matching key. Compatibility with other utilities \u00b6 Validation utility \u00b6 The idempotency utility can be used with the validator decorator. Ensure that idempotency is the innermost decorator. Warning If you use an envelope with the validator, the event received by the idempotency utility will be the unwrapped event - not the \"raw\" event Lambda was invoked with. Make sure to account for this behaviour, if you set the event_key_jmespath . Using Idempotency with JSONSchema Validation utility 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.validation import validator , envelopes from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"[message, username]\" ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @validator ( envelope = envelopes . API_GATEWAY_HTTP ) @idempotent ( config = config , persistence_store = persistence_layer ) def lambda_handler ( event , context ): cause_some_side_effects ( event [ 'username' ) return { \"message\" : event [ 'message' ], \"statusCode\" : 200 } Tip: JMESPath Powertools functions are also available Built-in functions known in the validation utility like powertools_json , powertools_base64 , powertools_base64_gzip are also available to use in this utility. Testing your code \u00b6 The idempotency utility provides several routes to test your code. Disabling the idempotency utility \u00b6 When testing your code, you may wish to disable the idempotency logic altogether and focus on testing your business logic. To do this, you can set the environment variable POWERTOOLS_IDEMPOTENCY_DISABLED with a truthy value. If you prefer setting this for specific tests, and are using Pytest, you can use monkeypatch fixture: tests.py 1 2 3 4 5 6 def test_idempotent_lambda_handler ( monkeypatch ): # Set POWERTOOLS_IDEMPOTENCY_DISABLED before calling decorated functions monkeypatch . setenv ( \"POWERTOOLS_IDEMPOTENCY_DISABLED\" , 1 ) result = handler () ... app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"idempotency\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): print ( 'expensive operation' ) return { \"payment_id\" : 12345 , \"message\" : \"success\" , \"statusCode\" : 200 , } Testing with DynamoDB Local \u00b6 To test with DynamoDB Local , you can replace the Table resource used by the persistence layer with one you create inside your tests. This allows you to set the endpoint_url. tests.py 1 2 3 4 5 6 7 8 9 10 11 12 import boto3 import app def test_idempotent_lambda (): # Create our own Table resource using the endpoint for our DynamoDB Local instance resource = boto3 . resource ( \"dynamodb\" , endpoint_url = 'http://localhost:8000' ) table = resource . Table ( app . persistence_layer . table_name ) app . persistence_layer . table = table result = app . handler ({ 'testkey' : 'testvalue' }, {}) assert result [ 'payment_id' ] == 12345 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"idempotency\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): print ( 'expensive operation' ) return { \"payment_id\" : 12345 , \"message\" : \"success\" , \"statusCode\" : 200 , } How do I mock all DynamoDB I/O operations \u00b6 The idempotency utility lazily creates the dynamodb Table which it uses to access DynamoDB. This means it is possible to pass a mocked Table resource, or stub various methods. tests.py 1 2 3 4 5 6 7 8 9 10 from unittest.mock import MagicMock import app def test_idempotent_lambda (): table = MagicMock () app . persistence_layer . table = table result = app . handler ({ 'testkey' : 'testvalue' }, {}) table . put_item . assert_called () ... app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"idempotency\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): print ( 'expensive operation' ) return { \"payment_id\" : 12345 , \"message\" : \"success\" , \"statusCode\" : 200 , } Extra resources \u00b6 If you're interested in a deep dive on how Amazon uses idempotency when building our APIs, check out this article .","title":"Idempotency"},{"location":"tutorial/idempotency/#terminology","text":"The property of idempotency means that an operation does not cause additional side effects if it is called more than once with the same input parameters. Idempotent operations will return the same result when they are called multiple times with the same parameters . This makes idempotent operations safe to retry. Idempotency key is a hash representation of either the entire event or a specific configured subset of the event, and invocation results are JSON serialized and stored in your persistence storage layer.","title":"Terminology"},{"location":"tutorial/idempotency/#key-features","text":"Prevent Lambda handler from executing more than once on the same event payload during a time window Ensure Lambda handler returns the same result when called with the same payload Select a subset of the event as the idempotency key using JMESPath expressions Set a time window in which records with the same payload should be considered duplicates","title":"Key features"},{"location":"tutorial/idempotency/#getting-started","text":"","title":"Getting started"},{"location":"tutorial/idempotency/#required-resources","text":"Before getting started, you need to create a persistent storage layer where the idempotency utility can store its state - your lambda functions will need read and write access to it. As of now, Amazon DynamoDB is the only supported persistent storage layer, so you'll need to create a table first. Default table configuration If you're not changing the default configuration for the DynamoDB persistence layer , this is the expected default configuration: Configuration Value Notes Partition key id TTL attribute name expiration This can only be configured after your table is created if you're using AWS Console Tip: You can share a single state table for all functions You can reuse the same DynamoDB table to store idempotency state. We add your function_name in addition to the idempotency key as a hash key. AWS Serverless Application Model (SAM) example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Resources : IdempotencyTable : Type : AWS::DynamoDB::Table Properties : AttributeDefinitions : - AttributeName : id AttributeType : S KeySchema : - AttributeName : id KeyType : HASH TimeToLiveSpecification : AttributeName : expiration Enabled : true BillingMode : PAY_PER_REQUEST HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 ... Policies : - DynamoDBCrudPolicy : TableName : !Ref IdempotencyTable Warning: Large responses with DynamoDB persistence layer When using this utility with DynamoDB, your function's responses must be smaller than 400KB . Larger items cannot be written to DynamoDB and will cause exceptions. Info: DynamoDB Each function invocation will generally make 2 requests to DynamoDB. If the result returned by your Lambda is less than 1kb, you can expect 2 WCUs per invocation. For retried invocations, you will see 1WCU and 1RCU. Review the DynamoDB pricing documentation to estimate the cost.","title":"Required resources"},{"location":"tutorial/idempotency/#idempotent-decorator","text":"You can quickly start by initializing the DynamoDBPersistenceLayer class and using it with the idempotent decorator on your lambda handler. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): payment = create_subscription_payment ( user = event [ 'user' ], product = event [ 'product_id' ] ) ... return { \"payment_id\" : payment . id , \"message\" : \"success\" , \"statusCode\" : 200 , } Example event 1 2 3 4 { \"username\" : \"xyz\" , \"product_id\" : \"123456789\" }","title":"Idempotent decorator"},{"location":"tutorial/idempotency/#idempotent_function-decorator","text":"Similar to idempotent decorator , you can use idempotent_function decorator for any synchronous Python function. When using idempotent_function , you must tell us which keyword parameter in your function signature has the data we should use via data_keyword_argument . We support JSON serializable data, Python Dataclasses , Parser/Pydantic Models , and our Event Source Data Classes . Warning Make sure to call your decorated function using keyword arguments batch_sample.py This example also demonstrates how you can integrate with Batch utility , so you can process each record in an idempotent manner. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from aws_lambda_powertools.utilities.batch import ( BatchProcessor , EventType , batch_processor ) from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , IdempotencyConfig , idempotent_function ) processor = BatchProcessor ( event_type = EventType . SQS ) dynamodb = DynamoDBPersistenceLayer ( table_name = \"idem\" ) config = IdempotencyConfig ( event_key_jmespath = \"messageId\" , # see Choosing a payload subset section use_local_cache = True , ) @idempotent_function ( data_keyword_argument = \"record\" , config = config , persistence_store = dynamodb ) def record_handler ( record : SQSRecord ): return { \"message\" : record [ \"body\" ]} @idempotent_function ( data_keyword_argument = \"data\" , config = config , persistence_store = dynamodb ) def dummy ( arg_one , arg_two , data : dict , ** kwargs ): return { \"data\" : data } @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context ): # `data` parameter must be called as a keyword argument to work dummy ( \"hello\" , \"universe\" , data = \"test\" ) return processor . response () Batch event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \"Records\" : [ { \"messageId\" : \"059f36b4-87a3-44ab-83d2-661975830a7d\" , \"receiptHandle\" : \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...\" , \"body\" : \"Test message.\" , \"attributes\" : { \"ApproximateReceiveCount\" : \"1\" , \"SentTimestamp\" : \"1545082649183\" , \"SenderId\" : \"AIDAIENQZJOLO23YVJ4VO\" , \"ApproximateFirstReceiveTimestamp\" : \"1545082649185\" }, \"messageAttributes\" : { \"testAttr\" : { \"stringValue\" : \"100\" , \"binaryValue\" : \"base64Str\" , \"dataType\" : \"Number\" } }, \"md5OfBody\" : \"e4e68fb7bd0e697a0ae8f1bb342846b3\" , \"eventSource\" : \"aws:sqs\" , \"eventSourceARN\" : \"arn:aws:sqs:us-east-2:123456789012:my-queue\" , \"awsRegion\" : \"us-east-2\" } ] } dataclass_sample.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from dataclasses import dataclass from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , IdempotencyConfig , idempotent_function ) dynamodb = DynamoDBPersistenceLayer ( table_name = \"idem\" ) config = IdempotencyConfig ( event_key_jmespath = \"order_id\" , # see Choosing a payload subset section use_local_cache = True , ) @dataclass class OrderItem : sku : str description : str @dataclass class Order : item : OrderItem order_id : int @idempotent_function ( data_keyword_argument = \"order\" , config = config , persistence_store = dynamodb ) def process_order ( order : Order ): return f \"processed order { order . order_id } \" order_item = OrderItem ( sku = \"fake\" , description = \"sample\" ) order = Order ( item = order_item , order_id = \"fake-id\" ) # `order` parameter must be called as a keyword argument to work process_order ( order = order ) parser_pydantic_sample.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , IdempotencyConfig , idempotent_function ) from aws_lambda_powertools.utilities.parser import BaseModel dynamodb = DynamoDBPersistenceLayer ( table_name = \"idem\" ) config = IdempotencyConfig ( event_key_jmespath = \"order_id\" , # see Choosing a payload subset section use_local_cache = True , ) class OrderItem ( BaseModel ): sku : str description : str class Order ( BaseModel ): item : OrderItem order_id : int @idempotent_function ( data_keyword_argument = \"order\" , config = config , persistence_store = dynamodb ) def process_order ( order : Order ): return f \"processed order { order . order_id } \" order_item = OrderItem ( sku = \"fake\" , description = \"sample\" ) order = Order ( item = order_item , order_id = \"fake-id\" ) # `order` parameter must be called as a keyword argument to work process_order ( order = order )","title":"Idempotent_function decorator"},{"location":"tutorial/idempotency/#choosing-a-payload-subset-for-idempotency","text":"Tip: Dealing with always changing payloads When dealing with a more elaborate payload, where parts of the payload always change, you should use event_key_jmespath parameter. Use IdempotencyConfig to instruct the idempotent decorator to only use a portion of your payload to verify whether a request is idempotent, and therefore it should not be retried. Payment scenario In this example, we have a Lambda handler that creates a payment for a user subscribing to a product. We want to ensure that we don't accidentally charge our customer by subscribing them more than once. Imagine the function executes successfully, but the client never receives the response due to a connection issue. It is safe to retry in this instance, as the idempotent decorator will return a previously saved response. Warning: Idempotency for JSON payloads The payload extracted by the event_key_jmespath is treated as a string by default, so will be sensitive to differences in whitespace even when the JSON payload itself is identical. To alter this behaviour, we can use the JMESPath built-in function powertools_json() to treat the payload as a JSON object (dict) rather than a string. payment.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import json from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) # Treat everything under the \"body\" key # in the event json object as our payload config = IdempotencyConfig ( event_key_jmespath = \"powertools_json(body)\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): body = json . loads ( event [ 'body' ]) payment = create_subscription_payment ( user = body [ 'user' ], product = body [ 'product_id' ] ) ... return { \"payment_id\" : payment . id , \"message\" : \"success\" , \"statusCode\" : 200 } Example event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { \"version\" : \"2.0\" , \"routeKey\" : \"ANY /createpayment\" , \"rawPath\" : \"/createpayment\" , \"rawQueryString\" : \"\" , \"headers\" : { \"Header1\" : \"value1\" , \"Header2\" : \"value2\" }, \"requestContext\" :{ \"accountId\" : \"123456789012\" , \"apiId\" : \"api-id\" , \"domainName\" : \"id.execute-api.us-east-1.amazonaws.com\" , \"domainPrefix\" : \"id\" , \"http\" :{ \"method\" : \"POST\" , \"path\" : \"/createpayment\" , \"protocol\" : \"HTTP/1.1\" , \"sourceIp\" : \"ip\" , \"userAgent\" : \"agent\" }, \"requestId\" : \"id\" , \"routeKey\" : \"ANY /createpayment\" , \"stage\" : \"$default\" , \"time\" : \"10/Feb/2021:13:40:43 +0000\" , \"timeEpoch\" : 1612964443723 }, \"body\" : \"{\\\"user\\\":\\\"xyz\\\",\\\"product_id\\\":\\\"123456789\\\"}\" , \"isBase64Encoded\" : false }","title":"Choosing a payload subset for idempotency"},{"location":"tutorial/idempotency/#idempotency-request-flow","text":"This sequence diagram shows an example flow of what happens in the payment scenario: The client was successful in receiving the result after the retry. Since the Lambda handler was only executed once, our customer hasn't been charged twice. Note Bear in mind that the entire Lambda handler is treated as a single idempotent operation. If your Lambda handler can cause multiple side effects, consider splitting it into separate functions.","title":"Idempotency request flow"},{"location":"tutorial/idempotency/#handling-exceptions","text":"If you are using the idempotent decorator on your Lambda handler, any unhandled exceptions that are raised during the code execution will cause the record in the persistence layer to be deleted . This means that new invocations will execute your code again despite having the same payload. If you don't want the record to be deleted, you need to catch exceptions within the idempotent function and return a successful response. If you are using idempotent_function , any unhandled exceptions that are raised inside the decorated function will cause the record in the persistence layer to be deleted, and allow the function to be executed again if retried. If an Exception is raised outside the scope of the decorated function and after your function has been called, the persistent record will not be affected. In this case, idempotency will be maintained for your decorated function. Example: Exception not affecting idempotency record sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def lambda_handler ( event , context ): # If an exception is raised here, no idempotent record will ever get created as the # idempotent function does not get called do_some_stuff () result = call_external_service ( data = { \"user\" : \"user1\" , \"id\" : 5 }) # This exception will not cause the idempotent record to be deleted, since it # happens after the decorated function has been successfully called raise Exception @idempotent_function ( data_keyword_argument = \"data\" , config = config , persistence_store = dynamodb ) def call_external_service ( data : dict , ** kwargs ): result = requests . post ( 'http://example.com' , json = { \"user\" : data [ 'user' ], \"transaction_id\" : data [ 'id' ]} return result . json () Warning We will raise IdempotencyPersistenceLayerError if any of the calls to the persistence layer fail unexpectedly. As this happens outside the scope of your decorated function, you are not able to catch it if you're using the idempotent decorator on your Lambda handler.","title":"Handling exceptions"},{"location":"tutorial/idempotency/#persistence-layers","text":"","title":"Persistence layers"},{"location":"tutorial/idempotency/#dynamodbpersistencelayer","text":"This persistence layer is built-in, and you can either use an existing DynamoDB table or create a new one dedicated for idempotency state (recommended). Customizing DynamoDBPersistenceLayer to suit your table structure 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.idempotency import DynamoDBPersistenceLayer persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , key_attr = \"idempotency_key\" , expiry_attr = \"expires_at\" , status_attr = \"current_status\" , data_attr = \"result_data\" , validation_key_attr = \"validation_key\" , ) When using DynamoDB as a persistence layer, you can alter the attribute names by passing these parameters when initializing the persistence layer: Parameter Required Default Description table_name Table name to store state key_attr id Partition key of the table. Hashed representation of the payload (unless sort_key_attr is specified) expiry_attr expiration Unix timestamp of when record expires status_attr status Stores status of the lambda execution during and after invocation data_attr data Stores results of successfully executed Lambda handlers validation_key_attr validation Hashed representation of the parts of the event used for validation sort_key_attr Sort key of the table (if table is configured with a sort key). static_pk_value idempotency#{LAMBDA_FUNCTION_NAME} Static value to use as the partition key. Only used when sort_key_attr is set.","title":"DynamoDBPersistenceLayer"},{"location":"tutorial/idempotency/#advanced","text":"","title":"Advanced"},{"location":"tutorial/idempotency/#customizing-the-default-behavior","text":"Idempotent decorator can be further configured with IdempotencyConfig as seen in the previous example. These are the available options for further configuration Parameter Default Description event_key_jmespath \"\" JMESPath expression to extract the idempotency key from the event record using built-in functions payload_validation_jmespath \"\" JMESPath expression to validate whether certain parameters have changed in the event while the event payload raise_on_no_idempotency_key False Raise exception if no idempotency key was found in the request expires_after_seconds 3600 The number of seconds to wait before a record is expired use_local_cache False Whether to locally cache idempotency results local_cache_max_items 256 Max number of items to store in local cache hash_function md5 Function to use for calculating hashes, as provided by hashlib in the standard library.","title":"Customizing the default behavior"},{"location":"tutorial/idempotency/#handling-concurrent-executions-with-the-same-payload","text":"This utility will raise an IdempotencyAlreadyInProgressError exception if you receive multiple invocations with the same payload while the first invocation hasn't completed yet . Info If you receive IdempotencyAlreadyInProgressError , you can safely retry the operation. This is a locking mechanism for correctness. Since we don't know the result from the first invocation yet, we can't safely allow another concurrent execution.","title":"Handling concurrent executions with the same payload"},{"location":"tutorial/idempotency/#using-in-memory-cache","text":"By default, in-memory local caching is disabled , since we don't know how much memory you consume per invocation compared to the maximum configured in your Lambda function. Note: This in-memory cache is local to each Lambda execution environment This means it will be effective in cases where your function's concurrency is low in comparison to the number of \"retry\" invocations with the same payload, because cache might be empty. You can enable in-memory caching with the use_local_cache parameter: Caching idempotent transactions in-memory to prevent multiple calls to storage 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) config = IdempotencyConfig ( event_key_jmespath = \"body\" , use_local_cache = True , ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... When enabled, the default is to cache a maximum of 256 records in each Lambda execution environment - You can change it with the local_cache_max_items parameter.","title":"Using in-memory cache"},{"location":"tutorial/idempotency/#expiring-idempotency-records","text":"Note By default, we expire idempotency records after an hour (3600 seconds). In most cases, it is not desirable to store the idempotency records forever. Rather, you want to guarantee that the same payload won't be executed within a period of time. You can change this window with the expires_after_seconds parameter: Adjusting cache TTL 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) config = IdempotencyConfig ( event_key_jmespath = \"body\" , expires_after_seconds = 5 * 60 , # 5 minutes ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... This will mark any records older than 5 minutes as expired, and the lambda handler will be executed as normal if it is invoked with a matching payload. Note: DynamoDB time-to-live field This utility uses expiration as the TTL field in DynamoDB, as demonstrated in the SAM example earlier .","title":"Expiring idempotency records"},{"location":"tutorial/idempotency/#payload-validation","text":"Question: What if your function is invoked with the same payload except some outer parameters have changed? Example: A payment transaction for a given productID was requested twice for the same customer, however the amount to be paid has changed in the second transaction . By default, we will return the same result as it returned before, however in this instance it may be misleading; we provide a fail fast payload validation to address this edge case. With payload_validation_jmespath , you can provide an additional JMESPath expression to specify which part of the event body should be validated against previous idempotent invocations app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"[userDetail, productId]\" , payload_validation_jmespath = \"amount\" ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): # Creating a subscription payment is a side # effect of calling this function! payment = create_subscription_payment ( user = event [ 'userDetail' ][ 'username' ], product = event [ 'product_id' ], amount = event [ 'amount' ] ) ... return { \"message\" : \"success\" , \"statusCode\" : 200 , \"payment_id\" : payment . id , \"amount\" : payment . amount } Example Event 1 1 2 3 4 5 6 7 8 9 { \"userDetail\" : { \"username\" : \"User1\" , \"user_email\" : \"user@example.com\" }, \"productId\" : 1500 , \"charge_type\" : \"subscription\" , \"amount\" : 500 } Example Event 2 1 2 3 4 5 6 7 8 9 { \"userDetail\" : { \"username\" : \"User1\" , \"user_email\" : \"user@example.com\" }, \"productId\" : 1500 , \"charge_type\" : \"subscription\" , \"amount\" : 1 } In this example, the userDetail and productId keys are used as the payload to generate the idempotency key, as per event_key_jmespath parameter. Note If we try to send the same request but with a different amount, we will raise IdempotencyValidationError . Without payload validation, we would have returned the same result as we did for the initial request. Since we're also returning an amount in the response, this could be quite confusing for the client. By using payload_validation_jmespath=\"amount\" , we prevent this potentially confusing behavior and instead raise an Exception.","title":"Payload validation"},{"location":"tutorial/idempotency/#making-idempotency-key-required","text":"If you want to enforce that an idempotency key is required, you can set raise_on_no_idempotency_key to True . This means that we will raise IdempotencyKeyError if the evaluation of event_key_jmespath is None . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) # Requires \"user\".\"uid\" and \"order_id\" to be present config = IdempotencyConfig ( event_key_jmespath = \"[user.uid, order_id]\" , raise_on_no_idempotency_key = True , ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): pass Success Event 1 2 3 4 5 6 7 { \"user\" : { \"uid\" : \"BB0D045C-8878-40C8-889E-38B3CB0A61B1\" , \"name\" : \"Foo\" }, \"order_id\" : 10000 } Failure Event Notice that order_id is now accidentally within user key 1 2 3 4 5 6 7 { \"user\" : { \"uid\" : \"DE0D000E-1234-10D1-991E-EAC1DD1D52C8\" , \"name\" : \"Joe Bloggs\" , \"order_id\" : 10000 }, }","title":"Making idempotency key required"},{"location":"tutorial/idempotency/#customizing-boto-configuration","text":"The boto_config and boto3_session parameters enable you to pass in a custom botocore config object or a custom boto3 session when constructing the persistence store. Custom session 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import boto3 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) boto3_session = boto3 . session . Session () persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , boto3_session = boto3_session ) config = IdempotencyConfig ( event_key_jmespath = \"body\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... Custom config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from botocore.config import Config from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"body\" ) boto_config = Config () persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , boto_config = boto_config ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ...","title":"Customizing boto configuration"},{"location":"tutorial/idempotency/#using-a-dynamodb-table-with-a-composite-primary-key","text":"When using a composite primary key table (hash+range key), use sort_key_attr parameter when initializing your persistence layer. With this setting, we will save the idempotency key in the sort key instead of the primary key. By default, the primary key will now be set to idempotency#{LAMBDA_FUNCTION_NAME} . You can optionally set a static value for the partition key using the static_pk_value parameter. Reusing a DynamoDB table that uses a composite primary key 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.idempotency import DynamoDBPersistenceLayer , idempotent persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , sort_key_attr = 'sort_key' ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): return { \"message\" : \"success\" : \"id\" : event [ 'body' ][ 'id]} The example function above would cause data to be stored in DynamoDB like this: id sort_key expiration status data idempotency#MyLambdaFunction 1e956ef7da78d0cb890be999aecc0c9e 1636549553 COMPLETED {\"id\": 12391, \"message\": \"success\"} idempotency#MyLambdaFunction 2b2cdb5f86361e97b4383087c1ffdf27 1636549571 COMPLETED {\"id\": 527212, \"message\": \"success\"} idempotency#MyLambdaFunction f091d2527ad1c78f05d54cc3f363be80 1636549585 IN_PROGRESS","title":"Using a DynamoDB table with a composite primary key"},{"location":"tutorial/idempotency/#bring-your-own-persistent-store","text":"This utility provides an abstract base class (ABC), so that you can implement your choice of persistent storage layer. You can inherit from the BasePersistenceLayer class and implement the abstract methods _get_record , _put_record , _update_record and _delete_record . Excerpt DynamoDB Persisntence Layer implementation for reference 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 import datetime import logging from typing import Any , Dict , Optional import boto3 from botocore.config import Config from aws_lambda_powertools.utilities.idempotency import BasePersistenceLayer from aws_lambda_powertools.utilities.idempotency.exceptions import ( IdempotencyItemAlreadyExistsError , IdempotencyItemNotFoundError , ) from aws_lambda_powertools.utilities.idempotency.persistence.base import DataRecord logger = logging . getLogger ( __name__ ) class DynamoDBPersistenceLayer ( BasePersistenceLayer ): def __init__ ( self , table_name : str , key_attr : str = \"id\" , expiry_attr : str = \"expiration\" , status_attr : str = \"status\" , data_attr : str = \"data\" , validation_key_attr : str = \"validation\" , boto_config : Optional [ Config ] = None , boto3_session : Optional [ boto3 . session . Session ] = None , ): boto_config = boto_config or Config () session = boto3_session or boto3 . session . Session () self . _ddb_resource = session . resource ( \"dynamodb\" , config = boto_config ) self . table_name = table_name self . table = self . _ddb_resource . Table ( self . table_name ) self . key_attr = key_attr self . expiry_attr = expiry_attr self . status_attr = status_attr self . data_attr = data_attr self . validation_key_attr = validation_key_attr super ( DynamoDBPersistenceLayer , self ) . __init__ () def _item_to_data_record ( self , item : Dict [ str , Any ]) -> DataRecord : \"\"\" Translate raw item records from DynamoDB to DataRecord Parameters ---------- item: Dict[str, Union[str, int]] Item format from dynamodb response Returns ------- DataRecord representation of item \"\"\" return DataRecord ( idempotency_key = item [ self . key_attr ], status = item [ self . status_attr ], expiry_timestamp = item [ self . expiry_attr ], response_data = item . get ( self . data_attr ), payload_hash = item . get ( self . validation_key_attr ), ) def _get_record ( self , idempotency_key ) -> DataRecord : response = self . table . get_item ( Key = { self . key_attr : idempotency_key }, ConsistentRead = True ) try : item = response [ \"Item\" ] except KeyError : raise IdempotencyItemNotFoundError return self . _item_to_data_record ( item ) def _put_record ( self , data_record : DataRecord ) -> None : item = { self . key_attr : data_record . idempotency_key , self . expiry_attr : data_record . expiry_timestamp , self . status_attr : data_record . status , } if self . payload_validation_enabled : item [ self . validation_key_attr ] = data_record . payload_hash now = datetime . datetime . now () try : logger . debug ( f \"Putting record for idempotency key: { data_record . idempotency_key } \" ) self . table . put_item ( Item = item , ConditionExpression = f \"attribute_not_exists( { self . key_attr } ) OR { self . expiry_attr } < :now\" , ExpressionAttributeValues = { \":now\" : int ( now . timestamp ())}, ) except self . _ddb_resource . meta . client . exceptions . ConditionalCheckFailedException : logger . debug ( f \"Failed to put record for already existing idempotency key: { data_record . idempotency_key } \" ) raise IdempotencyItemAlreadyExistsError def _update_record ( self , data_record : DataRecord ): logger . debug ( f \"Updating record for idempotency key: { data_record . idempotency_key } \" ) update_expression = \"SET #response_data = :response_data, #expiry = :expiry, #status = :status\" expression_attr_values = { \":expiry\" : data_record . expiry_timestamp , \":response_data\" : data_record . response_data , \":status\" : data_record . status , } expression_attr_names = { \"#response_data\" : self . data_attr , \"#expiry\" : self . expiry_attr , \"#status\" : self . status_attr , } if self . payload_validation_enabled : update_expression += \", #validation_key = :validation_key\" expression_attr_values [ \":validation_key\" ] = data_record . payload_hash expression_attr_names [ \"#validation_key\" ] = self . validation_key_attr kwargs = { \"Key\" : { self . key_attr : data_record . idempotency_key }, \"UpdateExpression\" : update_expression , \"ExpressionAttributeValues\" : expression_attr_values , \"ExpressionAttributeNames\" : expression_attr_names , } self . table . update_item ( ** kwargs ) def _delete_record ( self , data_record : DataRecord ) -> None : logger . debug ( f \"Deleting record for idempotency key: { data_record . idempotency_key } \" ) self . table . delete_item ( Key = { self . key_attr : data_record . idempotency_key },) Danger Pay attention to the documentation for each - you may need to perform additional checks inside these methods to ensure the idempotency guarantees remain intact. For example, the _put_record method needs to raise an exception if a non-expired record already exists in the data store with a matching key.","title":"Bring your own persistent store"},{"location":"tutorial/idempotency/#compatibility-with-other-utilities","text":"","title":"Compatibility with other utilities"},{"location":"tutorial/idempotency/#validation-utility","text":"The idempotency utility can be used with the validator decorator. Ensure that idempotency is the innermost decorator. Warning If you use an envelope with the validator, the event received by the idempotency utility will be the unwrapped event - not the \"raw\" event Lambda was invoked with. Make sure to account for this behaviour, if you set the event_key_jmespath . Using Idempotency with JSONSchema Validation utility 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.validation import validator , envelopes from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"[message, username]\" ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @validator ( envelope = envelopes . API_GATEWAY_HTTP ) @idempotent ( config = config , persistence_store = persistence_layer ) def lambda_handler ( event , context ): cause_some_side_effects ( event [ 'username' ) return { \"message\" : event [ 'message' ], \"statusCode\" : 200 } Tip: JMESPath Powertools functions are also available Built-in functions known in the validation utility like powertools_json , powertools_base64 , powertools_base64_gzip are also available to use in this utility.","title":"Validation utility"},{"location":"tutorial/idempotency/#testing-your-code","text":"The idempotency utility provides several routes to test your code.","title":"Testing your code"},{"location":"tutorial/idempotency/#disabling-the-idempotency-utility","text":"When testing your code, you may wish to disable the idempotency logic altogether and focus on testing your business logic. To do this, you can set the environment variable POWERTOOLS_IDEMPOTENCY_DISABLED with a truthy value. If you prefer setting this for specific tests, and are using Pytest, you can use monkeypatch fixture: tests.py 1 2 3 4 5 6 def test_idempotent_lambda_handler ( monkeypatch ): # Set POWERTOOLS_IDEMPOTENCY_DISABLED before calling decorated functions monkeypatch . setenv ( \"POWERTOOLS_IDEMPOTENCY_DISABLED\" , 1 ) result = handler () ... app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"idempotency\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): print ( 'expensive operation' ) return { \"payment_id\" : 12345 , \"message\" : \"success\" , \"statusCode\" : 200 , }","title":"Disabling the idempotency utility"},{"location":"tutorial/idempotency/#testing-with-dynamodb-local","text":"To test with DynamoDB Local , you can replace the Table resource used by the persistence layer with one you create inside your tests. This allows you to set the endpoint_url. tests.py 1 2 3 4 5 6 7 8 9 10 11 12 import boto3 import app def test_idempotent_lambda (): # Create our own Table resource using the endpoint for our DynamoDB Local instance resource = boto3 . resource ( \"dynamodb\" , endpoint_url = 'http://localhost:8000' ) table = resource . Table ( app . persistence_layer . table_name ) app . persistence_layer . table = table result = app . handler ({ 'testkey' : 'testvalue' }, {}) assert result [ 'payment_id' ] == 12345 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"idempotency\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): print ( 'expensive operation' ) return { \"payment_id\" : 12345 , \"message\" : \"success\" , \"statusCode\" : 200 , }","title":"Testing with DynamoDB Local"},{"location":"tutorial/idempotency/#how-do-i-mock-all-dynamodb-io-operations","text":"The idempotency utility lazily creates the dynamodb Table which it uses to access DynamoDB. This means it is possible to pass a mocked Table resource, or stub various methods. tests.py 1 2 3 4 5 6 7 8 9 10 from unittest.mock import MagicMock import app def test_idempotent_lambda (): table = MagicMock () app . persistence_layer . table = table result = app . handler ({ 'testkey' : 'testvalue' }, {}) table . put_item . assert_called () ... app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"idempotency\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): print ( 'expensive operation' ) return { \"payment_id\" : 12345 , \"message\" : \"success\" , \"statusCode\" : 200 , }","title":"How do I mock all DynamoDB I/O operations"},{"location":"tutorial/idempotency/#extra-resources","text":"If you're interested in a deep dive on how Amazon uses idempotency when building our APIs, check out this article .","title":"Extra resources"},{"location":"utilities/batch/","text":"The batch processing utility handles partial failures when processing batches from Amazon SQS, Amazon Kinesis Data Streams, and Amazon DynamoDB Streams. Key Features \u00b6 Reports batch item failures to reduce number of retries for a record upon errors Simple interface to process each batch record Integrates with Event Source Data Classes and Parser (Pydantic) for self-documenting record schema Build your own batch processor by extending primitives Background \u00b6 When using SQS, Kinesis Data Streams, or DynamoDB Streams as a Lambda event source, your Lambda functions are triggered with a batch of messages. If your function fails to process any message from the batch, the entire batch returns to your queue or stream. This same batch is then retried until either condition happens first: a) your Lambda function returns a successful response, b) record reaches maximum retry attempts, or c) when records expire. With this utility, batch records are processed individually \u2013 only messages that failed to be processed return to the queue or stream for a further retry. This works when two mechanisms are in place: ReportBatchItemFailures is set in your SQS, Kinesis, or DynamoDB event source properties A specific response is returned so Lambda knows which records should not be deleted during partial responses Warning: This utility lowers the chance of processing records more than once; it does not guarantee it We recommend implementing processing logic in an idempotent manner wherever possible. You can find more details on how Lambda works with either SQS , Kinesis , or DynamoDB in the AWS Documentation. Getting started \u00b6 Regardless whether you're using SQS, Kinesis Data Streams or DynamoDB Streams, you must configure your Lambda function event source to use `ReportBatchItemFailures . You do not need any additional IAM permissions to use this utility, except for what each event source requires. Required resources \u00b6 The remaining sections of the documentation will rely on these samples. For completeness, this demonstrates IAM permissions and Dead Letter Queue where batch records will be sent after 2 retries were attempted. SQS template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 AWSTemplateFormatVersion : '2010-09-09' Transform : AWS::Serverless-2016-10-31 Description : partial batch response sample Globals : Function : Timeout : 5 MemorySize : 256 Runtime : python3.9 Tracing : Active Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_SERVICE_NAME : hello Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Handler : app.lambda_handler CodeUri : hello_world Policies : - SQSPollerPolicy : QueueName : !GetAtt SampleQueue.QueueName Events : Batch : Type : SQS Properties : Queue : !GetAtt SampleQueue.Arn FunctionResponseTypes : - ReportBatchItemFailures SampleDLQ : Type : AWS::SQS::Queue SampleQueue : Type : AWS::SQS::Queue Properties : VisibilityTimeout : 30 # Fn timeout * 6 RedrivePolicy : maxReceiveCount : 2 deadLetterTargetArn : !GetAtt SampleDLQ.Arn Kinesis Data Streams template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 AWSTemplateFormatVersion : '2010-09-09' Transform : AWS::Serverless-2016-10-31 Description : partial batch response sample Globals : Function : Timeout : 5 MemorySize : 256 Runtime : python3.9 Tracing : Active Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_SERVICE_NAME : hello Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Handler : app.lambda_handler CodeUri : hello_world Policies : # Lambda Destinations require additional permissions # to send failure records to DLQ from Kinesis/DynamoDB - Version : \"2012-10-17\" Statement : Effect : \"Allow\" Action : - sqs:GetQueueAttributes - sqs:GetQueueUrl - sqs:SendMessage Resource : !GetAtt SampleDLQ.Arn Events : KinesisStream : Type : Kinesis Properties : Stream : !GetAtt SampleStream.Arn BatchSize : 100 StartingPosition : LATEST MaximumRetryAttempts : 2 DestinationConfig : OnFailure : Destination : !GetAtt SampleDLQ.Arn FunctionResponseTypes : - ReportBatchItemFailures SampleDLQ : Type : AWS::SQS::Queue SampleStream : Type : AWS::Kinesis::Stream Properties : ShardCount : 1 DynamoDB Streams template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 AWSTemplateFormatVersion : '2010-09-09' Transform : AWS::Serverless-2016-10-31 Description : partial batch response sample Globals : Function : Timeout : 5 MemorySize : 256 Runtime : python3.9 Tracing : Active Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_SERVICE_NAME : hello Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Handler : app.lambda_handler CodeUri : hello_world Policies : # Lambda Destinations require additional permissions # to send failure records from Kinesis/DynamoDB - Version : \"2012-10-17\" Statement : Effect : \"Allow\" Action : - sqs:GetQueueAttributes - sqs:GetQueueUrl - sqs:SendMessage Resource : !GetAtt SampleDLQ.Arn Events : DynamoDBStream : Type : DynamoDB Properties : Stream : !GetAtt SampleTable.StreamArn StartingPosition : LATEST MaximumRetryAttempts : 2 DestinationConfig : OnFailure : Destination : !GetAtt SampleDLQ.Arn FunctionResponseTypes : - ReportBatchItemFailures SampleDLQ : Type : AWS::SQS::Queue SampleTable : Type : AWS::DynamoDB::Table Properties : BillingMode : PAY_PER_REQUEST AttributeDefinitions : - AttributeName : pk AttributeType : S - AttributeName : sk AttributeType : S KeySchema : - AttributeName : pk KeyType : HASH - AttributeName : sk KeyType : RANGE SSESpecification : SSEEnabled : yes StreamSpecification : StreamViewType : NEW_AND_OLD_IMAGES Processing messages from SQS \u00b6 Processing batches from SQS works in four stages: Instantiate BatchProcessor and choose EventType.SQS for the event type Define your function to handle each batch record, and use SQSRecord type annotation for autocompletion Use either batch_processor decorator or your instantiated processor as a context manager to kick off processing Return the appropriate response contract to Lambda via .response() processor method Info This code example optionally uses Tracer and Logger for completion. As a decorator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . SQS ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : SQSRecord ): payload : str = record . body if payload : item : dict = json . loads ( payload ) ... @logger . inject_lambda_context @tracer . capture_lambda_handler @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response () As a context manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . SQS ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : SQSRecord ): payload : str = record . body if payload : item : dict = json . loads ( payload ) ... @logger . inject_lambda_context @tracer . capture_lambda_handler def lambda_handler ( event , context : LambdaContext ): batch = event [ \"Records\" ] with processor ( records = batch , handler = record_handler ): processed_messages = processor . process () # kick off processing, return list[tuple] return processor . response () Sample response The second record failed to be processed, therefore the processor added its message ID in the response. 1 2 3 4 5 6 7 { 'batchItemFailures' : [ { 'itemIdentifier' : '244fc6b4-87a3-44ab-83d2-361172410c3a' } ] } Sample event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"Records\" : [ { \"messageId\" : \"059f36b4-87a3-44ab-83d2-661975830a7d\" , \"receiptHandle\" : \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a\" , \"body\" : \"{\\\"Message\\\": \\\"success\\\"}\" , \"attributes\" : { \"ApproximateReceiveCount\" : \"1\" , \"SentTimestamp\" : \"1545082649183\" , \"SenderId\" : \"AIDAIENQZJOLO23YVJ4VO\" , \"ApproximateFirstReceiveTimestamp\" : \"1545082649185\" }, \"messageAttributes\" : {}, \"md5OfBody\" : \"e4e68fb7bd0e697a0ae8f1bb342846b3\" , \"eventSource\" : \"aws:sqs\" , \"eventSourceARN\" : \"arn:aws:sqs:us-east-2: 123456789012:my-queue\" , \"awsRegion\" : \"us-east-1\" }, { \"messageId\" : \"244fc6b4-87a3-44ab-83d2-361172410c3a\" , \"receiptHandle\" : \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a\" , \"body\" : \"SGVsbG8sIHRoaXMgaXMgYSB0ZXN0Lg==\" , \"attributes\" : { \"ApproximateReceiveCount\" : \"1\" , \"SentTimestamp\" : \"1545082649183\" , \"SenderId\" : \"AIDAIENQZJOLO23YVJ4VO\" , \"ApproximateFirstReceiveTimestamp\" : \"1545082649185\" }, \"messageAttributes\" : {}, \"md5OfBody\" : \"e4e68fb7bd0e697a0ae8f1bb342846b3\" , \"eventSource\" : \"aws:sqs\" , \"eventSourceARN\" : \"arn:aws:sqs:us-east-2: 123456789012:my-queue\" , \"awsRegion\" : \"us-east-1\" } ] } Processing messages from Kinesis \u00b6 Processing batches from Kinesis works in four stages: Instantiate BatchProcessor and choose EventType.KinesisDataStreams for the event type Define your function to handle each batch record, and use KinesisStreamRecord type annotation for autocompletion Use either batch_processor decorator or your instantiated processor as a context manager to kick off processing Return the appropriate response contract to Lambda via .response() processor method Info This code example optionally uses Tracer and Logger for completion. As a decorator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.data_classes.kinesis_stream_event import KinesisStreamRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . KinesisDataStreams ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : KinesisStreamRecord ): logger . info ( record . kinesis . data_as_text ) payload : dict = record . kinesis . data_as_json () ... @logger . inject_lambda_context @tracer . capture_lambda_handler @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response () As a context manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.data_classes.kinesis_stream_event import KinesisStreamRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . KinesisDataStreams ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : KinesisStreamRecord ): logger . info ( record . kinesis . data_as_text ) payload : dict = record . kinesis . data_as_json () ... @logger . inject_lambda_context @tracer . capture_lambda_handler def lambda_handler ( event , context : LambdaContext ): batch = event [ \"Records\" ] with processor ( records = batch , handler = record_handler ): processed_messages = processor . process () # kick off processing, return list[tuple] return processor . response () Sample response The second record failed to be processed, therefore the processor added its sequence number in the response. 1 2 3 4 5 6 7 { 'batchItemFailures' : [ { 'itemIdentifier' : '6006958808509702859251049540584488075644979031228738' } ] } Sample event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"Records\" : [ { \"kinesis\" : { \"kinesisSchemaVersion\" : \"1.0\" , \"partitionKey\" : \"1\" , \"sequenceNumber\" : \"4107859083838847772757075850904226111829882106684065\" , \"data\" : \"eyJNZXNzYWdlIjogInN1Y2Nlc3MifQ==\" , \"approximateArrivalTimestamp\" : 1545084650.987 }, \"eventSource\" : \"aws:kinesis\" , \"eventVersion\" : \"1.0\" , \"eventID\" : \"shardId-000000000006:4107859083838847772757075850904226111829882106684065\" , \"eventName\" : \"aws:kinesis:record\" , \"invokeIdentityArn\" : \"arn:aws:iam::123456789012:role/lambda-role\" , \"awsRegion\" : \"us-east-2\" , \"eventSourceARN\" : \"arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream\" }, { \"kinesis\" : { \"kinesisSchemaVersion\" : \"1.0\" , \"partitionKey\" : \"1\" , \"sequenceNumber\" : \"6006958808509702859251049540584488075644979031228738\" , \"data\" : \"c3VjY2Vzcw==\" , \"approximateArrivalTimestamp\" : 1545084650.987 }, \"eventSource\" : \"aws:kinesis\" , \"eventVersion\" : \"1.0\" , \"eventID\" : \"shardId-000000000006:6006958808509702859251049540584488075644979031228738\" , \"eventName\" : \"aws:kinesis:record\" , \"invokeIdentityArn\" : \"arn:aws:iam::123456789012:role/lambda-role\" , \"awsRegion\" : \"us-east-2\" , \"eventSourceARN\" : \"arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream\" } ] } Processing messages from DynamoDB \u00b6 Processing batches from Kinesis works in four stages: Instantiate BatchProcessor and choose EventType.DynamoDBStreams for the event type Define your function to handle each batch record, and use DynamoDBRecord type annotation for autocompletion Use either batch_processor decorator or your instantiated processor as a context manager to kick off processing Return the appropriate response contract to Lambda via .response() processor method Info This code example optionally uses Tracer and Logger for completion. As a decorator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import DynamoDBRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . DynamoDBStreams ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : DynamoDBRecord ): logger . info ( record . dynamodb . new_image ) payload : dict = json . loads ( record . dynamodb . new_image . get ( \"Message\" ) . get_value ) # alternatively: # changes: Dict[str, dynamo_db_stream_event.AttributeValue] = record.dynamodb.new_image # payload = change.get(\"Message\").raw_event -> {\"S\": \"<payload>\"} ... @logger . inject_lambda_context @tracer . capture_lambda_handler @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response () As a context manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import DynamoDBRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . DynamoDBStreams ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : DynamoDBRecord ): logger . info ( record . dynamodb . new_image ) payload : dict = json . loads ( record . dynamodb . new_image . get ( \"item\" ) . s_value ) # alternatively: # changes: Dict[str, dynamo_db_stream_event.AttributeValue] = record.dynamodb.new_image # payload = change.get(\"Message\").raw_event -> {\"S\": \"<payload>\"} ... @logger . inject_lambda_context @tracer . capture_lambda_handler def lambda_handler ( event , context : LambdaContext ): batch = event [ \"Records\" ] with processor ( records = batch , handler = record_handler ): processed_messages = processor . process () # kick off processing, return list[tuple] return processor . response () Sample response The second record failed to be processed, therefore the processor added its sequence number in the response. 1 2 3 4 5 6 7 { 'batchItemFailures' : [ { 'itemIdentifier' : '8640712661' } ] } Sample event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 { \"Records\" : [ { \"eventID\" : \"1\" , \"eventVersion\" : \"1.0\" , \"dynamodb\" : { \"Keys\" : { \"Id\" : { \"N\" : \"101\" } }, \"NewImage\" : { \"Message\" : { \"S\" : \"failure\" } }, \"StreamViewType\" : \"NEW_AND_OLD_IMAGES\" , \"SequenceNumber\" : \"3275880929\" , \"SizeBytes\" : 26 }, \"awsRegion\" : \"us-west-2\" , \"eventName\" : \"INSERT\" , \"eventSourceARN\" : \"eventsource_arn\" , \"eventSource\" : \"aws:dynamodb\" }, { \"eventID\" : \"1\" , \"eventVersion\" : \"1.0\" , \"dynamodb\" : { \"Keys\" : { \"Id\" : { \"N\" : \"101\" } }, \"NewImage\" : { \"SomethingElse\" : { \"S\" : \"success\" } }, \"StreamViewType\" : \"NEW_AND_OLD_IMAGES\" , \"SequenceNumber\" : \"8640712661\" , \"SizeBytes\" : 26 }, \"awsRegion\" : \"us-west-2\" , \"eventName\" : \"INSERT\" , \"eventSourceARN\" : \"eventsource_arn\" , \"eventSource\" : \"aws:dynamodb\" } ] } Partial failure mechanics \u00b6 All records in the batch will be passed to this handler for processing, even if exceptions are thrown - Here's the behaviour after completing the batch: All records successfully processed . We will return an empty list of item failures {'batchItemFailures': []} Partial success with some exceptions . We will return a list of all item IDs/sequence numbers that failed processing All records failed to be processed . We will raise BatchProcessingError exception with a list of all exceptions raised when processing Warning You will not have access to the processed messages within the Lambda Handler; use context manager for that. All processing logic will and should be performed by the record_handler function. Advanced \u00b6 Pydantic integration \u00b6 You can bring your own Pydantic models via model parameter when inheriting from SqsRecordModel , KinesisDataStreamRecord , or DynamoDBStreamRecordModel Inheritance is importance because we need to access message IDs and sequence numbers from these records in the event of failure. Mypy is fully integrated with this utility, so it should identify whether you're passing the incorrect Model. SQS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.parser.models import SqsRecordModel from aws_lambda_powertools.utilities.typing import LambdaContext class Order ( BaseModel ): item : dict class OrderSqsRecord ( SqsRecordModel ): body : Order # auto transform json string # so Pydantic can auto-initialize nested Order model @validator ( \"body\" , pre = True ) def transform_body_to_dict ( cls , value : str ): return json . loads ( value ) processor = BatchProcessor ( event_type = EventType . SQS , model = OrderSqsRecord ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : OrderSqsRecord ): return record . body . item @logger . inject_lambda_context @tracer . capture_lambda_handler @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response () Kinesis Data Streams 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.parser.models import KinesisDataStreamRecord from aws_lambda_powertools.utilities.typing import LambdaContext class Order ( BaseModel ): item : dict class OrderKinesisPayloadRecord ( KinesisDataStreamRecordPayload ): data : Order # auto transform json string # so Pydantic can auto-initialize nested Order model @validator ( \"data\" , pre = True ) def transform_message_to_dict ( cls , value : str ): # Powertools KinesisDataStreamRecordModel already decodes b64 to str here return json . loads ( value ) class OrderKinesisRecord ( KinesisDataStreamRecordModel ): kinesis : OrderKinesisPayloadRecord processor = BatchProcessor ( event_type = EventType . KinesisDataStreams , model = OrderKinesisRecord ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : OrderKinesisRecord ): return record . kinesis . data . item @logger . inject_lambda_context @tracer . capture_lambda_handler @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response () DynamoDB Streams 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import json from typing import Dict , Literal from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.parser.models import DynamoDBStreamRecordModel from aws_lambda_powertools.utilities.typing import LambdaContext class Order ( BaseModel ): item : dict class OrderDynamoDB ( BaseModel ): Message : Order # auto transform json string # so Pydantic can auto-initialize nested Order model @validator ( \"Message\" , pre = True ) def transform_message_to_dict ( cls , value : Dict [ Literal [ \"S\" ], str ]): return json . loads ( value [ \"S\" ]) class OrderDynamoDBChangeRecord ( DynamoDBStreamChangedRecordModel ): NewImage : Optional [ OrderDynamoDB ] OldImage : Optional [ OrderDynamoDB ] class OrderDynamoDBRecord ( DynamoDBStreamRecordModel ): dynamodb : OrderDynamoDBChangeRecord processor = BatchProcessor ( event_type = EventType . DynamoDBStreams , model = OrderKinesisRecord ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : OrderDynamoDBRecord ): return record . dynamodb . NewImage . Message . item @logger . inject_lambda_context @tracer . capture_lambda_handler @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response () Accessing processed messages \u00b6 Use the context manager to access a list of all returned values from your record_handler function. When successful . We will include a tuple with success , the result of record_handler , and the batch record When failed . We will include a tuple with fail , exception as a string, and the batch record Accessing processed messages via context manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import json from typing import Any , List , Literal , Union from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import ( BatchProcessor , EventType , FailureResponse , SuccessResponse , batch_processor ) from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . SQS ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : SQSRecord ): payload : str = record . body if payload : item : dict = json . loads ( payload ) ... @logger . inject_lambda_context @tracer . capture_lambda_handler def lambda_handler ( event , context : LambdaContext ): batch = event [ \"Records\" ] with processor ( records = batch , handler = record_handler ): processed_messages : List [ Union [ SuccessResponse , FailureResponse ]] = processor . process () for message in processed_messages : status : Union [ Literal [ \"success\" ], Literal [ \"fail\" ]] = message [ 0 ] result : Any = message [ 1 ] record : SQSRecord = message [ 2 ] return processor . response () Extending BatchProcessor \u00b6 You might want to bring custom logic to the existing BatchProcessor to slightly override how we handle successes and failures. For these scenarios, you can subclass BatchProcessor and quickly override success_handler and failure_handler methods: success_handler() \u2013 Keeps track of successful batch records failure_handler() \u2013 Keeps track of failed batch records Example Let's suppose you'd like to add a metric named BatchRecordFailures for each batch record that failed processing Extending failure handling mechanism in BatchProcessor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from typing import Tuple from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit from aws_lambda_powertools.utilities.batch import batch_processor , BatchProcessor , ExceptionInfo , EventType , FailureResponse from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord class MyProcessor ( BatchProcessor ): def failure_handler ( self , record : SQSRecord , exception : ExceptionInfo ) -> FailureResponse : metrics . add_metric ( name = \"BatchRecordFailures\" , unit = MetricUnit . Count , value = 1 ) return super () . failure_handler ( record , exception ) processor = MyProcessor ( event_type = EventType . SQS ) metrics = Metrics ( namespace = \"test\" ) @tracer . capture_method def record_handler ( record : SQSRecord ): payload : str = record . body if payload : item : dict = json . loads ( payload ) ... @metrics . log_metrics ( capture_cold_start_metric = True ) @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response () Create your own partial processor \u00b6 You can create your own partial batch processor from scratch by inheriting the BasePartialProcessor class, and implementing _prepare() , _clean() and _process_record() . _process_record() \u2013 handles all processing logic for each individual message of a batch, including calling the record_handler (self.handler) _prepare() \u2013 called once as part of the processor initialization clean() \u2013 teardown logic called once after _process_record completes You can then use this class as a context manager, or pass it to batch_processor to use as a decorator on your Lambda handler function. Creating a custom batch processor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 from random import randint from aws_lambda_powertools.utilities.batch import BasePartialProcessor , batch_processor import boto3 import os table_name = os . getenv ( \"TABLE_NAME\" , \"table_not_found\" ) class MyPartialProcessor ( BasePartialProcessor ): \"\"\" Process a record and stores successful results at a Amazon DynamoDB Table Parameters ---------- table_name: str DynamoDB table name to write results to \"\"\" def __init__ ( self , table_name : str ): self . table_name = table_name super () . __init__ () def _prepare ( self ): # It's called once, *before* processing # Creates table resource and clean previous results self . ddb_table = boto3 . resource ( \"dynamodb\" ) . Table ( self . table_name ) self . success_messages . clear () def _clean ( self ): # It's called once, *after* closing processing all records (closing the context manager) # Here we're sending, at once, all successful messages to a ddb table with self . ddb_table . batch_writer () as batch : for result in self . success_messages : batch . put_item ( Item = result ) def _process_record ( self , record ): # It handles how your record is processed # Here we're keeping the status of each run # where self.handler is the record_handler function passed as an argument try : result = self . handler ( record ) # record_handler passed to decorator/context manager return self . success_handler ( record , result ) except Exception as exc : return self . failure_handler ( record , exc ) def success_handler ( self , record ): entry = ( \"success\" , result , record ) message = { \"age\" : result } self . success_messages . append ( message ) return entry def record_handler ( record ): return randint ( 0 , 100 ) @batch_processor ( record_handler = record_handler , processor = MyPartialProcessor ( table_name )) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Caveats \u00b6 Tracer response auto-capture for large batch sizes \u00b6 When using Tracer to capture responses for each batch record processing, you might exceed 64K of tracing data depending on what you return from your record_handler function, or how big is your batch size. If that's the case, you can configure Tracer to disable response auto-capturing . Disabling Tracer response auto-capturing 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . SQS ) tracer = Tracer () logger = Logger () @tracer . capture_method ( capture_response = False ) def record_handler ( record : SQSRecord ): payload : str = record . body if payload : item : dict = json . loads ( payload ) ... @logger . inject_lambda_context @tracer . capture_lambda_handler @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response () Testing your code \u00b6 As there is no external calls, you can unit test your code with BatchProcessor quite easily. Example : Given a SQS batch where the first batch record succeeds and the second fails processing, we should have a single item reported in the function response. test_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 import json from pathlib import Path from dataclasses import dataclass import pytest from src.app import lambda_handler , processor def load_event ( path : Path ): with path . open () as f : return json . load ( f ) @pytest . fixture def lambda_context (): @dataclass class LambdaContext : function_name : str = \"test\" memory_limit_in_mb : int = 128 invoked_function_arn : str = \"arn:aws:lambda:eu-west-1:809313241:function:test\" aws_request_id : str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\" return LambdaContext () @pytest . fixture () def sqs_event (): \"\"\"Generates API GW Event\"\"\" return load_event ( path = Path ( \"events/sqs_event.json\" )) def test_app_batch_partial_response ( sqs_event , lambda_context ): # GIVEN processor = app . processor # access processor for additional assertions successful_record = sqs_event [ \"Records\" ][ 0 ] failed_record = sqs_event [ \"Records\" ][ 1 ] expected_response = { \"batchItemFailures: [ { \"itemIdentifier\" : failed_record [ \"messageId\" ] } ] } # WHEN ret = app . lambda_handler ( sqs_event , lambda_context ) # THEN assert ret == expected_response assert len ( processor . fail_messages ) == 1 assert processor . success_messages [ 0 ] == successful_record src/app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . SQS ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : SQSRecord ): payload : str = record . body if payload : item : dict = json . loads ( payload ) ... @logger . inject_lambda_context @tracer . capture_lambda_handler @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response () Sample SQS event events/sqs_sample.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"Records\" : [ { \"messageId\" : \"059f36b4-87a3-44ab-83d2-661975830a7d\" , \"receiptHandle\" : \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a\" , \"body\" : \"{\\\"Message\\\": \\\"success\\\"}\" , \"attributes\" : { \"ApproximateReceiveCount\" : \"1\" , \"SentTimestamp\" : \"1545082649183\" , \"SenderId\" : \"AIDAIENQZJOLO23YVJ4VO\" , \"ApproximateFirstReceiveTimestamp\" : \"1545082649185\" }, \"messageAttributes\" : {}, \"md5OfBody\" : \"e4e68fb7bd0e697a0ae8f1bb342846b3\" , \"eventSource\" : \"aws:sqs\" , \"eventSourceARN\" : \"arn:aws:sqs:us-east-2: 123456789012:my-queue\" , \"awsRegion\" : \"us-east-1\" }, { \"messageId\" : \"244fc6b4-87a3-44ab-83d2-361172410c3a\" , \"receiptHandle\" : \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a\" , \"body\" : \"SGVsbG8sIHRoaXMgaXMgYSB0ZXN0Lg==\" , \"attributes\" : { \"ApproximateReceiveCount\" : \"1\" , \"SentTimestamp\" : \"1545082649183\" , \"SenderId\" : \"AIDAIENQZJOLO23YVJ4VO\" , \"ApproximateFirstReceiveTimestamp\" : \"1545082649185\" }, \"messageAttributes\" : {}, \"md5OfBody\" : \"e4e68fb7bd0e697a0ae8f1bb342846b3\" , \"eventSource\" : \"aws:sqs\" , \"eventSourceARN\" : \"arn:aws:sqs:us-east-2: 123456789012:my-queue\" , \"awsRegion\" : \"us-east-1\" } ] } FAQ \u00b6 Choosing between decorator and context manager \u00b6 Use context manager when you want access to the processed messages or handle BatchProcessingError exception when all records within the batch fail to be processed. Integrating exception handling with Sentry.io \u00b6 When using Sentry.io for error monitoring, you can override failure_handler to capture each processing exception with Sentry SDK: Credits to Charles-Axel Dein Integrating error tracking with Sentry.io 1 2 3 4 5 6 7 8 9 10 from typing import Tuple from aws_lambda_powertools.utilities.batch import BatchProcessor , FailureResponse from sentry_sdk import capture_exception class MyProcessor ( BatchProcessor ): def failure_handler ( self , record , exception ) -> FailureResponse : capture_exception () # send exception to Sentry return super () . failure_handler ( record , exception ) Legacy \u00b6 Tip This is kept for historical purposes. Use the new BatchProcessor instead. Migration guide \u00b6 Info Keep reading if you are using sqs_batch_processor or PartialSQSProcessor . As of Nov 2021 , this is no longer needed as both SQS, Kinesis, and DynamoDB Streams offer this capability natively with one caveat - it's an opt-in feature . Being a native feature, we no longer need to instantiate boto3 nor other customizations like exception suppressing \u2013 this lowers the cost of your Lambda function as you can delegate deleting partial failures to Lambda. Tip It's also easier to test since it's mostly a contract based response . You can migrate in three steps: If you are using sqs_batch_decorator you can now use batch_processor decorator If you were using PartialSQSProcessor you can now use BatchProcessor Change your Lambda Handler to return the new response format Decorator: Before 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.batch import sqs_batch_processor def record_handler ( record ): return do_something_with ( record [ \"body\" ]) @sqs_batch_processor ( record_handler = record_handler ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Decorator: After 1 2 3 4 5 6 7 8 9 10 11 12 13 import json from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor processor = BatchProcessor ( event_type = EventType . SQS ) def record_handler ( record ): return do_something_with ( record [ \"body\" ]) @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context ): return processor . response () Context manager: Before 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor from botocore.config import Config config = Config ( region_name = \"us-east-1\" ) def record_handler ( record ): return_value = do_something_with ( record [ \"body\" ]) return return_value def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = PartialSQSProcessor ( config = config ) with processor ( records , record_handler ): result = processor . process () return result Context manager: After 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor def record_handler ( record ): return_value = do_something_with ( record [ \"body\" ]) return return_value def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = BatchProcessor ( event_type = EventType . SQS ) with processor ( records , record_handler ): result = processor . process () return processor . response () Customizing boto configuration \u00b6 The config and boto3_session parameters enable you to pass in a custom botocore config object or a custom boto3 session when using the sqs_batch_processor decorator or PartialSQSProcessor class. Custom config example Decorator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.batch import sqs_batch_processor from botocore.config import Config config = Config ( region_name = \"us-east-1\" ) def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value @sqs_batch_processor ( record_handler = record_handler , config = config ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Context manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor from botocore.config import Config config = Config ( region_name = \"us-east-1\" ) def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = PartialSQSProcessor ( config = config ) with processor ( records , record_handler ): result = processor . process () return result Custom boto3 session example Decorator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.batch import sqs_batch_processor from botocore.config import Config session = boto3 . session . Session () def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value @sqs_batch_processor ( record_handler = record_handler , boto3_session = session ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Context manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor import boto3 session = boto3 . session . Session () def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = PartialSQSProcessor ( boto3_session = session ) with processor ( records , record_handler ): result = processor . process () return result Suppressing exceptions \u00b6 If you want to disable the default behavior where SQSBatchProcessingError is raised if there are any errors, you can pass the suppress_exception boolean argument. Decorator 1 2 3 4 5 from aws_lambda_powertools.utilities.batch import sqs_batch_processor @sqs_batch_processor ( record_handler = record_handler , config = config , suppress_exception = True ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Context manager 1 2 3 4 5 6 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor processor = PartialSQSProcessor ( config = config , suppress_exception = True ) with processor ( records , record_handler ): result = processor . process ()","title":"Batch Processing"},{"location":"utilities/batch/#key-features","text":"Reports batch item failures to reduce number of retries for a record upon errors Simple interface to process each batch record Integrates with Event Source Data Classes and Parser (Pydantic) for self-documenting record schema Build your own batch processor by extending primitives","title":"Key Features"},{"location":"utilities/batch/#background","text":"When using SQS, Kinesis Data Streams, or DynamoDB Streams as a Lambda event source, your Lambda functions are triggered with a batch of messages. If your function fails to process any message from the batch, the entire batch returns to your queue or stream. This same batch is then retried until either condition happens first: a) your Lambda function returns a successful response, b) record reaches maximum retry attempts, or c) when records expire. With this utility, batch records are processed individually \u2013 only messages that failed to be processed return to the queue or stream for a further retry. This works when two mechanisms are in place: ReportBatchItemFailures is set in your SQS, Kinesis, or DynamoDB event source properties A specific response is returned so Lambda knows which records should not be deleted during partial responses Warning: This utility lowers the chance of processing records more than once; it does not guarantee it We recommend implementing processing logic in an idempotent manner wherever possible. You can find more details on how Lambda works with either SQS , Kinesis , or DynamoDB in the AWS Documentation.","title":"Background"},{"location":"utilities/batch/#getting-started","text":"Regardless whether you're using SQS, Kinesis Data Streams or DynamoDB Streams, you must configure your Lambda function event source to use `ReportBatchItemFailures . You do not need any additional IAM permissions to use this utility, except for what each event source requires.","title":"Getting started"},{"location":"utilities/batch/#required-resources","text":"The remaining sections of the documentation will rely on these samples. For completeness, this demonstrates IAM permissions and Dead Letter Queue where batch records will be sent after 2 retries were attempted. SQS template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 AWSTemplateFormatVersion : '2010-09-09' Transform : AWS::Serverless-2016-10-31 Description : partial batch response sample Globals : Function : Timeout : 5 MemorySize : 256 Runtime : python3.9 Tracing : Active Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_SERVICE_NAME : hello Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Handler : app.lambda_handler CodeUri : hello_world Policies : - SQSPollerPolicy : QueueName : !GetAtt SampleQueue.QueueName Events : Batch : Type : SQS Properties : Queue : !GetAtt SampleQueue.Arn FunctionResponseTypes : - ReportBatchItemFailures SampleDLQ : Type : AWS::SQS::Queue SampleQueue : Type : AWS::SQS::Queue Properties : VisibilityTimeout : 30 # Fn timeout * 6 RedrivePolicy : maxReceiveCount : 2 deadLetterTargetArn : !GetAtt SampleDLQ.Arn Kinesis Data Streams template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 AWSTemplateFormatVersion : '2010-09-09' Transform : AWS::Serverless-2016-10-31 Description : partial batch response sample Globals : Function : Timeout : 5 MemorySize : 256 Runtime : python3.9 Tracing : Active Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_SERVICE_NAME : hello Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Handler : app.lambda_handler CodeUri : hello_world Policies : # Lambda Destinations require additional permissions # to send failure records to DLQ from Kinesis/DynamoDB - Version : \"2012-10-17\" Statement : Effect : \"Allow\" Action : - sqs:GetQueueAttributes - sqs:GetQueueUrl - sqs:SendMessage Resource : !GetAtt SampleDLQ.Arn Events : KinesisStream : Type : Kinesis Properties : Stream : !GetAtt SampleStream.Arn BatchSize : 100 StartingPosition : LATEST MaximumRetryAttempts : 2 DestinationConfig : OnFailure : Destination : !GetAtt SampleDLQ.Arn FunctionResponseTypes : - ReportBatchItemFailures SampleDLQ : Type : AWS::SQS::Queue SampleStream : Type : AWS::Kinesis::Stream Properties : ShardCount : 1 DynamoDB Streams template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 AWSTemplateFormatVersion : '2010-09-09' Transform : AWS::Serverless-2016-10-31 Description : partial batch response sample Globals : Function : Timeout : 5 MemorySize : 256 Runtime : python3.9 Tracing : Active Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_SERVICE_NAME : hello Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Handler : app.lambda_handler CodeUri : hello_world Policies : # Lambda Destinations require additional permissions # to send failure records from Kinesis/DynamoDB - Version : \"2012-10-17\" Statement : Effect : \"Allow\" Action : - sqs:GetQueueAttributes - sqs:GetQueueUrl - sqs:SendMessage Resource : !GetAtt SampleDLQ.Arn Events : DynamoDBStream : Type : DynamoDB Properties : Stream : !GetAtt SampleTable.StreamArn StartingPosition : LATEST MaximumRetryAttempts : 2 DestinationConfig : OnFailure : Destination : !GetAtt SampleDLQ.Arn FunctionResponseTypes : - ReportBatchItemFailures SampleDLQ : Type : AWS::SQS::Queue SampleTable : Type : AWS::DynamoDB::Table Properties : BillingMode : PAY_PER_REQUEST AttributeDefinitions : - AttributeName : pk AttributeType : S - AttributeName : sk AttributeType : S KeySchema : - AttributeName : pk KeyType : HASH - AttributeName : sk KeyType : RANGE SSESpecification : SSEEnabled : yes StreamSpecification : StreamViewType : NEW_AND_OLD_IMAGES","title":"Required resources"},{"location":"utilities/batch/#processing-messages-from-sqs","text":"Processing batches from SQS works in four stages: Instantiate BatchProcessor and choose EventType.SQS for the event type Define your function to handle each batch record, and use SQSRecord type annotation for autocompletion Use either batch_processor decorator or your instantiated processor as a context manager to kick off processing Return the appropriate response contract to Lambda via .response() processor method Info This code example optionally uses Tracer and Logger for completion. As a decorator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . SQS ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : SQSRecord ): payload : str = record . body if payload : item : dict = json . loads ( payload ) ... @logger . inject_lambda_context @tracer . capture_lambda_handler @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response () As a context manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . SQS ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : SQSRecord ): payload : str = record . body if payload : item : dict = json . loads ( payload ) ... @logger . inject_lambda_context @tracer . capture_lambda_handler def lambda_handler ( event , context : LambdaContext ): batch = event [ \"Records\" ] with processor ( records = batch , handler = record_handler ): processed_messages = processor . process () # kick off processing, return list[tuple] return processor . response () Sample response The second record failed to be processed, therefore the processor added its message ID in the response. 1 2 3 4 5 6 7 { 'batchItemFailures' : [ { 'itemIdentifier' : '244fc6b4-87a3-44ab-83d2-361172410c3a' } ] } Sample event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"Records\" : [ { \"messageId\" : \"059f36b4-87a3-44ab-83d2-661975830a7d\" , \"receiptHandle\" : \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a\" , \"body\" : \"{\\\"Message\\\": \\\"success\\\"}\" , \"attributes\" : { \"ApproximateReceiveCount\" : \"1\" , \"SentTimestamp\" : \"1545082649183\" , \"SenderId\" : \"AIDAIENQZJOLO23YVJ4VO\" , \"ApproximateFirstReceiveTimestamp\" : \"1545082649185\" }, \"messageAttributes\" : {}, \"md5OfBody\" : \"e4e68fb7bd0e697a0ae8f1bb342846b3\" , \"eventSource\" : \"aws:sqs\" , \"eventSourceARN\" : \"arn:aws:sqs:us-east-2: 123456789012:my-queue\" , \"awsRegion\" : \"us-east-1\" }, { \"messageId\" : \"244fc6b4-87a3-44ab-83d2-361172410c3a\" , \"receiptHandle\" : \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a\" , \"body\" : \"SGVsbG8sIHRoaXMgaXMgYSB0ZXN0Lg==\" , \"attributes\" : { \"ApproximateReceiveCount\" : \"1\" , \"SentTimestamp\" : \"1545082649183\" , \"SenderId\" : \"AIDAIENQZJOLO23YVJ4VO\" , \"ApproximateFirstReceiveTimestamp\" : \"1545082649185\" }, \"messageAttributes\" : {}, \"md5OfBody\" : \"e4e68fb7bd0e697a0ae8f1bb342846b3\" , \"eventSource\" : \"aws:sqs\" , \"eventSourceARN\" : \"arn:aws:sqs:us-east-2: 123456789012:my-queue\" , \"awsRegion\" : \"us-east-1\" } ] }","title":"Processing messages from SQS"},{"location":"utilities/batch/#processing-messages-from-kinesis","text":"Processing batches from Kinesis works in four stages: Instantiate BatchProcessor and choose EventType.KinesisDataStreams for the event type Define your function to handle each batch record, and use KinesisStreamRecord type annotation for autocompletion Use either batch_processor decorator or your instantiated processor as a context manager to kick off processing Return the appropriate response contract to Lambda via .response() processor method Info This code example optionally uses Tracer and Logger for completion. As a decorator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.data_classes.kinesis_stream_event import KinesisStreamRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . KinesisDataStreams ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : KinesisStreamRecord ): logger . info ( record . kinesis . data_as_text ) payload : dict = record . kinesis . data_as_json () ... @logger . inject_lambda_context @tracer . capture_lambda_handler @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response () As a context manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.data_classes.kinesis_stream_event import KinesisStreamRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . KinesisDataStreams ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : KinesisStreamRecord ): logger . info ( record . kinesis . data_as_text ) payload : dict = record . kinesis . data_as_json () ... @logger . inject_lambda_context @tracer . capture_lambda_handler def lambda_handler ( event , context : LambdaContext ): batch = event [ \"Records\" ] with processor ( records = batch , handler = record_handler ): processed_messages = processor . process () # kick off processing, return list[tuple] return processor . response () Sample response The second record failed to be processed, therefore the processor added its sequence number in the response. 1 2 3 4 5 6 7 { 'batchItemFailures' : [ { 'itemIdentifier' : '6006958808509702859251049540584488075644979031228738' } ] } Sample event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"Records\" : [ { \"kinesis\" : { \"kinesisSchemaVersion\" : \"1.0\" , \"partitionKey\" : \"1\" , \"sequenceNumber\" : \"4107859083838847772757075850904226111829882106684065\" , \"data\" : \"eyJNZXNzYWdlIjogInN1Y2Nlc3MifQ==\" , \"approximateArrivalTimestamp\" : 1545084650.987 }, \"eventSource\" : \"aws:kinesis\" , \"eventVersion\" : \"1.0\" , \"eventID\" : \"shardId-000000000006:4107859083838847772757075850904226111829882106684065\" , \"eventName\" : \"aws:kinesis:record\" , \"invokeIdentityArn\" : \"arn:aws:iam::123456789012:role/lambda-role\" , \"awsRegion\" : \"us-east-2\" , \"eventSourceARN\" : \"arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream\" }, { \"kinesis\" : { \"kinesisSchemaVersion\" : \"1.0\" , \"partitionKey\" : \"1\" , \"sequenceNumber\" : \"6006958808509702859251049540584488075644979031228738\" , \"data\" : \"c3VjY2Vzcw==\" , \"approximateArrivalTimestamp\" : 1545084650.987 }, \"eventSource\" : \"aws:kinesis\" , \"eventVersion\" : \"1.0\" , \"eventID\" : \"shardId-000000000006:6006958808509702859251049540584488075644979031228738\" , \"eventName\" : \"aws:kinesis:record\" , \"invokeIdentityArn\" : \"arn:aws:iam::123456789012:role/lambda-role\" , \"awsRegion\" : \"us-east-2\" , \"eventSourceARN\" : \"arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream\" } ] }","title":"Processing messages from Kinesis"},{"location":"utilities/batch/#processing-messages-from-dynamodb","text":"Processing batches from Kinesis works in four stages: Instantiate BatchProcessor and choose EventType.DynamoDBStreams for the event type Define your function to handle each batch record, and use DynamoDBRecord type annotation for autocompletion Use either batch_processor decorator or your instantiated processor as a context manager to kick off processing Return the appropriate response contract to Lambda via .response() processor method Info This code example optionally uses Tracer and Logger for completion. As a decorator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import DynamoDBRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . DynamoDBStreams ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : DynamoDBRecord ): logger . info ( record . dynamodb . new_image ) payload : dict = json . loads ( record . dynamodb . new_image . get ( \"Message\" ) . get_value ) # alternatively: # changes: Dict[str, dynamo_db_stream_event.AttributeValue] = record.dynamodb.new_image # payload = change.get(\"Message\").raw_event -> {\"S\": \"<payload>\"} ... @logger . inject_lambda_context @tracer . capture_lambda_handler @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response () As a context manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import DynamoDBRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . DynamoDBStreams ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : DynamoDBRecord ): logger . info ( record . dynamodb . new_image ) payload : dict = json . loads ( record . dynamodb . new_image . get ( \"item\" ) . s_value ) # alternatively: # changes: Dict[str, dynamo_db_stream_event.AttributeValue] = record.dynamodb.new_image # payload = change.get(\"Message\").raw_event -> {\"S\": \"<payload>\"} ... @logger . inject_lambda_context @tracer . capture_lambda_handler def lambda_handler ( event , context : LambdaContext ): batch = event [ \"Records\" ] with processor ( records = batch , handler = record_handler ): processed_messages = processor . process () # kick off processing, return list[tuple] return processor . response () Sample response The second record failed to be processed, therefore the processor added its sequence number in the response. 1 2 3 4 5 6 7 { 'batchItemFailures' : [ { 'itemIdentifier' : '8640712661' } ] } Sample event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 { \"Records\" : [ { \"eventID\" : \"1\" , \"eventVersion\" : \"1.0\" , \"dynamodb\" : { \"Keys\" : { \"Id\" : { \"N\" : \"101\" } }, \"NewImage\" : { \"Message\" : { \"S\" : \"failure\" } }, \"StreamViewType\" : \"NEW_AND_OLD_IMAGES\" , \"SequenceNumber\" : \"3275880929\" , \"SizeBytes\" : 26 }, \"awsRegion\" : \"us-west-2\" , \"eventName\" : \"INSERT\" , \"eventSourceARN\" : \"eventsource_arn\" , \"eventSource\" : \"aws:dynamodb\" }, { \"eventID\" : \"1\" , \"eventVersion\" : \"1.0\" , \"dynamodb\" : { \"Keys\" : { \"Id\" : { \"N\" : \"101\" } }, \"NewImage\" : { \"SomethingElse\" : { \"S\" : \"success\" } }, \"StreamViewType\" : \"NEW_AND_OLD_IMAGES\" , \"SequenceNumber\" : \"8640712661\" , \"SizeBytes\" : 26 }, \"awsRegion\" : \"us-west-2\" , \"eventName\" : \"INSERT\" , \"eventSourceARN\" : \"eventsource_arn\" , \"eventSource\" : \"aws:dynamodb\" } ] }","title":"Processing messages from DynamoDB"},{"location":"utilities/batch/#partial-failure-mechanics","text":"All records in the batch will be passed to this handler for processing, even if exceptions are thrown - Here's the behaviour after completing the batch: All records successfully processed . We will return an empty list of item failures {'batchItemFailures': []} Partial success with some exceptions . We will return a list of all item IDs/sequence numbers that failed processing All records failed to be processed . We will raise BatchProcessingError exception with a list of all exceptions raised when processing Warning You will not have access to the processed messages within the Lambda Handler; use context manager for that. All processing logic will and should be performed by the record_handler function.","title":"Partial failure mechanics"},{"location":"utilities/batch/#advanced","text":"","title":"Advanced"},{"location":"utilities/batch/#pydantic-integration","text":"You can bring your own Pydantic models via model parameter when inheriting from SqsRecordModel , KinesisDataStreamRecord , or DynamoDBStreamRecordModel Inheritance is importance because we need to access message IDs and sequence numbers from these records in the event of failure. Mypy is fully integrated with this utility, so it should identify whether you're passing the incorrect Model. SQS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.parser.models import SqsRecordModel from aws_lambda_powertools.utilities.typing import LambdaContext class Order ( BaseModel ): item : dict class OrderSqsRecord ( SqsRecordModel ): body : Order # auto transform json string # so Pydantic can auto-initialize nested Order model @validator ( \"body\" , pre = True ) def transform_body_to_dict ( cls , value : str ): return json . loads ( value ) processor = BatchProcessor ( event_type = EventType . SQS , model = OrderSqsRecord ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : OrderSqsRecord ): return record . body . item @logger . inject_lambda_context @tracer . capture_lambda_handler @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response () Kinesis Data Streams 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.parser.models import KinesisDataStreamRecord from aws_lambda_powertools.utilities.typing import LambdaContext class Order ( BaseModel ): item : dict class OrderKinesisPayloadRecord ( KinesisDataStreamRecordPayload ): data : Order # auto transform json string # so Pydantic can auto-initialize nested Order model @validator ( \"data\" , pre = True ) def transform_message_to_dict ( cls , value : str ): # Powertools KinesisDataStreamRecordModel already decodes b64 to str here return json . loads ( value ) class OrderKinesisRecord ( KinesisDataStreamRecordModel ): kinesis : OrderKinesisPayloadRecord processor = BatchProcessor ( event_type = EventType . KinesisDataStreams , model = OrderKinesisRecord ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : OrderKinesisRecord ): return record . kinesis . data . item @logger . inject_lambda_context @tracer . capture_lambda_handler @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response () DynamoDB Streams 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import json from typing import Dict , Literal from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.parser.models import DynamoDBStreamRecordModel from aws_lambda_powertools.utilities.typing import LambdaContext class Order ( BaseModel ): item : dict class OrderDynamoDB ( BaseModel ): Message : Order # auto transform json string # so Pydantic can auto-initialize nested Order model @validator ( \"Message\" , pre = True ) def transform_message_to_dict ( cls , value : Dict [ Literal [ \"S\" ], str ]): return json . loads ( value [ \"S\" ]) class OrderDynamoDBChangeRecord ( DynamoDBStreamChangedRecordModel ): NewImage : Optional [ OrderDynamoDB ] OldImage : Optional [ OrderDynamoDB ] class OrderDynamoDBRecord ( DynamoDBStreamRecordModel ): dynamodb : OrderDynamoDBChangeRecord processor = BatchProcessor ( event_type = EventType . DynamoDBStreams , model = OrderKinesisRecord ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : OrderDynamoDBRecord ): return record . dynamodb . NewImage . Message . item @logger . inject_lambda_context @tracer . capture_lambda_handler @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response ()","title":"Pydantic integration"},{"location":"utilities/batch/#accessing-processed-messages","text":"Use the context manager to access a list of all returned values from your record_handler function. When successful . We will include a tuple with success , the result of record_handler , and the batch record When failed . We will include a tuple with fail , exception as a string, and the batch record Accessing processed messages via context manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import json from typing import Any , List , Literal , Union from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import ( BatchProcessor , EventType , FailureResponse , SuccessResponse , batch_processor ) from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . SQS ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : SQSRecord ): payload : str = record . body if payload : item : dict = json . loads ( payload ) ... @logger . inject_lambda_context @tracer . capture_lambda_handler def lambda_handler ( event , context : LambdaContext ): batch = event [ \"Records\" ] with processor ( records = batch , handler = record_handler ): processed_messages : List [ Union [ SuccessResponse , FailureResponse ]] = processor . process () for message in processed_messages : status : Union [ Literal [ \"success\" ], Literal [ \"fail\" ]] = message [ 0 ] result : Any = message [ 1 ] record : SQSRecord = message [ 2 ] return processor . response ()","title":"Accessing processed messages"},{"location":"utilities/batch/#extending-batchprocessor","text":"You might want to bring custom logic to the existing BatchProcessor to slightly override how we handle successes and failures. For these scenarios, you can subclass BatchProcessor and quickly override success_handler and failure_handler methods: success_handler() \u2013 Keeps track of successful batch records failure_handler() \u2013 Keeps track of failed batch records Example Let's suppose you'd like to add a metric named BatchRecordFailures for each batch record that failed processing Extending failure handling mechanism in BatchProcessor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from typing import Tuple from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit from aws_lambda_powertools.utilities.batch import batch_processor , BatchProcessor , ExceptionInfo , EventType , FailureResponse from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord class MyProcessor ( BatchProcessor ): def failure_handler ( self , record : SQSRecord , exception : ExceptionInfo ) -> FailureResponse : metrics . add_metric ( name = \"BatchRecordFailures\" , unit = MetricUnit . Count , value = 1 ) return super () . failure_handler ( record , exception ) processor = MyProcessor ( event_type = EventType . SQS ) metrics = Metrics ( namespace = \"test\" ) @tracer . capture_method def record_handler ( record : SQSRecord ): payload : str = record . body if payload : item : dict = json . loads ( payload ) ... @metrics . log_metrics ( capture_cold_start_metric = True ) @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response ()","title":"Extending BatchProcessor"},{"location":"utilities/batch/#create-your-own-partial-processor","text":"You can create your own partial batch processor from scratch by inheriting the BasePartialProcessor class, and implementing _prepare() , _clean() and _process_record() . _process_record() \u2013 handles all processing logic for each individual message of a batch, including calling the record_handler (self.handler) _prepare() \u2013 called once as part of the processor initialization clean() \u2013 teardown logic called once after _process_record completes You can then use this class as a context manager, or pass it to batch_processor to use as a decorator on your Lambda handler function. Creating a custom batch processor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 from random import randint from aws_lambda_powertools.utilities.batch import BasePartialProcessor , batch_processor import boto3 import os table_name = os . getenv ( \"TABLE_NAME\" , \"table_not_found\" ) class MyPartialProcessor ( BasePartialProcessor ): \"\"\" Process a record and stores successful results at a Amazon DynamoDB Table Parameters ---------- table_name: str DynamoDB table name to write results to \"\"\" def __init__ ( self , table_name : str ): self . table_name = table_name super () . __init__ () def _prepare ( self ): # It's called once, *before* processing # Creates table resource and clean previous results self . ddb_table = boto3 . resource ( \"dynamodb\" ) . Table ( self . table_name ) self . success_messages . clear () def _clean ( self ): # It's called once, *after* closing processing all records (closing the context manager) # Here we're sending, at once, all successful messages to a ddb table with self . ddb_table . batch_writer () as batch : for result in self . success_messages : batch . put_item ( Item = result ) def _process_record ( self , record ): # It handles how your record is processed # Here we're keeping the status of each run # where self.handler is the record_handler function passed as an argument try : result = self . handler ( record ) # record_handler passed to decorator/context manager return self . success_handler ( record , result ) except Exception as exc : return self . failure_handler ( record , exc ) def success_handler ( self , record ): entry = ( \"success\" , result , record ) message = { \"age\" : result } self . success_messages . append ( message ) return entry def record_handler ( record ): return randint ( 0 , 100 ) @batch_processor ( record_handler = record_handler , processor = MyPartialProcessor ( table_name )) def lambda_handler ( event , context ): return { \"statusCode\" : 200 }","title":"Create your own partial processor"},{"location":"utilities/batch/#caveats","text":"","title":"Caveats"},{"location":"utilities/batch/#tracer-response-auto-capture-for-large-batch-sizes","text":"When using Tracer to capture responses for each batch record processing, you might exceed 64K of tracing data depending on what you return from your record_handler function, or how big is your batch size. If that's the case, you can configure Tracer to disable response auto-capturing . Disabling Tracer response auto-capturing 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . SQS ) tracer = Tracer () logger = Logger () @tracer . capture_method ( capture_response = False ) def record_handler ( record : SQSRecord ): payload : str = record . body if payload : item : dict = json . loads ( payload ) ... @logger . inject_lambda_context @tracer . capture_lambda_handler @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response ()","title":"Tracer response auto-capture for large batch sizes"},{"location":"utilities/batch/#testing-your-code","text":"As there is no external calls, you can unit test your code with BatchProcessor quite easily. Example : Given a SQS batch where the first batch record succeeds and the second fails processing, we should have a single item reported in the function response. test_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 import json from pathlib import Path from dataclasses import dataclass import pytest from src.app import lambda_handler , processor def load_event ( path : Path ): with path . open () as f : return json . load ( f ) @pytest . fixture def lambda_context (): @dataclass class LambdaContext : function_name : str = \"test\" memory_limit_in_mb : int = 128 invoked_function_arn : str = \"arn:aws:lambda:eu-west-1:809313241:function:test\" aws_request_id : str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\" return LambdaContext () @pytest . fixture () def sqs_event (): \"\"\"Generates API GW Event\"\"\" return load_event ( path = Path ( \"events/sqs_event.json\" )) def test_app_batch_partial_response ( sqs_event , lambda_context ): # GIVEN processor = app . processor # access processor for additional assertions successful_record = sqs_event [ \"Records\" ][ 0 ] failed_record = sqs_event [ \"Records\" ][ 1 ] expected_response = { \"batchItemFailures: [ { \"itemIdentifier\" : failed_record [ \"messageId\" ] } ] } # WHEN ret = app . lambda_handler ( sqs_event , lambda_context ) # THEN assert ret == expected_response assert len ( processor . fail_messages ) == 1 assert processor . success_messages [ 0 ] == successful_record src/app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import json from aws_lambda_powertools import Logger , Tracer from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord from aws_lambda_powertools.utilities.typing import LambdaContext processor = BatchProcessor ( event_type = EventType . SQS ) tracer = Tracer () logger = Logger () @tracer . capture_method def record_handler ( record : SQSRecord ): payload : str = record . body if payload : item : dict = json . loads ( payload ) ... @logger . inject_lambda_context @tracer . capture_lambda_handler @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context : LambdaContext ): return processor . response () Sample SQS event events/sqs_sample.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"Records\" : [ { \"messageId\" : \"059f36b4-87a3-44ab-83d2-661975830a7d\" , \"receiptHandle\" : \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a\" , \"body\" : \"{\\\"Message\\\": \\\"success\\\"}\" , \"attributes\" : { \"ApproximateReceiveCount\" : \"1\" , \"SentTimestamp\" : \"1545082649183\" , \"SenderId\" : \"AIDAIENQZJOLO23YVJ4VO\" , \"ApproximateFirstReceiveTimestamp\" : \"1545082649185\" }, \"messageAttributes\" : {}, \"md5OfBody\" : \"e4e68fb7bd0e697a0ae8f1bb342846b3\" , \"eventSource\" : \"aws:sqs\" , \"eventSourceARN\" : \"arn:aws:sqs:us-east-2: 123456789012:my-queue\" , \"awsRegion\" : \"us-east-1\" }, { \"messageId\" : \"244fc6b4-87a3-44ab-83d2-361172410c3a\" , \"receiptHandle\" : \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a\" , \"body\" : \"SGVsbG8sIHRoaXMgaXMgYSB0ZXN0Lg==\" , \"attributes\" : { \"ApproximateReceiveCount\" : \"1\" , \"SentTimestamp\" : \"1545082649183\" , \"SenderId\" : \"AIDAIENQZJOLO23YVJ4VO\" , \"ApproximateFirstReceiveTimestamp\" : \"1545082649185\" }, \"messageAttributes\" : {}, \"md5OfBody\" : \"e4e68fb7bd0e697a0ae8f1bb342846b3\" , \"eventSource\" : \"aws:sqs\" , \"eventSourceARN\" : \"arn:aws:sqs:us-east-2: 123456789012:my-queue\" , \"awsRegion\" : \"us-east-1\" } ] }","title":"Testing your code"},{"location":"utilities/batch/#faq","text":"","title":"FAQ"},{"location":"utilities/batch/#choosing-between-decorator-and-context-manager","text":"Use context manager when you want access to the processed messages or handle BatchProcessingError exception when all records within the batch fail to be processed.","title":"Choosing between decorator and context manager"},{"location":"utilities/batch/#integrating-exception-handling-with-sentryio","text":"When using Sentry.io for error monitoring, you can override failure_handler to capture each processing exception with Sentry SDK: Credits to Charles-Axel Dein Integrating error tracking with Sentry.io 1 2 3 4 5 6 7 8 9 10 from typing import Tuple from aws_lambda_powertools.utilities.batch import BatchProcessor , FailureResponse from sentry_sdk import capture_exception class MyProcessor ( BatchProcessor ): def failure_handler ( self , record , exception ) -> FailureResponse : capture_exception () # send exception to Sentry return super () . failure_handler ( record , exception )","title":"Integrating exception handling with Sentry.io"},{"location":"utilities/batch/#legacy","text":"Tip This is kept for historical purposes. Use the new BatchProcessor instead.","title":"Legacy"},{"location":"utilities/batch/#migration-guide","text":"Info Keep reading if you are using sqs_batch_processor or PartialSQSProcessor . As of Nov 2021 , this is no longer needed as both SQS, Kinesis, and DynamoDB Streams offer this capability natively with one caveat - it's an opt-in feature . Being a native feature, we no longer need to instantiate boto3 nor other customizations like exception suppressing \u2013 this lowers the cost of your Lambda function as you can delegate deleting partial failures to Lambda. Tip It's also easier to test since it's mostly a contract based response . You can migrate in three steps: If you are using sqs_batch_decorator you can now use batch_processor decorator If you were using PartialSQSProcessor you can now use BatchProcessor Change your Lambda Handler to return the new response format Decorator: Before 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.batch import sqs_batch_processor def record_handler ( record ): return do_something_with ( record [ \"body\" ]) @sqs_batch_processor ( record_handler = record_handler ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Decorator: After 1 2 3 4 5 6 7 8 9 10 11 12 13 import json from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor processor = BatchProcessor ( event_type = EventType . SQS ) def record_handler ( record ): return do_something_with ( record [ \"body\" ]) @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context ): return processor . response () Context manager: Before 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor from botocore.config import Config config = Config ( region_name = \"us-east-1\" ) def record_handler ( record ): return_value = do_something_with ( record [ \"body\" ]) return return_value def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = PartialSQSProcessor ( config = config ) with processor ( records , record_handler ): result = processor . process () return result Context manager: After 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from aws_lambda_powertools.utilities.batch import BatchProcessor , EventType , batch_processor def record_handler ( record ): return_value = do_something_with ( record [ \"body\" ]) return return_value def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = BatchProcessor ( event_type = EventType . SQS ) with processor ( records , record_handler ): result = processor . process () return processor . response ()","title":"Migration guide"},{"location":"utilities/batch/#customizing-boto-configuration","text":"The config and boto3_session parameters enable you to pass in a custom botocore config object or a custom boto3 session when using the sqs_batch_processor decorator or PartialSQSProcessor class. Custom config example Decorator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.batch import sqs_batch_processor from botocore.config import Config config = Config ( region_name = \"us-east-1\" ) def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value @sqs_batch_processor ( record_handler = record_handler , config = config ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Context manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor from botocore.config import Config config = Config ( region_name = \"us-east-1\" ) def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = PartialSQSProcessor ( config = config ) with processor ( records , record_handler ): result = processor . process () return result Custom boto3 session example Decorator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.batch import sqs_batch_processor from botocore.config import Config session = boto3 . session . Session () def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value @sqs_batch_processor ( record_handler = record_handler , boto3_session = session ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Context manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor import boto3 session = boto3 . session . Session () def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = PartialSQSProcessor ( boto3_session = session ) with processor ( records , record_handler ): result = processor . process () return result","title":"Customizing boto configuration"},{"location":"utilities/batch/#suppressing-exceptions","text":"If you want to disable the default behavior where SQSBatchProcessingError is raised if there are any errors, you can pass the suppress_exception boolean argument. Decorator 1 2 3 4 5 from aws_lambda_powertools.utilities.batch import sqs_batch_processor @sqs_batch_processor ( record_handler = record_handler , config = config , suppress_exception = True ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Context manager 1 2 3 4 5 6 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor processor = PartialSQSProcessor ( config = config , suppress_exception = True ) with processor ( records , record_handler ): result = processor . process ()","title":"Suppressing exceptions"},{"location":"utilities/data_classes/","text":"Event Source Data Classes utility provides classes self-describing Lambda event sources. Key Features \u00b6 Type hinting and code completion for common event types Helper functions for decoding/deserializing nested fields Docstrings for fields contained in event schemas Background When authoring Lambda functions, you often need to understand the schema of the event dictionary which is passed to the handler. There are several common event types which follow a specific schema, depending on the service triggering the Lambda function. Getting started \u00b6 Utilizing the data classes \u00b6 The classes are initialized by passing in the Lambda event object into the constructor of the appropriate data class or by using the event_source decorator. For example, if your Lambda function is being triggered by an API Gateway proxy integration, you can use the APIGatewayProxyEvent class. app.py 1 2 3 4 5 6 from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent def lambda_handler ( event : dict , context ): event = APIGatewayProxyEvent ( event ) if 'helloworld' in event . path and event . http_method == 'GET' : do_something_with ( event . body , user ) Same example as above, but using the event_source decorator app.py 1 2 3 4 5 6 from aws_lambda_powertools.utilities.data_classes import event_source , APIGatewayProxyEvent @event_source ( data_class = APIGatewayProxyEvent ) def lambda_handler ( event : APIGatewayProxyEvent , context ): if 'helloworld' in event . path and event . http_method == 'GET' : do_something_with ( event . body , user ) Autocomplete with self-documented properties and methods Supported event sources \u00b6 Event Source Data_class Active MQ ActiveMQEvent API Gateway Authorizer APIGatewayAuthorizerRequestEvent API Gateway Authorizer V2 APIGatewayAuthorizerEventV2 API Gateway Proxy APIGatewayProxyEvent API Gateway Proxy V2 APIGatewayProxyEventV2 Application Load Balancer ALBEvent AppSync Authorizer AppSyncAuthorizerEvent AppSync Resolver AppSyncResolverEvent CloudWatch Logs CloudWatchLogsEvent CodePipeline Job Event CodePipelineJobEvent Cognito User Pool Multiple available under cognito_user_pool_event Connect Contact Flow ConnectContactFlowEvent DynamoDB streams DynamoDBStreamEvent , DynamoDBRecordEventName EventBridge EventBridgeEvent Kinesis Data Stream KinesisStreamEvent Rabbit MQ RabbitMQEvent S3 S3Event S3 Object Lambda S3ObjectLambdaEvent SES SESEvent SNS SNSEvent SQS SQSEvent Info The examples provided below are far from exhaustive - the data classes themselves are designed to provide a form of documentation inherently (via autocompletion, types and docstrings). Active MQ \u00b6 It is used for Active MQ payloads , also see the AWS blog post for more details. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from typing import Dict from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.data_classes import event_source from aws_lambda_powertools.utilities.data_classes.active_mq_event import ActiveMQEvent logger = Logger () @event_source ( data_class = ActiveMQEvent ) def lambda_handler ( event : ActiveMQEvent , context ): for message in event . messages : logger . debug ( f \"MessageID: { message . message_id } \" ) data : Dict = message . json_data logger . debug ( \"Process json in base64 encoded data str\" , data ) API Gateway Authorizer \u00b6 New in 1.20.0 It is used for API Gateway Rest API Lambda Authorizer payload . Use APIGatewayAuthorizerRequestEvent for type REQUEST and APIGatewayAuthorizerTokenEvent for type TOKEN . app_type_request.py This example uses the APIGatewayAuthorizerResponse to decline a given request if the user is not found. When the user is found, it includes the user details in the request context that will be available to the back-end, and returns a full access policy for admin users. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 from aws_lambda_powertools.utilities.data_classes import event_source from aws_lambda_powertools.utilities.data_classes.api_gateway_authorizer_event import ( DENY_ALL_RESPONSE , APIGatewayAuthorizerRequestEvent , APIGatewayAuthorizerResponse , HttpVerb , ) from secrets import compare_digest def get_user_by_token ( token ): if compare_digest ( token , \"admin-foo\" ): return { \"id\" : 0 , \"name\" : \"Admin\" , \"isAdmin\" : True } elif compare_digest ( token , \"regular-foo\" ): return { \"id\" : 1 , \"name\" : \"Joe\" } else : return None @event_source ( data_class = APIGatewayAuthorizerRequestEvent ) def handler ( event : APIGatewayAuthorizerRequestEvent , context ): user = get_user_by_token ( event . get_header_value ( \"Authorization\" )) if user is None : # No user was found # to return 401 - `{\"message\":\"Unauthorized\"}`, but pollutes lambda error count metrics # raise Exception(\"Unauthorized\") # to return 403 - `{\"message\":\"Forbidden\"}` return DENY_ALL_RESPONSE # parse the `methodArn` as an `APIGatewayRouteArn` arn = event . parsed_arn # Create the response builder from parts of the `methodArn` # and set the logged in user id and context policy = APIGatewayAuthorizerResponse ( principal_id = user [ \"id\" ], context = user , region = arn . region , aws_account_id = arn . aws_account_id , api_id = arn . api_id , stage = arn . stage , ) # Conditional IAM Policy if user . get ( \"isAdmin\" , False ): policy . allow_all_routes () else : policy . allow_route ( HttpVerb . GET , \"/user-profile\" ) return policy . asdict () app_type_token.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from aws_lambda_powertools.utilities.data_classes import event_source from aws_lambda_powertools.utilities.data_classes.api_gateway_authorizer_event import ( APIGatewayAuthorizerTokenEvent , APIGatewayAuthorizerResponse , ) @event_source ( data_class = APIGatewayAuthorizerTokenEvent ) def handler ( event : APIGatewayAuthorizerTokenEvent , context ): arn = event . parsed_arn policy = APIGatewayAuthorizerResponse ( principal_id = \"user\" , region = arn . region , aws_account_id = arn . aws_account_id , api_id = arn . api_id , stage = arn . stage ) if event . authorization_token == \"42\" : policy . allow_all_routes () else : policy . deny_all_routes () return policy . asdict () API Gateway Authorizer V2 \u00b6 New in 1.20.0 It is used for API Gateway HTTP API Lambda Authorizer payload version 2 . See also this blog post for more details. app.py This example looks up user details via x-token header. It uses APIGatewayAuthorizerResponseV2 to return a deny policy when user is not found or authorized. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from aws_lambda_powertools.utilities.data_classes import event_source from aws_lambda_powertools.utilities.data_classes.api_gateway_authorizer_event import ( APIGatewayAuthorizerEventV2 , APIGatewayAuthorizerResponseV2 , ) from secrets import compare_digest def get_user_by_token ( token ): if compare_digest ( token , \"Foo\" ): return { \"name\" : \"Foo\" } return None @event_source ( data_class = APIGatewayAuthorizerEventV2 ) def handler ( event : APIGatewayAuthorizerEventV2 , context ): user = get_user_by_token ( event . get_header_value ( \"x-token\" )) if user is None : # No user was found, so we return not authorized return APIGatewayAuthorizerResponseV2 () . asdict () # Found the user and setting the details in the context return APIGatewayAuthorizerResponseV2 ( authorize = True , context = user ) . asdict () API Gateway Proxy \u00b6 It is used for either API Gateway REST API or HTTP API using v1 proxy event. app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.data_classes import event_source , APIGatewayProxyEvent @event_source ( data_class = APIGatewayProxyEvent ) def lambda_handler ( event : APIGatewayProxyEvent , context ): if \"helloworld\" in event . path and event . http_method == \"GET\" : request_context = event . request_context identity = request_context . identity user = identity . user do_something_with ( event . json_body , user ) API Gateway Proxy V2 \u00b6 It is used for HTTP API using v2 proxy event. app.py 1 2 3 4 5 6 from aws_lambda_powertools.utilities.data_classes import event_source , APIGatewayProxyEventV2 @event_source ( data_class = APIGatewayProxyEventV2 ) def lambda_handler ( event : APIGatewayProxyEventV2 , context ): if \"helloworld\" in event . path and event . http_method == \"POST\" : do_something_with ( event . json_body , event . query_string_parameters ) Application Load Balancer \u00b6 Is it used for Application load balancer event. app.py 1 2 3 4 5 6 from aws_lambda_powertools.utilities.data_classes import event_source , ALBEvent @event_source ( data_class = ALBEvent ) def lambda_handler ( event : ALBEvent , context ): if \"helloworld\" in event . path and event . http_method == \"POST\" : do_something_with ( event . json_body , event . query_string_parameters ) AppSync Authorizer \u00b6 New in 1.20.0 Used when building an AWS_LAMBDA Authorization with AppSync. See blog post Introducing Lambda authorization for AWS AppSync GraphQL APIs or read the Amplify documentation on using AWS Lambda for authorization with AppSync. In this example extract the requestId as the correlation_id for logging, used @event_source decorator and builds the AppSync authorizer using the AppSyncAuthorizerResponse helper. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from typing import Dict from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.logging.logger import Logger from aws_lambda_powertools.utilities.data_classes.appsync_authorizer_event import ( AppSyncAuthorizerEvent , AppSyncAuthorizerResponse , ) from aws_lambda_powertools.utilities.data_classes.event_source import event_source logger = Logger () def get_user_by_token ( token : str ): \"\"\"Look a user by token\"\"\" ... @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPSYNC_AUTHORIZER ) @event_source ( data_class = AppSyncAuthorizerEvent ) def lambda_handler ( event : AppSyncAuthorizerEvent , context ) -> Dict : user = get_user_by_token ( event . authorization_token ) if not user : # No user found, return not authorized return AppSyncAuthorizerResponse () . asdict () return AppSyncAuthorizerResponse ( authorize = True , resolver_context = { \"id\" : user . id }, # Only allow admins to delete events deny_fields = None if user . is_admin else [ \"Mutation.deleteEvent\" ], ) . asdict () AppSync Resolver \u00b6 New in 1.12.0 Used when building Lambda GraphQL Resolvers with Amplify GraphQL Transform Library ( @function ), and AppSync Direct Lambda Resolvers . In this example, we also use the new Logger correlation_id and built-in correlation_paths to extract, if available, X-Ray Trace ID in AppSync request headers: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from aws_lambda_powertools.logging import Logger , correlation_paths from aws_lambda_powertools.utilities.data_classes.appsync_resolver_event import ( AppSyncResolverEvent , AppSyncIdentityCognito ) logger = Logger () def get_locations ( name : str = None , size : int = 0 , page : int = 0 ): \"\"\"Your resolver logic here\"\"\" @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPSYNC_RESOLVER ) def lambda_handler ( event , context ): event : AppSyncResolverEvent = AppSyncResolverEvent ( event ) # Case insensitive look up of request headers x_forwarded_for = event . get_header_value ( \"x-forwarded-for\" ) # Support for AppSyncIdentityCognito or AppSyncIdentityIAM identity types assert isinstance ( event . identity , AppSyncIdentityCognito ) identity : AppSyncIdentityCognito = event . identity # Logging with correlation_id logger . debug ({ \"x-forwarded-for\" : x_forwarded_for , \"username\" : identity . username }) if event . type_name == \"Merchant\" and event . field_name == \"locations\" : return get_locations ( ** event . arguments ) raise ValueError ( f \"Unsupported field resolver: { event . field_name } \" ) Example AppSync Event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \"typeName\" : \"Merchant\" , \"fieldName\" : \"locations\" , \"arguments\" : { \"page\" : 2 , \"size\" : 1 , \"name\" : \"value\" }, \"identity\" : { \"claims\" : { \"iat\" : 1615366261 ... }, \"username\" : \"mike\" , ... }, \"request\" : { \"headers\" : { \"x-amzn-trace-id\" : \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\" , \"x-forwarded-for\" : \"127.0.0.1\" ... } }, ... } Example CloudWatch Log 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"level\" : \"DEBUG\" , \"location\" : \"lambda_handler:22\" , \"message\" :{ \"x-forwarded-for\" : \"127.0.0.1\" , \"username\" : \"mike\" }, \"timestamp\" : \"2021-03-10 12:38:40,062\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 , \"cold_start\" : true , \"function_name\" : \"func_name\" , \"function_memory_size\" : 512 , \"function_arn\" : \"func_arn\" , \"function_request_id\" : \"6735a29c-c000-4ae3-94e6-1f1c934f7f94\" , \"correlation_id\" : \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\" } CloudWatch Logs \u00b6 CloudWatch Logs events by default are compressed and base64 encoded. You can use the helper function provided to decode, decompress and parse json data from the event. app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.data_classes import event_source , CloudWatchLogsEvent from aws_lambda_powertools.utilities.data_classes.cloud_watch_logs_event import CloudWatchLogsDecodedData @event_source ( data_class = CloudWatchLogsEvent ) def lambda_handler ( event : CloudWatchLogsEvent , context ): decompressed_log : CloudWatchLogsDecodedData = event . parse_logs_data log_events = decompressed_log . log_events for event in log_events : do_something_with ( event . timestamp , event . message ) CodePipeline Job \u00b6 Data classes and utility functions to help create continuous delivery pipelines tasks with AWS Lambda app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.data_classes import event_source , CodePipelineJobEvent logger = Logger () @event_source ( data_class = CodePipelineJobEvent ) def lambda_handler ( event , context ): \"\"\"The Lambda function handler If a continuing job then checks the CloudFormation stack status and updates the job accordingly. If a new job then kick of an update or creation of the target CloudFormation stack. \"\"\" # Extract the Job ID job_id = event . get_id # Extract the params params : dict = event . decoded_user_parameters stack = params [ \"stack\" ] artifact_name = params [ \"artifact\" ] template_file = params [ \"file\" ] try : if event . data . continuation_token : # If we're continuing then the create/update has already been triggered # we just need to check if it has finished. check_stack_update_status ( job_id , stack ) else : template = event . get_artifact ( artifact_name , template_file ) # Kick off a stack update or create start_update_or_create ( job_id , stack , template ) except Exception as e : # If any other exceptions which we didn't expect are raised # then fail the job and log the exception message. logger . exception ( \"Function failed due to exception.\" ) put_job_failure ( job_id , \"Function exception: \" + str ( e )) logger . debug ( \"Function complete.\" ) return \"Complete.\" Cognito User Pool \u00b6 Cognito User Pools have several different Lambda trigger sources , all of which map to a different data class, which can be imported from aws_lambda_powertools.data_classes.cognito_user_pool_event : Trigger/Event Source Data Class Custom message event data_classes.cognito_user_pool_event.CustomMessageTriggerEvent Post authentication data_classes.cognito_user_pool_event.PostAuthenticationTriggerEvent Post confirmation data_classes.cognito_user_pool_event.PostConfirmationTriggerEvent Pre authentication data_classes.cognito_user_pool_event.PreAuthenticationTriggerEvent Pre sign-up data_classes.cognito_user_pool_event.PreSignUpTriggerEvent Pre token generation data_classes.cognito_user_pool_event.PreTokenGenerationTriggerEvent User migration data_classes.cognito_user_pool_event.UserMigrationTriggerEvent Define Auth Challenge data_classes.cognito_user_pool_event.DefineAuthChallengeTriggerEvent Create Auth Challenge data_classes.cognito_user_pool_event.CreateAuthChallengeTriggerEvent Verify Auth Challenge data_classes.cognito_user_pool_event.VerifyAuthChallengeResponseTriggerEvent Post Confirmation Example \u00b6 app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import PostConfirmationTriggerEvent def lambda_handler ( event , context ): event : PostConfirmationTriggerEvent = PostConfirmationTriggerEvent ( event ) user_attributes = event . request . user_attributes do_something_with ( user_attributes ) Define Auth Challenge Example \u00b6 Note In this example we are modifying the wrapped dict response fields, so we need to return the json serializable wrapped event in event.raw_event . This example is based on the AWS Cognito docs for Define Auth Challenge Lambda Trigger . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import DefineAuthChallengeTriggerEvent def handler ( event : dict , context ) -> dict : event : DefineAuthChallengeTriggerEvent = DefineAuthChallengeTriggerEvent ( event ) if ( len ( event . request . session ) == 1 and event . request . session [ 0 ] . challenge_name == \"SRP_A\" ): event . response . issue_tokens = False event . response . fail_authentication = False event . response . challenge_name = \"PASSWORD_VERIFIER\" elif ( len ( event . request . session ) == 2 and event . request . session [ 1 ] . challenge_name == \"PASSWORD_VERIFIER\" and event . request . session [ 1 ] . challenge_result ): event . response . issue_tokens = False event . response . fail_authentication = False event . response . challenge_name = \"CUSTOM_CHALLENGE\" elif ( len ( event . request . session ) == 3 and event . request . session [ 2 ] . challenge_name == \"CUSTOM_CHALLENGE\" and event . request . session [ 2 ] . challenge_result ): event . response . issue_tokens = True event . response . fail_authentication = False else : event . response . issue_tokens = False event . response . fail_authentication = True return event . raw_event SPR_A response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { \"version\" : \"1\" , \"region\" : \"us-east-1\" , \"userPoolId\" : \"us-east-1_example\" , \"userName\" : \"UserName\" , \"callerContext\" : { \"awsSdkVersion\" : \"awsSdkVersion\" , \"clientId\" : \"clientId\" }, \"triggerSource\" : \"DefineAuthChallenge_Authentication\" , \"request\" : { \"userAttributes\" : { \"sub\" : \"4A709A36-7D63-4785-829D-4198EF10EBDA\" , \"email_verified\" : \"true\" , \"name\" : \"First Last\" , \"email\" : \"define-auth@mail.com\" }, \"session\" : [ { \"challengeName\" : \"SRP_A\" , \"challengeResult\" : true } ] }, \"response\" : { \"issueTokens\" : false , \"failAuthentication\" : false , \"challengeName\" : \"PASSWORD_VERIFIER\" } } PASSWORD_VERIFIER success response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 { \"version\" : \"1\" , \"region\" : \"us-east-1\" , \"userPoolId\" : \"us-east-1_example\" , \"userName\" : \"UserName\" , \"callerContext\" : { \"awsSdkVersion\" : \"awsSdkVersion\" , \"clientId\" : \"clientId\" }, \"triggerSource\" : \"DefineAuthChallenge_Authentication\" , \"request\" : { \"userAttributes\" : { \"sub\" : \"4A709A36-7D63-4785-829D-4198EF10EBDA\" , \"email_verified\" : \"true\" , \"name\" : \"First Last\" , \"email\" : \"define-auth@mail.com\" }, \"session\" : [ { \"challengeName\" : \"SRP_A\" , \"challengeResult\" : true }, { \"challengeName\" : \"PASSWORD_VERIFIER\" , \"challengeResult\" : true } ] }, \"response\" : { \"issueTokens\" : false , \"failAuthentication\" : false , \"challengeName\" : \"CUSTOM_CHALLENGE\" } } CUSTOM_CHALLENGE success response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { \"version\" : \"1\" , \"region\" : \"us-east-1\" , \"userPoolId\" : \"us-east-1_example\" , \"userName\" : \"UserName\" , \"callerContext\" : { \"awsSdkVersion\" : \"awsSdkVersion\" , \"clientId\" : \"clientId\" }, \"triggerSource\" : \"DefineAuthChallenge_Authentication\" , \"request\" : { \"userAttributes\" : { \"sub\" : \"4A709A36-7D63-4785-829D-4198EF10EBDA\" , \"email_verified\" : \"true\" , \"name\" : \"First Last\" , \"email\" : \"define-auth@mail.com\" }, \"session\" : [ { \"challengeName\" : \"SRP_A\" , \"challengeResult\" : true }, { \"challengeName\" : \"PASSWORD_VERIFIER\" , \"challengeResult\" : true }, { \"challengeName\" : \"CUSTOM_CHALLENGE\" , \"challengeResult\" : true } ] }, \"response\" : { \"issueTokens\" : true , \"failAuthentication\" : false } } Create Auth Challenge Example \u00b6 This example is based on the AWS Cognito docs for Create Auth Challenge Lambda Trigger . app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.data_classes import event_source from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import CreateAuthChallengeTriggerEvent @event_source ( data_class = CreateAuthChallengeTriggerEvent ) def handler ( event : CreateAuthChallengeTriggerEvent , context ) -> dict : if event . request . challenge_name == \"CUSTOM_CHALLENGE\" : event . response . public_challenge_parameters = { \"captchaUrl\" : \"url/123.jpg\" } event . response . private_challenge_parameters = { \"answer\" : \"5\" } event . response . challenge_metadata = \"CAPTCHA_CHALLENGE\" return event . raw_event Verify Auth Challenge Response Example \u00b6 This example is based on the AWS Cognito docs for Verify Auth Challenge Response Lambda Trigger . app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.data_classes import event_source from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import VerifyAuthChallengeResponseTriggerEvent @event_source ( data_class = VerifyAuthChallengeResponseTriggerEvent ) def handler ( event : VerifyAuthChallengeResponseTriggerEvent , context ) -> dict : event . response . answer_correct = ( event . request . private_challenge_parameters . get ( \"answer\" ) == event . request . challenge_answer ) return event . raw_event Connect Contact Flow \u00b6 New in 1.11.0 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.data_classes.connect_contact_flow_event import ( ConnectContactFlowChannel , ConnectContactFlowEndpointType , ConnectContactFlowEvent , ConnectContactFlowInitiationMethod , ) def lambda_handler ( event , context ): event : ConnectContactFlowEvent = ConnectContactFlowEvent ( event ) assert event . contact_data . attributes == { \"Language\" : \"en-US\" } assert event . contact_data . channel == ConnectContactFlowChannel . VOICE assert event . contact_data . customer_endpoint . endpoint_type == ConnectContactFlowEndpointType . TELEPHONE_NUMBER assert event . contact_data . initiation_method == ConnectContactFlowInitiationMethod . API DynamoDB Streams \u00b6 The DynamoDB data class utility provides the base class for DynamoDBStreamEvent , a typed class for attributes values ( AttributeValue ), as well as enums for stream view type ( StreamViewType ) and event type ( DynamoDBRecordEventName ). app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import ( DynamoDBStreamEvent , DynamoDBRecordEventName ) def lambda_handler ( event , context ): event : DynamoDBStreamEvent = DynamoDBStreamEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : if record . event_name == DynamoDBRecordEventName . MODIFY : do_something_with ( record . dynamodb . new_image ) do_something_with ( record . dynamodb . old_image ) multiple_records_types.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from aws_lambda_powertools.utilities.data_classes import event_source , DynamoDBStreamEvent from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import AttributeValueType , AttributeValue from aws_lambda_powertools.utilities.typing import LambdaContext @event_source ( data_class = DynamoDBStreamEvent ) def lambda_handler ( event : DynamoDBStreamEvent , context : LambdaContext ): for record in event . records : key : AttributeValue = record . dynamodb . keys [ \"id\" ] if key == AttributeValueType . Number : # {\"N\": \"123.45\"} => \"123.45\" assert key . get_value == key . n_value print ( key . get_value ) elif key == AttributeValueType . Map : assert key . get_value == key . map_value print ( key . get_value ) EventBridge \u00b6 app.py 1 2 3 4 5 from aws_lambda_powertools.utilities.data_classes import event_source , EventBridgeEvent @event_source ( data_class = EventBridgeEvent ) def lambda_handler ( event : EventBridgeEvent , context ): do_something_with ( event . detail ) Kinesis streams \u00b6 Kinesis events by default contain base64 encoded data. You can use the helper function to access the data either as json or plain text, depending on the original payload. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.data_classes import event_source , KinesisStreamEvent @event_source ( data_class = KinesisStreamEvent ) def lambda_handler ( event : KinesisStreamEvent , context ): kinesis_record = next ( event . records ) . kinesis # if data was delivered as text data = kinesis_record . data_as_text () # if data was delivered as json data = kinesis_record . data_as_json () do_something_with ( data ) Rabbit MQ \u00b6 It is used for Rabbit MQ payloads , also see the blog post for more details. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from typing import Dict from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.data_classes import event_source from aws_lambda_powertools.utilities.data_classes.rabbit_mq_event import RabbitMQEvent logger = Logger () @event_source ( data_class = RabbitMQEvent ) def lambda_handler ( event : RabbitMQEvent , context ): for queue_name , messages in event . rmq_messages_by_queue . items (): logger . debug ( f \"Messages for queue: { queue_name } \" ) for message in messages : logger . debug ( f \"MessageID: { message . basic_properties . message_id } \" ) data : Dict = message . json_data logger . debug ( \"Process json in base64 encoded data str\" , data ) S3 \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 from urllib.parse import unquote_plus from aws_lambda_powertools.utilities.data_classes import event_source , S3Event @event_source ( data_class = S3Event ) def lambda_handler ( event : S3Event , context ): bucket_name = event . bucket_name # Multiple records can be delivered in a single event for record in event . records : object_key = unquote_plus ( record . s3 . get_object . key ) do_something_with ( f \" { bucket_name } / { object_key } \" ) S3 Object Lambda \u00b6 This example is based on the AWS Blog post Introducing Amazon S3 Object Lambda \u2013 Use Your Code to Process Data as It Is Being Retrieved from S3 . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import boto3 import requests from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.correlation_paths import S3_OBJECT_LAMBDA from aws_lambda_powertools.utilities.data_classes.s3_object_event import S3ObjectLambdaEvent logger = Logger () session = boto3 . Session () s3 = session . client ( \"s3\" ) @logger . inject_lambda_context ( correlation_id_path = S3_OBJECT_LAMBDA , log_event = True ) def lambda_handler ( event , context ): event = S3ObjectLambdaEvent ( event ) # Get object from S3 response = requests . get ( event . input_s3_url ) original_object = response . content . decode ( \"utf-8\" ) # Make changes to the object about to be returned transformed_object = original_object . upper () # Write object back to S3 Object Lambda s3 . write_get_object_response ( Body = transformed_object , RequestRoute = event . request_route , RequestToken = event . request_token ) return { \"status_code\" : 200 } SES \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.data_classes import event_source , SESEvent @event_source ( data_class = SESEvent ) def lambda_handler ( event : SESEvent , context ): # Multiple records can be delivered in a single event for record in event . records : mail = record . ses . mail common_headers = mail . common_headers do_something_with ( common_headers . to , common_headers . subject ) SNS \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.data_classes import event_source , SNSEvent @event_source ( data_class = SNSEvent ) def lambda_handler ( event : SNSEvent , context ): # Multiple records can be delivered in a single event for record in event . records : message = record . sns . message subject = record . sns . subject do_something_with ( subject , message ) SQS \u00b6 app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities.data_classes import event_source , SQSEvent @event_source ( data_class = SQSEvent ) def lambda_handler ( event : SQSEvent , context ): # Multiple records can be delivered in a single event for record in event . records : do_something_with ( record . body )","title":"Event Source Data Classes"},{"location":"utilities/data_classes/#key-features","text":"Type hinting and code completion for common event types Helper functions for decoding/deserializing nested fields Docstrings for fields contained in event schemas Background When authoring Lambda functions, you often need to understand the schema of the event dictionary which is passed to the handler. There are several common event types which follow a specific schema, depending on the service triggering the Lambda function.","title":"Key Features"},{"location":"utilities/data_classes/#getting-started","text":"","title":"Getting started"},{"location":"utilities/data_classes/#utilizing-the-data-classes","text":"The classes are initialized by passing in the Lambda event object into the constructor of the appropriate data class or by using the event_source decorator. For example, if your Lambda function is being triggered by an API Gateway proxy integration, you can use the APIGatewayProxyEvent class. app.py 1 2 3 4 5 6 from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent def lambda_handler ( event : dict , context ): event = APIGatewayProxyEvent ( event ) if 'helloworld' in event . path and event . http_method == 'GET' : do_something_with ( event . body , user ) Same example as above, but using the event_source decorator app.py 1 2 3 4 5 6 from aws_lambda_powertools.utilities.data_classes import event_source , APIGatewayProxyEvent @event_source ( data_class = APIGatewayProxyEvent ) def lambda_handler ( event : APIGatewayProxyEvent , context ): if 'helloworld' in event . path and event . http_method == 'GET' : do_something_with ( event . body , user ) Autocomplete with self-documented properties and methods","title":"Utilizing the data classes"},{"location":"utilities/data_classes/#supported-event-sources","text":"Event Source Data_class Active MQ ActiveMQEvent API Gateway Authorizer APIGatewayAuthorizerRequestEvent API Gateway Authorizer V2 APIGatewayAuthorizerEventV2 API Gateway Proxy APIGatewayProxyEvent API Gateway Proxy V2 APIGatewayProxyEventV2 Application Load Balancer ALBEvent AppSync Authorizer AppSyncAuthorizerEvent AppSync Resolver AppSyncResolverEvent CloudWatch Logs CloudWatchLogsEvent CodePipeline Job Event CodePipelineJobEvent Cognito User Pool Multiple available under cognito_user_pool_event Connect Contact Flow ConnectContactFlowEvent DynamoDB streams DynamoDBStreamEvent , DynamoDBRecordEventName EventBridge EventBridgeEvent Kinesis Data Stream KinesisStreamEvent Rabbit MQ RabbitMQEvent S3 S3Event S3 Object Lambda S3ObjectLambdaEvent SES SESEvent SNS SNSEvent SQS SQSEvent Info The examples provided below are far from exhaustive - the data classes themselves are designed to provide a form of documentation inherently (via autocompletion, types and docstrings).","title":"Supported event sources"},{"location":"utilities/data_classes/#active-mq","text":"It is used for Active MQ payloads , also see the AWS blog post for more details. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from typing import Dict from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.data_classes import event_source from aws_lambda_powertools.utilities.data_classes.active_mq_event import ActiveMQEvent logger = Logger () @event_source ( data_class = ActiveMQEvent ) def lambda_handler ( event : ActiveMQEvent , context ): for message in event . messages : logger . debug ( f \"MessageID: { message . message_id } \" ) data : Dict = message . json_data logger . debug ( \"Process json in base64 encoded data str\" , data )","title":"Active MQ"},{"location":"utilities/data_classes/#api-gateway-authorizer","text":"New in 1.20.0 It is used for API Gateway Rest API Lambda Authorizer payload . Use APIGatewayAuthorizerRequestEvent for type REQUEST and APIGatewayAuthorizerTokenEvent for type TOKEN . app_type_request.py This example uses the APIGatewayAuthorizerResponse to decline a given request if the user is not found. When the user is found, it includes the user details in the request context that will be available to the back-end, and returns a full access policy for admin users. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 from aws_lambda_powertools.utilities.data_classes import event_source from aws_lambda_powertools.utilities.data_classes.api_gateway_authorizer_event import ( DENY_ALL_RESPONSE , APIGatewayAuthorizerRequestEvent , APIGatewayAuthorizerResponse , HttpVerb , ) from secrets import compare_digest def get_user_by_token ( token ): if compare_digest ( token , \"admin-foo\" ): return { \"id\" : 0 , \"name\" : \"Admin\" , \"isAdmin\" : True } elif compare_digest ( token , \"regular-foo\" ): return { \"id\" : 1 , \"name\" : \"Joe\" } else : return None @event_source ( data_class = APIGatewayAuthorizerRequestEvent ) def handler ( event : APIGatewayAuthorizerRequestEvent , context ): user = get_user_by_token ( event . get_header_value ( \"Authorization\" )) if user is None : # No user was found # to return 401 - `{\"message\":\"Unauthorized\"}`, but pollutes lambda error count metrics # raise Exception(\"Unauthorized\") # to return 403 - `{\"message\":\"Forbidden\"}` return DENY_ALL_RESPONSE # parse the `methodArn` as an `APIGatewayRouteArn` arn = event . parsed_arn # Create the response builder from parts of the `methodArn` # and set the logged in user id and context policy = APIGatewayAuthorizerResponse ( principal_id = user [ \"id\" ], context = user , region = arn . region , aws_account_id = arn . aws_account_id , api_id = arn . api_id , stage = arn . stage , ) # Conditional IAM Policy if user . get ( \"isAdmin\" , False ): policy . allow_all_routes () else : policy . allow_route ( HttpVerb . GET , \"/user-profile\" ) return policy . asdict () app_type_token.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from aws_lambda_powertools.utilities.data_classes import event_source from aws_lambda_powertools.utilities.data_classes.api_gateway_authorizer_event import ( APIGatewayAuthorizerTokenEvent , APIGatewayAuthorizerResponse , ) @event_source ( data_class = APIGatewayAuthorizerTokenEvent ) def handler ( event : APIGatewayAuthorizerTokenEvent , context ): arn = event . parsed_arn policy = APIGatewayAuthorizerResponse ( principal_id = \"user\" , region = arn . region , aws_account_id = arn . aws_account_id , api_id = arn . api_id , stage = arn . stage ) if event . authorization_token == \"42\" : policy . allow_all_routes () else : policy . deny_all_routes () return policy . asdict ()","title":"API Gateway Authorizer"},{"location":"utilities/data_classes/#api-gateway-authorizer-v2","text":"New in 1.20.0 It is used for API Gateway HTTP API Lambda Authorizer payload version 2 . See also this blog post for more details. app.py This example looks up user details via x-token header. It uses APIGatewayAuthorizerResponseV2 to return a deny policy when user is not found or authorized. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from aws_lambda_powertools.utilities.data_classes import event_source from aws_lambda_powertools.utilities.data_classes.api_gateway_authorizer_event import ( APIGatewayAuthorizerEventV2 , APIGatewayAuthorizerResponseV2 , ) from secrets import compare_digest def get_user_by_token ( token ): if compare_digest ( token , \"Foo\" ): return { \"name\" : \"Foo\" } return None @event_source ( data_class = APIGatewayAuthorizerEventV2 ) def handler ( event : APIGatewayAuthorizerEventV2 , context ): user = get_user_by_token ( event . get_header_value ( \"x-token\" )) if user is None : # No user was found, so we return not authorized return APIGatewayAuthorizerResponseV2 () . asdict () # Found the user and setting the details in the context return APIGatewayAuthorizerResponseV2 ( authorize = True , context = user ) . asdict ()","title":"API Gateway Authorizer V2"},{"location":"utilities/data_classes/#api-gateway-proxy","text":"It is used for either API Gateway REST API or HTTP API using v1 proxy event. app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.data_classes import event_source , APIGatewayProxyEvent @event_source ( data_class = APIGatewayProxyEvent ) def lambda_handler ( event : APIGatewayProxyEvent , context ): if \"helloworld\" in event . path and event . http_method == \"GET\" : request_context = event . request_context identity = request_context . identity user = identity . user do_something_with ( event . json_body , user )","title":"API Gateway Proxy"},{"location":"utilities/data_classes/#api-gateway-proxy-v2","text":"It is used for HTTP API using v2 proxy event. app.py 1 2 3 4 5 6 from aws_lambda_powertools.utilities.data_classes import event_source , APIGatewayProxyEventV2 @event_source ( data_class = APIGatewayProxyEventV2 ) def lambda_handler ( event : APIGatewayProxyEventV2 , context ): if \"helloworld\" in event . path and event . http_method == \"POST\" : do_something_with ( event . json_body , event . query_string_parameters )","title":"API Gateway Proxy V2"},{"location":"utilities/data_classes/#application-load-balancer","text":"Is it used for Application load balancer event. app.py 1 2 3 4 5 6 from aws_lambda_powertools.utilities.data_classes import event_source , ALBEvent @event_source ( data_class = ALBEvent ) def lambda_handler ( event : ALBEvent , context ): if \"helloworld\" in event . path and event . http_method == \"POST\" : do_something_with ( event . json_body , event . query_string_parameters )","title":"Application Load Balancer"},{"location":"utilities/data_classes/#appsync-authorizer","text":"New in 1.20.0 Used when building an AWS_LAMBDA Authorization with AppSync. See blog post Introducing Lambda authorization for AWS AppSync GraphQL APIs or read the Amplify documentation on using AWS Lambda for authorization with AppSync. In this example extract the requestId as the correlation_id for logging, used @event_source decorator and builds the AppSync authorizer using the AppSyncAuthorizerResponse helper. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from typing import Dict from aws_lambda_powertools.logging import correlation_paths from aws_lambda_powertools.logging.logger import Logger from aws_lambda_powertools.utilities.data_classes.appsync_authorizer_event import ( AppSyncAuthorizerEvent , AppSyncAuthorizerResponse , ) from aws_lambda_powertools.utilities.data_classes.event_source import event_source logger = Logger () def get_user_by_token ( token : str ): \"\"\"Look a user by token\"\"\" ... @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPSYNC_AUTHORIZER ) @event_source ( data_class = AppSyncAuthorizerEvent ) def lambda_handler ( event : AppSyncAuthorizerEvent , context ) -> Dict : user = get_user_by_token ( event . authorization_token ) if not user : # No user found, return not authorized return AppSyncAuthorizerResponse () . asdict () return AppSyncAuthorizerResponse ( authorize = True , resolver_context = { \"id\" : user . id }, # Only allow admins to delete events deny_fields = None if user . is_admin else [ \"Mutation.deleteEvent\" ], ) . asdict ()","title":"AppSync Authorizer"},{"location":"utilities/data_classes/#appsync-resolver","text":"New in 1.12.0 Used when building Lambda GraphQL Resolvers with Amplify GraphQL Transform Library ( @function ), and AppSync Direct Lambda Resolvers . In this example, we also use the new Logger correlation_id and built-in correlation_paths to extract, if available, X-Ray Trace ID in AppSync request headers: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from aws_lambda_powertools.logging import Logger , correlation_paths from aws_lambda_powertools.utilities.data_classes.appsync_resolver_event import ( AppSyncResolverEvent , AppSyncIdentityCognito ) logger = Logger () def get_locations ( name : str = None , size : int = 0 , page : int = 0 ): \"\"\"Your resolver logic here\"\"\" @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPSYNC_RESOLVER ) def lambda_handler ( event , context ): event : AppSyncResolverEvent = AppSyncResolverEvent ( event ) # Case insensitive look up of request headers x_forwarded_for = event . get_header_value ( \"x-forwarded-for\" ) # Support for AppSyncIdentityCognito or AppSyncIdentityIAM identity types assert isinstance ( event . identity , AppSyncIdentityCognito ) identity : AppSyncIdentityCognito = event . identity # Logging with correlation_id logger . debug ({ \"x-forwarded-for\" : x_forwarded_for , \"username\" : identity . username }) if event . type_name == \"Merchant\" and event . field_name == \"locations\" : return get_locations ( ** event . arguments ) raise ValueError ( f \"Unsupported field resolver: { event . field_name } \" ) Example AppSync Event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \"typeName\" : \"Merchant\" , \"fieldName\" : \"locations\" , \"arguments\" : { \"page\" : 2 , \"size\" : 1 , \"name\" : \"value\" }, \"identity\" : { \"claims\" : { \"iat\" : 1615366261 ... }, \"username\" : \"mike\" , ... }, \"request\" : { \"headers\" : { \"x-amzn-trace-id\" : \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\" , \"x-forwarded-for\" : \"127.0.0.1\" ... } }, ... } Example CloudWatch Log 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"level\" : \"DEBUG\" , \"location\" : \"lambda_handler:22\" , \"message\" :{ \"x-forwarded-for\" : \"127.0.0.1\" , \"username\" : \"mike\" }, \"timestamp\" : \"2021-03-10 12:38:40,062\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 , \"cold_start\" : true , \"function_name\" : \"func_name\" , \"function_memory_size\" : 512 , \"function_arn\" : \"func_arn\" , \"function_request_id\" : \"6735a29c-c000-4ae3-94e6-1f1c934f7f94\" , \"correlation_id\" : \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\" }","title":"AppSync Resolver"},{"location":"utilities/data_classes/#cloudwatch-logs","text":"CloudWatch Logs events by default are compressed and base64 encoded. You can use the helper function provided to decode, decompress and parse json data from the event. app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.data_classes import event_source , CloudWatchLogsEvent from aws_lambda_powertools.utilities.data_classes.cloud_watch_logs_event import CloudWatchLogsDecodedData @event_source ( data_class = CloudWatchLogsEvent ) def lambda_handler ( event : CloudWatchLogsEvent , context ): decompressed_log : CloudWatchLogsDecodedData = event . parse_logs_data log_events = decompressed_log . log_events for event in log_events : do_something_with ( event . timestamp , event . message )","title":"CloudWatch Logs"},{"location":"utilities/data_classes/#codepipeline-job","text":"Data classes and utility functions to help create continuous delivery pipelines tasks with AWS Lambda app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.data_classes import event_source , CodePipelineJobEvent logger = Logger () @event_source ( data_class = CodePipelineJobEvent ) def lambda_handler ( event , context ): \"\"\"The Lambda function handler If a continuing job then checks the CloudFormation stack status and updates the job accordingly. If a new job then kick of an update or creation of the target CloudFormation stack. \"\"\" # Extract the Job ID job_id = event . get_id # Extract the params params : dict = event . decoded_user_parameters stack = params [ \"stack\" ] artifact_name = params [ \"artifact\" ] template_file = params [ \"file\" ] try : if event . data . continuation_token : # If we're continuing then the create/update has already been triggered # we just need to check if it has finished. check_stack_update_status ( job_id , stack ) else : template = event . get_artifact ( artifact_name , template_file ) # Kick off a stack update or create start_update_or_create ( job_id , stack , template ) except Exception as e : # If any other exceptions which we didn't expect are raised # then fail the job and log the exception message. logger . exception ( \"Function failed due to exception.\" ) put_job_failure ( job_id , \"Function exception: \" + str ( e )) logger . debug ( \"Function complete.\" ) return \"Complete.\"","title":"CodePipeline Job"},{"location":"utilities/data_classes/#cognito-user-pool","text":"Cognito User Pools have several different Lambda trigger sources , all of which map to a different data class, which can be imported from aws_lambda_powertools.data_classes.cognito_user_pool_event : Trigger/Event Source Data Class Custom message event data_classes.cognito_user_pool_event.CustomMessageTriggerEvent Post authentication data_classes.cognito_user_pool_event.PostAuthenticationTriggerEvent Post confirmation data_classes.cognito_user_pool_event.PostConfirmationTriggerEvent Pre authentication data_classes.cognito_user_pool_event.PreAuthenticationTriggerEvent Pre sign-up data_classes.cognito_user_pool_event.PreSignUpTriggerEvent Pre token generation data_classes.cognito_user_pool_event.PreTokenGenerationTriggerEvent User migration data_classes.cognito_user_pool_event.UserMigrationTriggerEvent Define Auth Challenge data_classes.cognito_user_pool_event.DefineAuthChallengeTriggerEvent Create Auth Challenge data_classes.cognito_user_pool_event.CreateAuthChallengeTriggerEvent Verify Auth Challenge data_classes.cognito_user_pool_event.VerifyAuthChallengeResponseTriggerEvent","title":"Cognito User Pool"},{"location":"utilities/data_classes/#post-confirmation-example","text":"app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import PostConfirmationTriggerEvent def lambda_handler ( event , context ): event : PostConfirmationTriggerEvent = PostConfirmationTriggerEvent ( event ) user_attributes = event . request . user_attributes do_something_with ( user_attributes )","title":"Post Confirmation Example"},{"location":"utilities/data_classes/#define-auth-challenge-example","text":"Note In this example we are modifying the wrapped dict response fields, so we need to return the json serializable wrapped event in event.raw_event . This example is based on the AWS Cognito docs for Define Auth Challenge Lambda Trigger . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import DefineAuthChallengeTriggerEvent def handler ( event : dict , context ) -> dict : event : DefineAuthChallengeTriggerEvent = DefineAuthChallengeTriggerEvent ( event ) if ( len ( event . request . session ) == 1 and event . request . session [ 0 ] . challenge_name == \"SRP_A\" ): event . response . issue_tokens = False event . response . fail_authentication = False event . response . challenge_name = \"PASSWORD_VERIFIER\" elif ( len ( event . request . session ) == 2 and event . request . session [ 1 ] . challenge_name == \"PASSWORD_VERIFIER\" and event . request . session [ 1 ] . challenge_result ): event . response . issue_tokens = False event . response . fail_authentication = False event . response . challenge_name = \"CUSTOM_CHALLENGE\" elif ( len ( event . request . session ) == 3 and event . request . session [ 2 ] . challenge_name == \"CUSTOM_CHALLENGE\" and event . request . session [ 2 ] . challenge_result ): event . response . issue_tokens = True event . response . fail_authentication = False else : event . response . issue_tokens = False event . response . fail_authentication = True return event . raw_event SPR_A response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { \"version\" : \"1\" , \"region\" : \"us-east-1\" , \"userPoolId\" : \"us-east-1_example\" , \"userName\" : \"UserName\" , \"callerContext\" : { \"awsSdkVersion\" : \"awsSdkVersion\" , \"clientId\" : \"clientId\" }, \"triggerSource\" : \"DefineAuthChallenge_Authentication\" , \"request\" : { \"userAttributes\" : { \"sub\" : \"4A709A36-7D63-4785-829D-4198EF10EBDA\" , \"email_verified\" : \"true\" , \"name\" : \"First Last\" , \"email\" : \"define-auth@mail.com\" }, \"session\" : [ { \"challengeName\" : \"SRP_A\" , \"challengeResult\" : true } ] }, \"response\" : { \"issueTokens\" : false , \"failAuthentication\" : false , \"challengeName\" : \"PASSWORD_VERIFIER\" } } PASSWORD_VERIFIER success response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 { \"version\" : \"1\" , \"region\" : \"us-east-1\" , \"userPoolId\" : \"us-east-1_example\" , \"userName\" : \"UserName\" , \"callerContext\" : { \"awsSdkVersion\" : \"awsSdkVersion\" , \"clientId\" : \"clientId\" }, \"triggerSource\" : \"DefineAuthChallenge_Authentication\" , \"request\" : { \"userAttributes\" : { \"sub\" : \"4A709A36-7D63-4785-829D-4198EF10EBDA\" , \"email_verified\" : \"true\" , \"name\" : \"First Last\" , \"email\" : \"define-auth@mail.com\" }, \"session\" : [ { \"challengeName\" : \"SRP_A\" , \"challengeResult\" : true }, { \"challengeName\" : \"PASSWORD_VERIFIER\" , \"challengeResult\" : true } ] }, \"response\" : { \"issueTokens\" : false , \"failAuthentication\" : false , \"challengeName\" : \"CUSTOM_CHALLENGE\" } } CUSTOM_CHALLENGE success response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { \"version\" : \"1\" , \"region\" : \"us-east-1\" , \"userPoolId\" : \"us-east-1_example\" , \"userName\" : \"UserName\" , \"callerContext\" : { \"awsSdkVersion\" : \"awsSdkVersion\" , \"clientId\" : \"clientId\" }, \"triggerSource\" : \"DefineAuthChallenge_Authentication\" , \"request\" : { \"userAttributes\" : { \"sub\" : \"4A709A36-7D63-4785-829D-4198EF10EBDA\" , \"email_verified\" : \"true\" , \"name\" : \"First Last\" , \"email\" : \"define-auth@mail.com\" }, \"session\" : [ { \"challengeName\" : \"SRP_A\" , \"challengeResult\" : true }, { \"challengeName\" : \"PASSWORD_VERIFIER\" , \"challengeResult\" : true }, { \"challengeName\" : \"CUSTOM_CHALLENGE\" , \"challengeResult\" : true } ] }, \"response\" : { \"issueTokens\" : true , \"failAuthentication\" : false } }","title":"Define Auth Challenge Example"},{"location":"utilities/data_classes/#create-auth-challenge-example","text":"This example is based on the AWS Cognito docs for Create Auth Challenge Lambda Trigger . app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.data_classes import event_source from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import CreateAuthChallengeTriggerEvent @event_source ( data_class = CreateAuthChallengeTriggerEvent ) def handler ( event : CreateAuthChallengeTriggerEvent , context ) -> dict : if event . request . challenge_name == \"CUSTOM_CHALLENGE\" : event . response . public_challenge_parameters = { \"captchaUrl\" : \"url/123.jpg\" } event . response . private_challenge_parameters = { \"answer\" : \"5\" } event . response . challenge_metadata = \"CAPTCHA_CHALLENGE\" return event . raw_event","title":"Create Auth Challenge Example"},{"location":"utilities/data_classes/#verify-auth-challenge-response-example","text":"This example is based on the AWS Cognito docs for Verify Auth Challenge Response Lambda Trigger . app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.data_classes import event_source from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import VerifyAuthChallengeResponseTriggerEvent @event_source ( data_class = VerifyAuthChallengeResponseTriggerEvent ) def handler ( event : VerifyAuthChallengeResponseTriggerEvent , context ) -> dict : event . response . answer_correct = ( event . request . private_challenge_parameters . get ( \"answer\" ) == event . request . challenge_answer ) return event . raw_event","title":"Verify Auth Challenge Response Example"},{"location":"utilities/data_classes/#connect-contact-flow","text":"New in 1.11.0 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.data_classes.connect_contact_flow_event import ( ConnectContactFlowChannel , ConnectContactFlowEndpointType , ConnectContactFlowEvent , ConnectContactFlowInitiationMethod , ) def lambda_handler ( event , context ): event : ConnectContactFlowEvent = ConnectContactFlowEvent ( event ) assert event . contact_data . attributes == { \"Language\" : \"en-US\" } assert event . contact_data . channel == ConnectContactFlowChannel . VOICE assert event . contact_data . customer_endpoint . endpoint_type == ConnectContactFlowEndpointType . TELEPHONE_NUMBER assert event . contact_data . initiation_method == ConnectContactFlowInitiationMethod . API","title":"Connect Contact Flow"},{"location":"utilities/data_classes/#dynamodb-streams","text":"The DynamoDB data class utility provides the base class for DynamoDBStreamEvent , a typed class for attributes values ( AttributeValue ), as well as enums for stream view type ( StreamViewType ) and event type ( DynamoDBRecordEventName ). app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import ( DynamoDBStreamEvent , DynamoDBRecordEventName ) def lambda_handler ( event , context ): event : DynamoDBStreamEvent = DynamoDBStreamEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : if record . event_name == DynamoDBRecordEventName . MODIFY : do_something_with ( record . dynamodb . new_image ) do_something_with ( record . dynamodb . old_image ) multiple_records_types.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from aws_lambda_powertools.utilities.data_classes import event_source , DynamoDBStreamEvent from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import AttributeValueType , AttributeValue from aws_lambda_powertools.utilities.typing import LambdaContext @event_source ( data_class = DynamoDBStreamEvent ) def lambda_handler ( event : DynamoDBStreamEvent , context : LambdaContext ): for record in event . records : key : AttributeValue = record . dynamodb . keys [ \"id\" ] if key == AttributeValueType . Number : # {\"N\": \"123.45\"} => \"123.45\" assert key . get_value == key . n_value print ( key . get_value ) elif key == AttributeValueType . Map : assert key . get_value == key . map_value print ( key . get_value )","title":"DynamoDB Streams"},{"location":"utilities/data_classes/#eventbridge","text":"app.py 1 2 3 4 5 from aws_lambda_powertools.utilities.data_classes import event_source , EventBridgeEvent @event_source ( data_class = EventBridgeEvent ) def lambda_handler ( event : EventBridgeEvent , context ): do_something_with ( event . detail )","title":"EventBridge"},{"location":"utilities/data_classes/#kinesis-streams","text":"Kinesis events by default contain base64 encoded data. You can use the helper function to access the data either as json or plain text, depending on the original payload. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.data_classes import event_source , KinesisStreamEvent @event_source ( data_class = KinesisStreamEvent ) def lambda_handler ( event : KinesisStreamEvent , context ): kinesis_record = next ( event . records ) . kinesis # if data was delivered as text data = kinesis_record . data_as_text () # if data was delivered as json data = kinesis_record . data_as_json () do_something_with ( data )","title":"Kinesis streams"},{"location":"utilities/data_classes/#rabbit-mq","text":"It is used for Rabbit MQ payloads , also see the blog post for more details. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from typing import Dict from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.data_classes import event_source from aws_lambda_powertools.utilities.data_classes.rabbit_mq_event import RabbitMQEvent logger = Logger () @event_source ( data_class = RabbitMQEvent ) def lambda_handler ( event : RabbitMQEvent , context ): for queue_name , messages in event . rmq_messages_by_queue . items (): logger . debug ( f \"Messages for queue: { queue_name } \" ) for message in messages : logger . debug ( f \"MessageID: { message . basic_properties . message_id } \" ) data : Dict = message . json_data logger . debug ( \"Process json in base64 encoded data str\" , data )","title":"Rabbit MQ"},{"location":"utilities/data_classes/#s3","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 from urllib.parse import unquote_plus from aws_lambda_powertools.utilities.data_classes import event_source , S3Event @event_source ( data_class = S3Event ) def lambda_handler ( event : S3Event , context ): bucket_name = event . bucket_name # Multiple records can be delivered in a single event for record in event . records : object_key = unquote_plus ( record . s3 . get_object . key ) do_something_with ( f \" { bucket_name } / { object_key } \" )","title":"S3"},{"location":"utilities/data_classes/#s3-object-lambda","text":"This example is based on the AWS Blog post Introducing Amazon S3 Object Lambda \u2013 Use Your Code to Process Data as It Is Being Retrieved from S3 . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import boto3 import requests from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.correlation_paths import S3_OBJECT_LAMBDA from aws_lambda_powertools.utilities.data_classes.s3_object_event import S3ObjectLambdaEvent logger = Logger () session = boto3 . Session () s3 = session . client ( \"s3\" ) @logger . inject_lambda_context ( correlation_id_path = S3_OBJECT_LAMBDA , log_event = True ) def lambda_handler ( event , context ): event = S3ObjectLambdaEvent ( event ) # Get object from S3 response = requests . get ( event . input_s3_url ) original_object = response . content . decode ( \"utf-8\" ) # Make changes to the object about to be returned transformed_object = original_object . upper () # Write object back to S3 Object Lambda s3 . write_get_object_response ( Body = transformed_object , RequestRoute = event . request_route , RequestToken = event . request_token ) return { \"status_code\" : 200 }","title":"S3 Object Lambda"},{"location":"utilities/data_classes/#ses","text":"app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.data_classes import event_source , SESEvent @event_source ( data_class = SESEvent ) def lambda_handler ( event : SESEvent , context ): # Multiple records can be delivered in a single event for record in event . records : mail = record . ses . mail common_headers = mail . common_headers do_something_with ( common_headers . to , common_headers . subject )","title":"SES"},{"location":"utilities/data_classes/#sns","text":"app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.data_classes import event_source , SNSEvent @event_source ( data_class = SNSEvent ) def lambda_handler ( event : SNSEvent , context ): # Multiple records can be delivered in a single event for record in event . records : message = record . sns . message subject = record . sns . subject do_something_with ( subject , message )","title":"SNS"},{"location":"utilities/data_classes/#sqs","text":"app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities.data_classes import event_source , SQSEvent @event_source ( data_class = SQSEvent ) def lambda_handler ( event : SQSEvent , context ): # Multiple records can be delivered in a single event for record in event . records : do_something_with ( record . body )","title":"SQS"},{"location":"utilities/feature_flags/","text":"Note This is currently in Beta, as we might change Store parameters in the next release. The feature flags utility provides a simple rule engine to define when one or multiple features should be enabled depending on the input. Terminology \u00b6 Feature flags are used to modify behaviour without changing the application's code. These flags can be static or dynamic . Static flags . Indicates something is simply on or off , for example TRACER_ENABLED=True . Dynamic flags . Indicates something can have varying states, for example enable a list of premium features for customer X not Y. Tip You can use Parameters utility for static flags while this utility can do both static and dynamic feature flags. Warning Be mindful that feature flags can increase the complexity of your application over time; use them sparingly. If you want to learn more about feature flags, their variations and trade-offs, check these articles: Feature Toggles (aka Feature Flags) - Pete Hodgson AWS Lambda Feature Toggles Made Simple - Ran Isenberg Feature Flags Getting Started - CloudBees Key features \u00b6 Define simple feature flags to dynamically decide when to enable a feature Fetch one or all feature flags enabled for a given application context Support for static feature flags to simply turn on/off a feature without rules Getting started \u00b6 IAM Permissions \u00b6 Your Lambda function must have appconfig:GetConfiguration IAM permission in order to fetch configuration from AWS AppConfig. Required resources \u00b6 By default, this utility provides AWS AppConfig as a configuration store. The following sample infrastructure will be used throughout this documentation: template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 AWSTemplateFormatVersion : \"2010-09-09\" Description : Lambda Powertools Feature flags sample template Resources : FeatureStoreApp : Type : AWS::AppConfig::Application Properties : Description : \"AppConfig Application for feature toggles\" Name : product-catalogue FeatureStoreDevEnv : Type : AWS::AppConfig::Environment Properties : ApplicationId : !Ref FeatureStoreApp Description : \"Development Environment for the App Config Store\" Name : dev FeatureStoreConfigProfile : Type : AWS::AppConfig::ConfigurationProfile Properties : ApplicationId : !Ref FeatureStoreApp Name : features LocationUri : \"hosted\" HostedConfigVersion : Type : AWS::AppConfig::HostedConfigurationVersion Properties : ApplicationId : !Ref FeatureStoreApp ConfigurationProfileId : !Ref FeatureStoreConfigProfile Description : 'A sample hosted configuration version' Content : | { \"premium_features\": { \"default\": false, \"rules\": { \"customer tier equals premium\": { \"when_match\": true, \"conditions\": [ { \"action\": \"EQUALS\", \"key\": \"tier\", \"value\": \"premium\" } ] } } }, \"ten_percent_off_campaign\": { \"default\": false } } ContentType : 'application/json' ConfigDeployment : Type : AWS::AppConfig::Deployment Properties : ApplicationId : !Ref FeatureStoreApp ConfigurationProfileId : !Ref FeatureStoreConfigProfile ConfigurationVersion : !Ref HostedConfigVersion DeploymentStrategyId : \"AppConfig.AllAtOnce\" EnvironmentId : !Ref FeatureStoreDevEnv CDK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 import json import aws_cdk.aws_appconfig as appconfig from aws_cdk import core class SampleFeatureFlagStore ( core . Construct ): def __init__ ( self , scope : core . Construct , id_ : str ) -> None : super () . __init__ ( scope , id_ ) features_config = { \"premium_features\" : { \"default\" : False , \"rules\" : { \"customer tier equals premium\" : { \"when_match\" : True , \"conditions\" : [{ \"action\" : \"EQUALS\" , \"key\" : \"tier\" , \"value\" : \"premium\" }], } }, }, \"ten_percent_off_campaign\" : { \"default\" : True }, } self . config_app = appconfig . CfnApplication ( self , id = \"app\" , name = \"product-catalogue\" , ) self . config_env = appconfig . CfnEnvironment ( self , id = \"env\" , application_id = self . config_app . ref , name = \"dev-env\" , ) self . config_profile = appconfig . CfnConfigurationProfile ( self , id = \"profile\" , application_id = self . config_app . ref , location_uri = \"hosted\" , name = \"features\" , ) self . hosted_cfg_version = appconfig . CfnHostedConfigurationVersion ( self , \"version\" , application_id = self . config_app . ref , configuration_profile_id = self . config_profile . ref , content = json . dumps ( features_config ), content_type = \"application/json\" , ) self . app_config_deployment = appconfig . CfnDeployment ( self , id = \"deploy\" , application_id = self . config_app . ref , configuration_profile_id = self . config_profile . ref , configuration_version = self . hosted_cfg_version . ref , deployment_strategy_id = \"AppConfig.AllAtOnce\" , environment_id = self . config_env . ref , ) Evaluating a single feature flag \u00b6 To get started, you'd need to initialize AppConfigStore and FeatureFlags . Then call FeatureFlags evaluate method to fetch, validate, and evaluate your feature. The evaluate method supports two optional parameters: context : Value to be evaluated against each rule defined for the given feature default : Sentinel value to use in case we experience any issues with our store, or feature doesn't exist app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from aws_lambda_powertools.utilities.feature_flags import FeatureFlags , AppConfigStore app_config = AppConfigStore ( environment = \"dev\" , application = \"product-catalogue\" , name = \"features\" ) feature_flags = FeatureFlags ( store = app_config ) def lambda_handler ( event , context ): # Get customer's tier from incoming request ctx = { \"tier\" : event . get ( \"tier\" , \"standard\" ) } # Evaluate whether customer's tier has access to premium features # based on `has_premium_features` rules has_premium_features : bool = feature_flags . evaluate ( name = \"premium_features\" , context = ctx , default = False ) if has_premium_features : # enable premium features ... event.json 1 2 3 4 5 { \"username\" : \"lessa\" , \"tier\" : \"premium\" , \"basked_id\" : \"random_id\" } features.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"premium_features\" : { \"default\" : false , \"rules\" : { \"customer tier equals premium\" : { \"when_match\" : true , \"conditions\" : [ { \"action\" : \"EQUALS\" , \"key\" : \"tier\" , \"value\" : \"premium\" } ] } } }, \"ten_percent_off_campaign\" : { \"default\" : false } } Static flags \u00b6 We have a static flag named ten_percent_off_campaign . Meaning, there are no conditional rules, it's either ON or OFF for all customers. In this case, we could omit the context parameter and simply evaluate whether we should apply the 10% discount. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from aws_lambda_powertools.utilities.feature_flags import FeatureFlags , AppConfigStore app_config = AppConfigStore ( environment = \"dev\" , application = \"product-catalogue\" , name = \"features\" ) feature_flags = FeatureFlags ( store = app_config ) def lambda_handler ( event , context ): apply_discount : bool = feature_flags . evaluate ( name = \"ten_percent_off_campaign\" , default = False ) if apply_discount : # apply 10% discount to product ... features.json 1 2 3 4 5 { \"ten_percent_off_campaign\" : { \"default\" : false } } Getting all enabled features \u00b6 As you might have noticed, each evaluate call means an API call to the Store and the more features you have the more costly this becomes. You can use get_enabled_features method for scenarios where you need a list of all enabled features according to the input context. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver from aws_lambda_powertools.utilities.feature_flags import FeatureFlags , AppConfigStore app = ApiGatewayResolver () app_config = AppConfigStore ( environment = \"dev\" , application = \"product-catalogue\" , name = \"features\" ) feature_flags = FeatureFlags ( store = app_config ) @app . get ( \"/products\" ) def list_products (): ctx = { ** app . current_event . headers , ** app . current_event . json_body } # all_features is evaluated to [\"geo_customer_campaign\", \"ten_percent_off_campaign\"] all_features : list [ str ] = feature_flags . get_enabled_features ( context = ctx ) if \"geo_customer_campaign\" in all_features : # apply discounts based on geo ... if \"ten_percent_off_campaign\" in all_features : # apply additional 10% for all customers ... def lambda_handler ( event , context ): return app . resolve ( event , context ) event.json 1 2 3 4 5 6 7 8 9 10 { \"body\" : \"{\\\"username\\\": \\\"lessa\\\", \\\"tier\\\": \\\"premium\\\", \\\"basked_id\\\": \\\"random_id\\\"}\" , \"resource\" : \"/products\" , \"path\" : \"/products\" , \"httpMethod\" : \"GET\" , \"isBase64Encoded\" : false , \"headers\" : { \"CloudFront-Viewer-Country\" : \"NL\" } } features.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 { \"premium_features\" : { \"default\" : false , \"rules\" : { \"customer tier equals premium\" : { \"when_match\" : true , \"conditions\" : [ { \"action\" : \"EQUALS\" , \"key\" : \"tier\" , \"value\" : \"premium\" } ] } } }, \"ten_percent_off_campaign\" : { \"default\" : true }, \"geo_customer_campaign\" : { \"default\" : false , \"rules\" : { \"customer in temporary discount geo\" : { \"when_match\" : true , \"conditions\" : [ { \"action\" : \"KEY_IN_VALUE\" , \"key\" : \"CloudFront-Viewer-Country\" , \"value\" : [ \"NL\" , \"IE\" , \"UK\" , \"PL\" , \"PT\" ] } ] } } } } Beyond boolean feature flags \u00b6 When is this useful? You might have a list of features to unlock for premium customers, unlock a specific set of features for admin users, etc. Feature flags can return any JSON values when boolean_type parameter is set to false . These can be dictionaries, list, string, integers, etc. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from aws_lambda_powertools.utilities.feature_flags import FeatureFlags , AppConfigStore app_config = AppConfigStore ( environment = \"dev\" , application = \"product-catalogue\" , name = \"features\" ) feature_flags = FeatureFlags ( store = app_config ) def lambda_handler ( event , context ): # Get customer's tier from incoming request ctx = { \"tier\" : event . get ( \"tier\" , \"standard\" ) } # Evaluate `has_premium_features` base don customer's tier premium_features : list [ str ] = feature_flags . evaluate ( name = \"premium_features\" , context = ctx , default = False ) for feature in premium_features : # enable premium features ... event.json 1 2 3 4 5 { \"username\" : \"lessa\" , \"tier\" : \"premium\" , \"basked_id\" : \"random_id\" } features.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"premium_features\" : { \"boolean_type\" : false , \"default\" : [], \"rules\" : { \"customer tier equals premium\" : { \"when_match\" : [ \"no_ads\" , \"no_limits\" , \"chat\" ], \"conditions\" : [ { \"action\" : \"EQUALS\" , \"key\" : \"tier\" , \"value\" : \"premium\" } ] } } } } Advanced \u00b6 Adjusting in-memory cache \u00b6 By default, we cache configuration retrieved from the Store for 5 seconds for performance and reliability reasons. You can override max_age parameter when instantiating the store. 1 2 3 4 5 6 7 8 9 10 ```python hl_lines=\"7\" from aws_lambda_powertools.utilities.feature_flags import FeatureFlags, AppConfigStore app_config = AppConfigStore( environment=\"dev\", application=\"product-catalogue\", name=\"features\", max_age=300 ) ``` Getting fetched configuration \u00b6 When is this useful? You might have application configuration in addition to feature flags in your store. This means you don't need to make another call only to fetch app configuration. You can access the configuration fetched from the store via get_raw_configuration property within the store instance. app.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.feature_flags import FeatureFlags , AppConfigStore app_config = AppConfigStore ( environment = \"dev\" , application = \"product-catalogue\" , name = \"configuration\" , envelope = \"feature_flags\" ) feature_flags = FeatureFlags ( store = app_config ) config = app_config . get_raw_configuration Schema \u00b6 This utility expects a certain schema to be stored as JSON within AWS AppConfig. Features \u00b6 A feature can simply have its name and a default value. This is either on or off, also known as a static flag . minimal_schema.json 1 2 3 4 5 6 7 8 9 { \"global_feature\" : { \"default\" : true }, \"non_boolean_global_feature\" : { \"default\" : { \"group\" : \"read-only\" }, \"boolean_type\" : false }, } If you need more control and want to provide context such as user group, permissions, location, etc., you need to add rules to your feature flag configuration. Rules \u00b6 When adding rules to a feature, they must contain: A rule name as a key when_match boolean or JSON value that should be used when conditions match A list of conditions for evaluation feature_with_rules.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 { \"premium_feature\" : { \"default\" : false , \"rules\" : { \"customer tier equals premium\" : { \"when_match\" : true , \"conditions\" : [ { \"action\" : \"EQUALS\" , \"key\" : \"tier\" , \"value\" : \"premium\" } ] } } }, \"non_boolean_premium_feature\" : { \"default\" : [], \"rules\" : { \"customer tier equals premium\" : { \"when_match\" : [ \"remove_limits\" , \"remove_ads\" ], \"conditions\" : [ { \"action\" : \"EQUALS\" , \"key\" : \"tier\" , \"value\" : \"premium\" } ] } } } } You can have multiple rules with different names. The rule engine will return the first result when_match of the matching rule configuration, or default value when none of the rules apply. Conditions \u00b6 The conditions block is a list of conditions that contain action , key , and value keys: conditions.json 1 2 3 4 5 6 7 8 9 10 { ... \"conditions\" : [ { \"action\" : \"EQUALS\" , \"key\" : \"tier\" , \"value\" : \"premium\" } ] } The action configuration can have the following values, where the expressions a is the key and b is the value above: Action Equivalent expression EQUALS lambda a, b: a == b NOT_EQUALS lambda a, b: a != b KEY_GREATER_THAN_VALUE lambda a, b: a > b KEY_GREATER_THAN_OR_EQUAL_VALUE lambda a, b: a >= b KEY_LESS_THAN_VALUE lambda a, b: a < b KEY_LESS_THAN_OR_EQUAL_VALUE lambda a, b: a <= b STARTSWITH lambda a, b: a.startswith(b) ENDSWITH lambda a, b: a.endswith(b) KEY_IN_VALUE lambda a, b: a in b KEY_NOT_IN_VALUE lambda a, b: a not in b VALUE_IN_KEY lambda a, b: b in a VALUE_NOT_IN_KEY lambda a, b: b not in a Info The **key** and **value** will be compared to the input from the **context** parameter. For multiple conditions , we will evaluate the list of conditions as a logical AND , so all conditions needs to match to return when_match value. Rule engine flowchart \u00b6 Now that you've seen all properties of a feature flag schema, this flowchart describes how the rule engine decides what value to return. Envelope \u00b6 There are scenarios where you might want to include feature flags as part of an existing application configuration. For this to work, you need to use a JMESPath expression via the envelope parameter to extract that key as the feature flags configuration. app.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.feature_flags import FeatureFlags , AppConfigStore app_config = AppConfigStore ( environment = \"dev\" , application = \"product-catalogue\" , name = \"configuration\" , envelope = \"feature_flags\" ) configuration.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \"logging\" : { \"level\" : \"INFO\" , \"sampling_rate\" : 0.1 }, \"feature_flags\" : { \"premium_feature\" : { \"default\" : false , \"rules\" : { \"customer tier equals premium\" : { \"when_match\" : true , \"conditions\" : [ { \"action\" : \"EQUALS\" , \"key\" : \"tier\" , \"value\" : \"premium\" } ] } } }, \"feature2\" : { \"default\" : false } } } Built-in store provider \u00b6 Info For GA, you'll be able to bring your own store. AppConfig \u00b6 AppConfig store provider fetches any JSON document from AWS AppConfig. These are the available options for further customization. Parameter Default Description environment \"\" AWS AppConfig Environment, e.g. test application \"\" AWS AppConfig Application name \"\" AWS AppConfig Configuration name envelope None JMESPath expression to use to extract feature flags configuration from AWS AppConfig configuration max_age 5 Number of seconds to cache feature flags configuration fetched from AWS AppConfig sdk_config None Botocore Config object jmespath_options None For advanced use cases when you want to bring your own JMESPath functions logger logging.Logger Logger to use for debug. You can optionally supply an instance of Powertools Logger. AppConfigStore sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from botocore.config import Config import jmespath boto_config = Config ( read_timeout = 10 , retries = { \"total_max_attempts\" : 2 }) # Custom JMESPath functions class CustomFunctions ( jmespath . functions . Functions ): @jmespath . functions . signature ({ 'types' : [ 'string' ]}) def _func_special_decoder ( self , s ): return my_custom_decoder_logic ( s ) custom_jmespath_options = { \"custom_functions\" : CustomFunctions ()} app_config = AppConfigStore ( environment = \"dev\" , application = \"product-catalogue\" , name = \"configuration\" , max_age = 120 , envelope = \"features\" , sdk_config = boto_config , jmespath_options = custom_jmespath_options ) Testing your code \u00b6 You can unit test your feature flags locally and independently without setting up AWS AppConfig. AppConfigStore only fetches a JSON document with a specific schema. This allows you to mock the response and use it to verify the rule evaluation. Warning This excerpt relies on pytest and pytest-mock dependencies. Unit testing feature flags 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 from typing import Dict , List , Optional from aws_lambda_powertools.utilities.feature_flags import FeatureFlags , AppConfigStore , RuleAction def init_feature_flags ( mocker , mock_schema , envelope = \"\" ) -> FeatureFlags : \"\"\"Mock AppConfig Store get_configuration method to use mock schema instead\"\"\" method_to_mock = \"aws_lambda_powertools.utilities.feature_flags.AppConfigStore.get_configuration\" mocked_get_conf = mocker . patch ( method_to_mock ) mocked_get_conf . return_value = mock_schema app_conf_store = AppConfigStore ( environment = \"test_env\" , application = \"test_app\" , name = \"test_conf_name\" , envelope = envelope , ) return FeatureFlags ( store = app_conf_store ) def test_flags_condition_match ( mocker ): # GIVEN expected_value = True mocked_app_config_schema = { \"my_feature\" : { \"default\" : expected_value , \"rules\" : { \"tenant id equals 12345\" : { \"when_match\" : True , \"conditions\" : [ { \"action\" : RuleAction . EQUALS . value , \"key\" : \"tenant_id\" , \"value\" : \"12345\" , } ], } }, } } # WHEN ctx = { \"tenant_id\" : \"12345\" , \"username\" : \"a\" } feature_flags = init_feature_flags ( mocker = mocker , mock_schema = mocked_app_config_schema ) flag = feature_flags . evaluate ( name = \"my_feature\" , context = ctx , default = False ) # THEN assert flag == expected_value Feature flags vs Parameters vs env vars \u00b6 Method When to use Requires new deployment on changes Supported services Environment variables Simple configuration that will rarely if ever change, because changing it requires a Lambda function deployment. Yes Lambda Parameters utility Access to secrets, or fetch parameters in different formats from AWS System Manager Parameter Store or Amazon DynamoDB. No Parameter Store, DynamoDB, Secrets Manager, AppConfig Feature flags utility Rule engine to define when one or multiple features should be enabled depending on the input. No AppConfig Deprecation list when GA \u00b6 Breaking change Recommendation IN RuleAction Use KEY_IN_VALUE instead NOT_IN RuleAction Use KEY_NOT_IN_VALUE instead get_enabled_features Return type changes from List[str] to Dict[str, Any] . New return will contain a list of features enabled and their values. List of enabled features will be in enabled_features key to keep ease of assertion we have in Beta. boolean_type Schema This might not be necessary anymore before we go GA. We will return either the default value when there are no rules as well as when_match value. This will simplify on-boarding if we can keep the same set of validations already offered.","title":"Feature flags"},{"location":"utilities/feature_flags/#terminology","text":"Feature flags are used to modify behaviour without changing the application's code. These flags can be static or dynamic . Static flags . Indicates something is simply on or off , for example TRACER_ENABLED=True . Dynamic flags . Indicates something can have varying states, for example enable a list of premium features for customer X not Y. Tip You can use Parameters utility for static flags while this utility can do both static and dynamic feature flags. Warning Be mindful that feature flags can increase the complexity of your application over time; use them sparingly. If you want to learn more about feature flags, their variations and trade-offs, check these articles: Feature Toggles (aka Feature Flags) - Pete Hodgson AWS Lambda Feature Toggles Made Simple - Ran Isenberg Feature Flags Getting Started - CloudBees","title":"Terminology"},{"location":"utilities/feature_flags/#key-features","text":"Define simple feature flags to dynamically decide when to enable a feature Fetch one or all feature flags enabled for a given application context Support for static feature flags to simply turn on/off a feature without rules","title":"Key features"},{"location":"utilities/feature_flags/#getting-started","text":"","title":"Getting started"},{"location":"utilities/feature_flags/#iam-permissions","text":"Your Lambda function must have appconfig:GetConfiguration IAM permission in order to fetch configuration from AWS AppConfig.","title":"IAM Permissions"},{"location":"utilities/feature_flags/#required-resources","text":"By default, this utility provides AWS AppConfig as a configuration store. The following sample infrastructure will be used throughout this documentation: template.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 AWSTemplateFormatVersion : \"2010-09-09\" Description : Lambda Powertools Feature flags sample template Resources : FeatureStoreApp : Type : AWS::AppConfig::Application Properties : Description : \"AppConfig Application for feature toggles\" Name : product-catalogue FeatureStoreDevEnv : Type : AWS::AppConfig::Environment Properties : ApplicationId : !Ref FeatureStoreApp Description : \"Development Environment for the App Config Store\" Name : dev FeatureStoreConfigProfile : Type : AWS::AppConfig::ConfigurationProfile Properties : ApplicationId : !Ref FeatureStoreApp Name : features LocationUri : \"hosted\" HostedConfigVersion : Type : AWS::AppConfig::HostedConfigurationVersion Properties : ApplicationId : !Ref FeatureStoreApp ConfigurationProfileId : !Ref FeatureStoreConfigProfile Description : 'A sample hosted configuration version' Content : | { \"premium_features\": { \"default\": false, \"rules\": { \"customer tier equals premium\": { \"when_match\": true, \"conditions\": [ { \"action\": \"EQUALS\", \"key\": \"tier\", \"value\": \"premium\" } ] } } }, \"ten_percent_off_campaign\": { \"default\": false } } ContentType : 'application/json' ConfigDeployment : Type : AWS::AppConfig::Deployment Properties : ApplicationId : !Ref FeatureStoreApp ConfigurationProfileId : !Ref FeatureStoreConfigProfile ConfigurationVersion : !Ref HostedConfigVersion DeploymentStrategyId : \"AppConfig.AllAtOnce\" EnvironmentId : !Ref FeatureStoreDevEnv CDK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 import json import aws_cdk.aws_appconfig as appconfig from aws_cdk import core class SampleFeatureFlagStore ( core . Construct ): def __init__ ( self , scope : core . Construct , id_ : str ) -> None : super () . __init__ ( scope , id_ ) features_config = { \"premium_features\" : { \"default\" : False , \"rules\" : { \"customer tier equals premium\" : { \"when_match\" : True , \"conditions\" : [{ \"action\" : \"EQUALS\" , \"key\" : \"tier\" , \"value\" : \"premium\" }], } }, }, \"ten_percent_off_campaign\" : { \"default\" : True }, } self . config_app = appconfig . CfnApplication ( self , id = \"app\" , name = \"product-catalogue\" , ) self . config_env = appconfig . CfnEnvironment ( self , id = \"env\" , application_id = self . config_app . ref , name = \"dev-env\" , ) self . config_profile = appconfig . CfnConfigurationProfile ( self , id = \"profile\" , application_id = self . config_app . ref , location_uri = \"hosted\" , name = \"features\" , ) self . hosted_cfg_version = appconfig . CfnHostedConfigurationVersion ( self , \"version\" , application_id = self . config_app . ref , configuration_profile_id = self . config_profile . ref , content = json . dumps ( features_config ), content_type = \"application/json\" , ) self . app_config_deployment = appconfig . CfnDeployment ( self , id = \"deploy\" , application_id = self . config_app . ref , configuration_profile_id = self . config_profile . ref , configuration_version = self . hosted_cfg_version . ref , deployment_strategy_id = \"AppConfig.AllAtOnce\" , environment_id = self . config_env . ref , )","title":"Required resources"},{"location":"utilities/feature_flags/#evaluating-a-single-feature-flag","text":"To get started, you'd need to initialize AppConfigStore and FeatureFlags . Then call FeatureFlags evaluate method to fetch, validate, and evaluate your feature. The evaluate method supports two optional parameters: context : Value to be evaluated against each rule defined for the given feature default : Sentinel value to use in case we experience any issues with our store, or feature doesn't exist app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from aws_lambda_powertools.utilities.feature_flags import FeatureFlags , AppConfigStore app_config = AppConfigStore ( environment = \"dev\" , application = \"product-catalogue\" , name = \"features\" ) feature_flags = FeatureFlags ( store = app_config ) def lambda_handler ( event , context ): # Get customer's tier from incoming request ctx = { \"tier\" : event . get ( \"tier\" , \"standard\" ) } # Evaluate whether customer's tier has access to premium features # based on `has_premium_features` rules has_premium_features : bool = feature_flags . evaluate ( name = \"premium_features\" , context = ctx , default = False ) if has_premium_features : # enable premium features ... event.json 1 2 3 4 5 { \"username\" : \"lessa\" , \"tier\" : \"premium\" , \"basked_id\" : \"random_id\" } features.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"premium_features\" : { \"default\" : false , \"rules\" : { \"customer tier equals premium\" : { \"when_match\" : true , \"conditions\" : [ { \"action\" : \"EQUALS\" , \"key\" : \"tier\" , \"value\" : \"premium\" } ] } } }, \"ten_percent_off_campaign\" : { \"default\" : false } }","title":"Evaluating a single feature flag"},{"location":"utilities/feature_flags/#static-flags","text":"We have a static flag named ten_percent_off_campaign . Meaning, there are no conditional rules, it's either ON or OFF for all customers. In this case, we could omit the context parameter and simply evaluate whether we should apply the 10% discount. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from aws_lambda_powertools.utilities.feature_flags import FeatureFlags , AppConfigStore app_config = AppConfigStore ( environment = \"dev\" , application = \"product-catalogue\" , name = \"features\" ) feature_flags = FeatureFlags ( store = app_config ) def lambda_handler ( event , context ): apply_discount : bool = feature_flags . evaluate ( name = \"ten_percent_off_campaign\" , default = False ) if apply_discount : # apply 10% discount to product ... features.json 1 2 3 4 5 { \"ten_percent_off_campaign\" : { \"default\" : false } }","title":"Static flags"},{"location":"utilities/feature_flags/#getting-all-enabled-features","text":"As you might have noticed, each evaluate call means an API call to the Store and the more features you have the more costly this becomes. You can use get_enabled_features method for scenarios where you need a list of all enabled features according to the input context. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from aws_lambda_powertools.event_handler.api_gateway import ApiGatewayResolver from aws_lambda_powertools.utilities.feature_flags import FeatureFlags , AppConfigStore app = ApiGatewayResolver () app_config = AppConfigStore ( environment = \"dev\" , application = \"product-catalogue\" , name = \"features\" ) feature_flags = FeatureFlags ( store = app_config ) @app . get ( \"/products\" ) def list_products (): ctx = { ** app . current_event . headers , ** app . current_event . json_body } # all_features is evaluated to [\"geo_customer_campaign\", \"ten_percent_off_campaign\"] all_features : list [ str ] = feature_flags . get_enabled_features ( context = ctx ) if \"geo_customer_campaign\" in all_features : # apply discounts based on geo ... if \"ten_percent_off_campaign\" in all_features : # apply additional 10% for all customers ... def lambda_handler ( event , context ): return app . resolve ( event , context ) event.json 1 2 3 4 5 6 7 8 9 10 { \"body\" : \"{\\\"username\\\": \\\"lessa\\\", \\\"tier\\\": \\\"premium\\\", \\\"basked_id\\\": \\\"random_id\\\"}\" , \"resource\" : \"/products\" , \"path\" : \"/products\" , \"httpMethod\" : \"GET\" , \"isBase64Encoded\" : false , \"headers\" : { \"CloudFront-Viewer-Country\" : \"NL\" } } features.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 { \"premium_features\" : { \"default\" : false , \"rules\" : { \"customer tier equals premium\" : { \"when_match\" : true , \"conditions\" : [ { \"action\" : \"EQUALS\" , \"key\" : \"tier\" , \"value\" : \"premium\" } ] } } }, \"ten_percent_off_campaign\" : { \"default\" : true }, \"geo_customer_campaign\" : { \"default\" : false , \"rules\" : { \"customer in temporary discount geo\" : { \"when_match\" : true , \"conditions\" : [ { \"action\" : \"KEY_IN_VALUE\" , \"key\" : \"CloudFront-Viewer-Country\" , \"value\" : [ \"NL\" , \"IE\" , \"UK\" , \"PL\" , \"PT\" ] } ] } } } }","title":"Getting all enabled features"},{"location":"utilities/feature_flags/#beyond-boolean-feature-flags","text":"When is this useful? You might have a list of features to unlock for premium customers, unlock a specific set of features for admin users, etc. Feature flags can return any JSON values when boolean_type parameter is set to false . These can be dictionaries, list, string, integers, etc. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from aws_lambda_powertools.utilities.feature_flags import FeatureFlags , AppConfigStore app_config = AppConfigStore ( environment = \"dev\" , application = \"product-catalogue\" , name = \"features\" ) feature_flags = FeatureFlags ( store = app_config ) def lambda_handler ( event , context ): # Get customer's tier from incoming request ctx = { \"tier\" : event . get ( \"tier\" , \"standard\" ) } # Evaluate `has_premium_features` base don customer's tier premium_features : list [ str ] = feature_flags . evaluate ( name = \"premium_features\" , context = ctx , default = False ) for feature in premium_features : # enable premium features ... event.json 1 2 3 4 5 { \"username\" : \"lessa\" , \"tier\" : \"premium\" , \"basked_id\" : \"random_id\" } features.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"premium_features\" : { \"boolean_type\" : false , \"default\" : [], \"rules\" : { \"customer tier equals premium\" : { \"when_match\" : [ \"no_ads\" , \"no_limits\" , \"chat\" ], \"conditions\" : [ { \"action\" : \"EQUALS\" , \"key\" : \"tier\" , \"value\" : \"premium\" } ] } } } }","title":"Beyond boolean feature flags"},{"location":"utilities/feature_flags/#advanced","text":"","title":"Advanced"},{"location":"utilities/feature_flags/#adjusting-in-memory-cache","text":"By default, we cache configuration retrieved from the Store for 5 seconds for performance and reliability reasons. You can override max_age parameter when instantiating the store. 1 2 3 4 5 6 7 8 9 10 ```python hl_lines=\"7\" from aws_lambda_powertools.utilities.feature_flags import FeatureFlags, AppConfigStore app_config = AppConfigStore( environment=\"dev\", application=\"product-catalogue\", name=\"features\", max_age=300 ) ```","title":"Adjusting in-memory cache"},{"location":"utilities/feature_flags/#getting-fetched-configuration","text":"When is this useful? You might have application configuration in addition to feature flags in your store. This means you don't need to make another call only to fetch app configuration. You can access the configuration fetched from the store via get_raw_configuration property within the store instance. app.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.feature_flags import FeatureFlags , AppConfigStore app_config = AppConfigStore ( environment = \"dev\" , application = \"product-catalogue\" , name = \"configuration\" , envelope = \"feature_flags\" ) feature_flags = FeatureFlags ( store = app_config ) config = app_config . get_raw_configuration","title":"Getting fetched configuration"},{"location":"utilities/feature_flags/#schema","text":"This utility expects a certain schema to be stored as JSON within AWS AppConfig.","title":"Schema"},{"location":"utilities/feature_flags/#features","text":"A feature can simply have its name and a default value. This is either on or off, also known as a static flag . minimal_schema.json 1 2 3 4 5 6 7 8 9 { \"global_feature\" : { \"default\" : true }, \"non_boolean_global_feature\" : { \"default\" : { \"group\" : \"read-only\" }, \"boolean_type\" : false }, } If you need more control and want to provide context such as user group, permissions, location, etc., you need to add rules to your feature flag configuration.","title":"Features"},{"location":"utilities/feature_flags/#rules","text":"When adding rules to a feature, they must contain: A rule name as a key when_match boolean or JSON value that should be used when conditions match A list of conditions for evaluation feature_with_rules.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 { \"premium_feature\" : { \"default\" : false , \"rules\" : { \"customer tier equals premium\" : { \"when_match\" : true , \"conditions\" : [ { \"action\" : \"EQUALS\" , \"key\" : \"tier\" , \"value\" : \"premium\" } ] } } }, \"non_boolean_premium_feature\" : { \"default\" : [], \"rules\" : { \"customer tier equals premium\" : { \"when_match\" : [ \"remove_limits\" , \"remove_ads\" ], \"conditions\" : [ { \"action\" : \"EQUALS\" , \"key\" : \"tier\" , \"value\" : \"premium\" } ] } } } } You can have multiple rules with different names. The rule engine will return the first result when_match of the matching rule configuration, or default value when none of the rules apply.","title":"Rules"},{"location":"utilities/feature_flags/#conditions","text":"The conditions block is a list of conditions that contain action , key , and value keys: conditions.json 1 2 3 4 5 6 7 8 9 10 { ... \"conditions\" : [ { \"action\" : \"EQUALS\" , \"key\" : \"tier\" , \"value\" : \"premium\" } ] } The action configuration can have the following values, where the expressions a is the key and b is the value above: Action Equivalent expression EQUALS lambda a, b: a == b NOT_EQUALS lambda a, b: a != b KEY_GREATER_THAN_VALUE lambda a, b: a > b KEY_GREATER_THAN_OR_EQUAL_VALUE lambda a, b: a >= b KEY_LESS_THAN_VALUE lambda a, b: a < b KEY_LESS_THAN_OR_EQUAL_VALUE lambda a, b: a <= b STARTSWITH lambda a, b: a.startswith(b) ENDSWITH lambda a, b: a.endswith(b) KEY_IN_VALUE lambda a, b: a in b KEY_NOT_IN_VALUE lambda a, b: a not in b VALUE_IN_KEY lambda a, b: b in a VALUE_NOT_IN_KEY lambda a, b: b not in a Info The **key** and **value** will be compared to the input from the **context** parameter. For multiple conditions , we will evaluate the list of conditions as a logical AND , so all conditions needs to match to return when_match value.","title":"Conditions"},{"location":"utilities/feature_flags/#rule-engine-flowchart","text":"Now that you've seen all properties of a feature flag schema, this flowchart describes how the rule engine decides what value to return.","title":"Rule engine flowchart"},{"location":"utilities/feature_flags/#envelope","text":"There are scenarios where you might want to include feature flags as part of an existing application configuration. For this to work, you need to use a JMESPath expression via the envelope parameter to extract that key as the feature flags configuration. app.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.feature_flags import FeatureFlags , AppConfigStore app_config = AppConfigStore ( environment = \"dev\" , application = \"product-catalogue\" , name = \"configuration\" , envelope = \"feature_flags\" ) configuration.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \"logging\" : { \"level\" : \"INFO\" , \"sampling_rate\" : 0.1 }, \"feature_flags\" : { \"premium_feature\" : { \"default\" : false , \"rules\" : { \"customer tier equals premium\" : { \"when_match\" : true , \"conditions\" : [ { \"action\" : \"EQUALS\" , \"key\" : \"tier\" , \"value\" : \"premium\" } ] } } }, \"feature2\" : { \"default\" : false } } }","title":"Envelope"},{"location":"utilities/feature_flags/#built-in-store-provider","text":"Info For GA, you'll be able to bring your own store.","title":"Built-in store provider"},{"location":"utilities/feature_flags/#appconfig","text":"AppConfig store provider fetches any JSON document from AWS AppConfig. These are the available options for further customization. Parameter Default Description environment \"\" AWS AppConfig Environment, e.g. test application \"\" AWS AppConfig Application name \"\" AWS AppConfig Configuration name envelope None JMESPath expression to use to extract feature flags configuration from AWS AppConfig configuration max_age 5 Number of seconds to cache feature flags configuration fetched from AWS AppConfig sdk_config None Botocore Config object jmespath_options None For advanced use cases when you want to bring your own JMESPath functions logger logging.Logger Logger to use for debug. You can optionally supply an instance of Powertools Logger. AppConfigStore sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from botocore.config import Config import jmespath boto_config = Config ( read_timeout = 10 , retries = { \"total_max_attempts\" : 2 }) # Custom JMESPath functions class CustomFunctions ( jmespath . functions . Functions ): @jmespath . functions . signature ({ 'types' : [ 'string' ]}) def _func_special_decoder ( self , s ): return my_custom_decoder_logic ( s ) custom_jmespath_options = { \"custom_functions\" : CustomFunctions ()} app_config = AppConfigStore ( environment = \"dev\" , application = \"product-catalogue\" , name = \"configuration\" , max_age = 120 , envelope = \"features\" , sdk_config = boto_config , jmespath_options = custom_jmespath_options )","title":"AppConfig"},{"location":"utilities/feature_flags/#testing-your-code","text":"You can unit test your feature flags locally and independently without setting up AWS AppConfig. AppConfigStore only fetches a JSON document with a specific schema. This allows you to mock the response and use it to verify the rule evaluation. Warning This excerpt relies on pytest and pytest-mock dependencies. Unit testing feature flags 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 from typing import Dict , List , Optional from aws_lambda_powertools.utilities.feature_flags import FeatureFlags , AppConfigStore , RuleAction def init_feature_flags ( mocker , mock_schema , envelope = \"\" ) -> FeatureFlags : \"\"\"Mock AppConfig Store get_configuration method to use mock schema instead\"\"\" method_to_mock = \"aws_lambda_powertools.utilities.feature_flags.AppConfigStore.get_configuration\" mocked_get_conf = mocker . patch ( method_to_mock ) mocked_get_conf . return_value = mock_schema app_conf_store = AppConfigStore ( environment = \"test_env\" , application = \"test_app\" , name = \"test_conf_name\" , envelope = envelope , ) return FeatureFlags ( store = app_conf_store ) def test_flags_condition_match ( mocker ): # GIVEN expected_value = True mocked_app_config_schema = { \"my_feature\" : { \"default\" : expected_value , \"rules\" : { \"tenant id equals 12345\" : { \"when_match\" : True , \"conditions\" : [ { \"action\" : RuleAction . EQUALS . value , \"key\" : \"tenant_id\" , \"value\" : \"12345\" , } ], } }, } } # WHEN ctx = { \"tenant_id\" : \"12345\" , \"username\" : \"a\" } feature_flags = init_feature_flags ( mocker = mocker , mock_schema = mocked_app_config_schema ) flag = feature_flags . evaluate ( name = \"my_feature\" , context = ctx , default = False ) # THEN assert flag == expected_value","title":"Testing your code"},{"location":"utilities/feature_flags/#feature-flags-vs-parameters-vs-env-vars","text":"Method When to use Requires new deployment on changes Supported services Environment variables Simple configuration that will rarely if ever change, because changing it requires a Lambda function deployment. Yes Lambda Parameters utility Access to secrets, or fetch parameters in different formats from AWS System Manager Parameter Store or Amazon DynamoDB. No Parameter Store, DynamoDB, Secrets Manager, AppConfig Feature flags utility Rule engine to define when one or multiple features should be enabled depending on the input. No AppConfig","title":"Feature flags vs Parameters vs env vars"},{"location":"utilities/feature_flags/#deprecation-list-when-ga","text":"Breaking change Recommendation IN RuleAction Use KEY_IN_VALUE instead NOT_IN RuleAction Use KEY_NOT_IN_VALUE instead get_enabled_features Return type changes from List[str] to Dict[str, Any] . New return will contain a list of features enabled and their values. List of enabled features will be in enabled_features key to keep ease of assertion we have in Beta. boolean_type Schema This might not be necessary anymore before we go GA. We will return either the default value when there are no rules as well as when_match value. This will simplify on-boarding if we can keep the same set of validations already offered.","title":"Deprecation list when GA"},{"location":"utilities/idempotency/","text":"The idempotency utility provides a simple solution to convert your Lambda functions into idempotent operations which are safe to retry. Terminology \u00b6 The property of idempotency means that an operation does not cause additional side effects if it is called more than once with the same input parameters. Idempotent operations will return the same result when they are called multiple times with the same parameters . This makes idempotent operations safe to retry. Idempotency key is a hash representation of either the entire event or a specific configured subset of the event, and invocation results are JSON serialized and stored in your persistence storage layer. Key features \u00b6 Prevent Lambda handler from executing more than once on the same event payload during a time window Ensure Lambda handler returns the same result when called with the same payload Select a subset of the event as the idempotency key using JMESPath expressions Set a time window in which records with the same payload should be considered duplicates Getting started \u00b6 Required resources \u00b6 Before getting started, you need to create a persistent storage layer where the idempotency utility can store its state - your lambda functions will need read and write access to it. As of now, Amazon DynamoDB is the only supported persistent storage layer, so you'll need to create a table first. Default table configuration If you're not changing the default configuration for the DynamoDB persistence layer , this is the expected default configuration: Configuration Value Notes Partition key id TTL attribute name expiration This can only be configured after your table is created if you're using AWS Console Tip: You can share a single state table for all functions You can reuse the same DynamoDB table to store idempotency state. We add your function_name in addition to the idempotency key as a hash key. AWS Serverless Application Model (SAM) example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Resources : IdempotencyTable : Type : AWS::DynamoDB::Table Properties : AttributeDefinitions : - AttributeName : id AttributeType : S KeySchema : - AttributeName : id KeyType : HASH TimeToLiveSpecification : AttributeName : expiration Enabled : true BillingMode : PAY_PER_REQUEST HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 ... Policies : - DynamoDBCrudPolicy : TableName : !Ref IdempotencyTable Warning: Large responses with DynamoDB persistence layer When using this utility with DynamoDB, your function's responses must be smaller than 400KB . Larger items cannot be written to DynamoDB and will cause exceptions. Info: DynamoDB Each function invocation will generally make 2 requests to DynamoDB. If the result returned by your Lambda is less than 1kb, you can expect 2 WCUs per invocation. For retried invocations, you will see 1WCU and 1RCU. Review the DynamoDB pricing documentation to estimate the cost. Idempotent decorator \u00b6 You can quickly start by initializing the DynamoDBPersistenceLayer class and using it with the idempotent decorator on your lambda handler. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): payment = create_subscription_payment ( user = event [ 'user' ], product = event [ 'product_id' ] ) ... return { \"payment_id\" : payment . id , \"message\" : \"success\" , \"statusCode\" : 200 , } Example event 1 2 3 4 { \"username\" : \"xyz\" , \"product_id\" : \"123456789\" } Idempotent_function decorator \u00b6 Similar to idempotent decorator , you can use idempotent_function decorator for any synchronous Python function. When using idempotent_function , you must tell us which keyword parameter in your function signature has the data we should use via data_keyword_argument . We support JSON serializable data, Python Dataclasses , Parser/Pydantic Models , and our Event Source Data Classes . Warning Make sure to call your decorated function using keyword arguments batch_sample.py This example also demonstrates how you can integrate with Batch utility , so you can process each record in an idempotent manner. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from aws_lambda_powertools.utilities.batch import ( BatchProcessor , EventType , batch_processor ) from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , IdempotencyConfig , idempotent_function ) processor = BatchProcessor ( event_type = EventType . SQS ) dynamodb = DynamoDBPersistenceLayer ( table_name = \"idem\" ) config = IdempotencyConfig ( event_key_jmespath = \"messageId\" , # see Choosing a payload subset section use_local_cache = True , ) @idempotent_function ( data_keyword_argument = \"record\" , config = config , persistence_store = dynamodb ) def record_handler ( record : SQSRecord ): return { \"message\" : record [ \"body\" ]} @idempotent_function ( data_keyword_argument = \"data\" , config = config , persistence_store = dynamodb ) def dummy ( arg_one , arg_two , data : dict , ** kwargs ): return { \"data\" : data } @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context ): # `data` parameter must be called as a keyword argument to work dummy ( \"hello\" , \"universe\" , data = \"test\" ) return processor . response () Batch event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \"Records\" : [ { \"messageId\" : \"059f36b4-87a3-44ab-83d2-661975830a7d\" , \"receiptHandle\" : \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...\" , \"body\" : \"Test message.\" , \"attributes\" : { \"ApproximateReceiveCount\" : \"1\" , \"SentTimestamp\" : \"1545082649183\" , \"SenderId\" : \"AIDAIENQZJOLO23YVJ4VO\" , \"ApproximateFirstReceiveTimestamp\" : \"1545082649185\" }, \"messageAttributes\" : { \"testAttr\" : { \"stringValue\" : \"100\" , \"binaryValue\" : \"base64Str\" , \"dataType\" : \"Number\" } }, \"md5OfBody\" : \"e4e68fb7bd0e697a0ae8f1bb342846b3\" , \"eventSource\" : \"aws:sqs\" , \"eventSourceARN\" : \"arn:aws:sqs:us-east-2:123456789012:my-queue\" , \"awsRegion\" : \"us-east-2\" } ] } dataclass_sample.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from dataclasses import dataclass from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , IdempotencyConfig , idempotent_function ) dynamodb = DynamoDBPersistenceLayer ( table_name = \"idem\" ) config = IdempotencyConfig ( event_key_jmespath = \"order_id\" , # see Choosing a payload subset section use_local_cache = True , ) @dataclass class OrderItem : sku : str description : str @dataclass class Order : item : OrderItem order_id : int @idempotent_function ( data_keyword_argument = \"order\" , config = config , persistence_store = dynamodb ) def process_order ( order : Order ): return f \"processed order { order . order_id } \" order_item = OrderItem ( sku = \"fake\" , description = \"sample\" ) order = Order ( item = order_item , order_id = \"fake-id\" ) # `order` parameter must be called as a keyword argument to work process_order ( order = order ) parser_pydantic_sample.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , IdempotencyConfig , idempotent_function ) from aws_lambda_powertools.utilities.parser import BaseModel dynamodb = DynamoDBPersistenceLayer ( table_name = \"idem\" ) config = IdempotencyConfig ( event_key_jmespath = \"order_id\" , # see Choosing a payload subset section use_local_cache = True , ) class OrderItem ( BaseModel ): sku : str description : str class Order ( BaseModel ): item : OrderItem order_id : int @idempotent_function ( data_keyword_argument = \"order\" , config = config , persistence_store = dynamodb ) def process_order ( order : Order ): return f \"processed order { order . order_id } \" order_item = OrderItem ( sku = \"fake\" , description = \"sample\" ) order = Order ( item = order_item , order_id = \"fake-id\" ) # `order` parameter must be called as a keyword argument to work process_order ( order = order ) Choosing a payload subset for idempotency \u00b6 Tip: Dealing with always changing payloads When dealing with a more elaborate payload, where parts of the payload always change, you should use event_key_jmespath parameter. Use IdempotencyConfig to instruct the idempotent decorator to only use a portion of your payload to verify whether a request is idempotent, and therefore it should not be retried. Payment scenario In this example, we have a Lambda handler that creates a payment for a user subscribing to a product. We want to ensure that we don't accidentally charge our customer by subscribing them more than once. Imagine the function executes successfully, but the client never receives the response due to a connection issue. It is safe to retry in this instance, as the idempotent decorator will return a previously saved response. Warning: Idempotency for JSON payloads The payload extracted by the event_key_jmespath is treated as a string by default, so will be sensitive to differences in whitespace even when the JSON payload itself is identical. To alter this behaviour, we can use the JMESPath built-in function powertools_json() to treat the payload as a JSON object (dict) rather than a string. payment.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import json from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) # Treat everything under the \"body\" key # in the event json object as our payload config = IdempotencyConfig ( event_key_jmespath = \"powertools_json(body)\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): body = json . loads ( event [ 'body' ]) payment = create_subscription_payment ( user = body [ 'user' ], product = body [ 'product_id' ] ) ... return { \"payment_id\" : payment . id , \"message\" : \"success\" , \"statusCode\" : 200 } Example event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { \"version\" : \"2.0\" , \"routeKey\" : \"ANY /createpayment\" , \"rawPath\" : \"/createpayment\" , \"rawQueryString\" : \"\" , \"headers\" : { \"Header1\" : \"value1\" , \"Header2\" : \"value2\" }, \"requestContext\" :{ \"accountId\" : \"123456789012\" , \"apiId\" : \"api-id\" , \"domainName\" : \"id.execute-api.us-east-1.amazonaws.com\" , \"domainPrefix\" : \"id\" , \"http\" :{ \"method\" : \"POST\" , \"path\" : \"/createpayment\" , \"protocol\" : \"HTTP/1.1\" , \"sourceIp\" : \"ip\" , \"userAgent\" : \"agent\" }, \"requestId\" : \"id\" , \"routeKey\" : \"ANY /createpayment\" , \"stage\" : \"$default\" , \"time\" : \"10/Feb/2021:13:40:43 +0000\" , \"timeEpoch\" : 1612964443723 }, \"body\" : \"{\\\"user\\\":\\\"xyz\\\",\\\"product_id\\\":\\\"123456789\\\"}\" , \"isBase64Encoded\" : false } Idempotency request flow \u00b6 This sequence diagram shows an example flow of what happens in the payment scenario: The client was successful in receiving the result after the retry. Since the Lambda handler was only executed once, our customer hasn't been charged twice. Note Bear in mind that the entire Lambda handler is treated as a single idempotent operation. If your Lambda handler can cause multiple side effects, consider splitting it into separate functions. Handling exceptions \u00b6 If you are using the idempotent decorator on your Lambda handler, any unhandled exceptions that are raised during the code execution will cause the record in the persistence layer to be deleted . This means that new invocations will execute your code again despite having the same payload. If you don't want the record to be deleted, you need to catch exceptions within the idempotent function and return a successful response. If you are using idempotent_function , any unhandled exceptions that are raised inside the decorated function will cause the record in the persistence layer to be deleted, and allow the function to be executed again if retried. If an Exception is raised outside the scope of the decorated function and after your function has been called, the persistent record will not be affected. In this case, idempotency will be maintained for your decorated function. Example: Exception not affecting idempotency record sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def lambda_handler ( event , context ): # If an exception is raised here, no idempotent record will ever get created as the # idempotent function does not get called do_some_stuff () result = call_external_service ( data = { \"user\" : \"user1\" , \"id\" : 5 }) # This exception will not cause the idempotent record to be deleted, since it # happens after the decorated function has been successfully called raise Exception @idempotent_function ( data_keyword_argument = \"data\" , config = config , persistence_store = dynamodb ) def call_external_service ( data : dict , ** kwargs ): result = requests . post ( 'http://example.com' , json = { \"user\" : data [ 'user' ], \"transaction_id\" : data [ 'id' ]} return result . json () Warning We will raise IdempotencyPersistenceLayerError if any of the calls to the persistence layer fail unexpectedly. As this happens outside the scope of your decorated function, you are not able to catch it if you're using the idempotent decorator on your Lambda handler. Persistence layers \u00b6 DynamoDBPersistenceLayer \u00b6 This persistence layer is built-in, and you can either use an existing DynamoDB table or create a new one dedicated for idempotency state (recommended). Customizing DynamoDBPersistenceLayer to suit your table structure 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.idempotency import DynamoDBPersistenceLayer persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , key_attr = \"idempotency_key\" , expiry_attr = \"expires_at\" , status_attr = \"current_status\" , data_attr = \"result_data\" , validation_key_attr = \"validation_key\" , ) When using DynamoDB as a persistence layer, you can alter the attribute names by passing these parameters when initializing the persistence layer: Parameter Required Default Description table_name Table name to store state key_attr id Partition key of the table. Hashed representation of the payload (unless sort_key_attr is specified) expiry_attr expiration Unix timestamp of when record expires status_attr status Stores status of the lambda execution during and after invocation data_attr data Stores results of successfully executed Lambda handlers validation_key_attr validation Hashed representation of the parts of the event used for validation sort_key_attr Sort key of the table (if table is configured with a sort key). static_pk_value idempotency#{LAMBDA_FUNCTION_NAME} Static value to use as the partition key. Only used when sort_key_attr is set. Advanced \u00b6 Customizing the default behavior \u00b6 Idempotent decorator can be further configured with IdempotencyConfig as seen in the previous example. These are the available options for further configuration Parameter Default Description event_key_jmespath \"\" JMESPath expression to extract the idempotency key from the event record using built-in functions payload_validation_jmespath \"\" JMESPath expression to validate whether certain parameters have changed in the event while the event payload raise_on_no_idempotency_key False Raise exception if no idempotency key was found in the request expires_after_seconds 3600 The number of seconds to wait before a record is expired use_local_cache False Whether to locally cache idempotency results local_cache_max_items 256 Max number of items to store in local cache hash_function md5 Function to use for calculating hashes, as provided by hashlib in the standard library. Handling concurrent executions with the same payload \u00b6 This utility will raise an IdempotencyAlreadyInProgressError exception if you receive multiple invocations with the same payload while the first invocation hasn't completed yet . Info If you receive IdempotencyAlreadyInProgressError , you can safely retry the operation. This is a locking mechanism for correctness. Since we don't know the result from the first invocation yet, we can't safely allow another concurrent execution. Using in-memory cache \u00b6 By default, in-memory local caching is disabled , since we don't know how much memory you consume per invocation compared to the maximum configured in your Lambda function. Note: This in-memory cache is local to each Lambda execution environment This means it will be effective in cases where your function's concurrency is low in comparison to the number of \"retry\" invocations with the same payload, because cache might be empty. You can enable in-memory caching with the use_local_cache parameter: Caching idempotent transactions in-memory to prevent multiple calls to storage 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) config = IdempotencyConfig ( event_key_jmespath = \"body\" , use_local_cache = True , ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... When enabled, the default is to cache a maximum of 256 records in each Lambda execution environment - You can change it with the local_cache_max_items parameter. Expiring idempotency records \u00b6 Note By default, we expire idempotency records after an hour (3600 seconds). In most cases, it is not desirable to store the idempotency records forever. Rather, you want to guarantee that the same payload won't be executed within a period of time. You can change this window with the expires_after_seconds parameter: Adjusting cache TTL 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) config = IdempotencyConfig ( event_key_jmespath = \"body\" , expires_after_seconds = 5 * 60 , # 5 minutes ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... This will mark any records older than 5 minutes as expired, and the lambda handler will be executed as normal if it is invoked with a matching payload. Note: DynamoDB time-to-live field This utility uses expiration as the TTL field in DynamoDB, as demonstrated in the SAM example earlier . Payload validation \u00b6 Question: What if your function is invoked with the same payload except some outer parameters have changed? Example: A payment transaction for a given productID was requested twice for the same customer, however the amount to be paid has changed in the second transaction . By default, we will return the same result as it returned before, however in this instance it may be misleading; we provide a fail fast payload validation to address this edge case. With payload_validation_jmespath , you can provide an additional JMESPath expression to specify which part of the event body should be validated against previous idempotent invocations app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"[userDetail, productId]\" , payload_validation_jmespath = \"amount\" ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): # Creating a subscription payment is a side # effect of calling this function! payment = create_subscription_payment ( user = event [ 'userDetail' ][ 'username' ], product = event [ 'product_id' ], amount = event [ 'amount' ] ) ... return { \"message\" : \"success\" , \"statusCode\" : 200 , \"payment_id\" : payment . id , \"amount\" : payment . amount } Example Event 1 1 2 3 4 5 6 7 8 9 { \"userDetail\" : { \"username\" : \"User1\" , \"user_email\" : \"user@example.com\" }, \"productId\" : 1500 , \"charge_type\" : \"subscription\" , \"amount\" : 500 } Example Event 2 1 2 3 4 5 6 7 8 9 { \"userDetail\" : { \"username\" : \"User1\" , \"user_email\" : \"user@example.com\" }, \"productId\" : 1500 , \"charge_type\" : \"subscription\" , \"amount\" : 1 } In this example, the userDetail and productId keys are used as the payload to generate the idempotency key, as per event_key_jmespath parameter. Note If we try to send the same request but with a different amount, we will raise IdempotencyValidationError . Without payload validation, we would have returned the same result as we did for the initial request. Since we're also returning an amount in the response, this could be quite confusing for the client. By using payload_validation_jmespath=\"amount\" , we prevent this potentially confusing behavior and instead raise an Exception. Making idempotency key required \u00b6 If you want to enforce that an idempotency key is required, you can set raise_on_no_idempotency_key to True . This means that we will raise IdempotencyKeyError if the evaluation of event_key_jmespath is None . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) # Requires \"user\".\"uid\" and \"order_id\" to be present config = IdempotencyConfig ( event_key_jmespath = \"[user.uid, order_id]\" , raise_on_no_idempotency_key = True , ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): pass Success Event 1 2 3 4 5 6 7 { \"user\" : { \"uid\" : \"BB0D045C-8878-40C8-889E-38B3CB0A61B1\" , \"name\" : \"Foo\" }, \"order_id\" : 10000 } Failure Event Notice that order_id is now accidentally within user key 1 2 3 4 5 6 7 { \"user\" : { \"uid\" : \"DE0D000E-1234-10D1-991E-EAC1DD1D52C8\" , \"name\" : \"Joe Bloggs\" , \"order_id\" : 10000 }, } Customizing boto configuration \u00b6 The boto_config and boto3_session parameters enable you to pass in a custom botocore config object or a custom boto3 session when constructing the persistence store. Custom session 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import boto3 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) boto3_session = boto3 . session . Session () persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , boto3_session = boto3_session ) config = IdempotencyConfig ( event_key_jmespath = \"body\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... Custom config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from botocore.config import Config from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"body\" ) boto_config = Config () persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , boto_config = boto_config ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... Using a DynamoDB table with a composite primary key \u00b6 When using a composite primary key table (hash+range key), use sort_key_attr parameter when initializing your persistence layer. With this setting, we will save the idempotency key in the sort key instead of the primary key. By default, the primary key will now be set to idempotency#{LAMBDA_FUNCTION_NAME} . You can optionally set a static value for the partition key using the static_pk_value parameter. Reusing a DynamoDB table that uses a composite primary key 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.idempotency import DynamoDBPersistenceLayer , idempotent persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , sort_key_attr = 'sort_key' ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): return { \"message\" : \"success\" : \"id\" : event [ 'body' ][ 'id]} The example function above would cause data to be stored in DynamoDB like this: id sort_key expiration status data idempotency#MyLambdaFunction 1e956ef7da78d0cb890be999aecc0c9e 1636549553 COMPLETED {\"id\": 12391, \"message\": \"success\"} idempotency#MyLambdaFunction 2b2cdb5f86361e97b4383087c1ffdf27 1636549571 COMPLETED {\"id\": 527212, \"message\": \"success\"} idempotency#MyLambdaFunction f091d2527ad1c78f05d54cc3f363be80 1636549585 IN_PROGRESS Bring your own persistent store \u00b6 This utility provides an abstract base class (ABC), so that you can implement your choice of persistent storage layer. You can inherit from the BasePersistenceLayer class and implement the abstract methods _get_record , _put_record , _update_record and _delete_record . Excerpt DynamoDB Persisntence Layer implementation for reference 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 import datetime import logging from typing import Any , Dict , Optional import boto3 from botocore.config import Config from aws_lambda_powertools.utilities.idempotency import BasePersistenceLayer from aws_lambda_powertools.utilities.idempotency.exceptions import ( IdempotencyItemAlreadyExistsError , IdempotencyItemNotFoundError , ) from aws_lambda_powertools.utilities.idempotency.persistence.base import DataRecord logger = logging . getLogger ( __name__ ) class DynamoDBPersistenceLayer ( BasePersistenceLayer ): def __init__ ( self , table_name : str , key_attr : str = \"id\" , expiry_attr : str = \"expiration\" , status_attr : str = \"status\" , data_attr : str = \"data\" , validation_key_attr : str = \"validation\" , boto_config : Optional [ Config ] = None , boto3_session : Optional [ boto3 . session . Session ] = None , ): boto_config = boto_config or Config () session = boto3_session or boto3 . session . Session () self . _ddb_resource = session . resource ( \"dynamodb\" , config = boto_config ) self . table_name = table_name self . table = self . _ddb_resource . Table ( self . table_name ) self . key_attr = key_attr self . expiry_attr = expiry_attr self . status_attr = status_attr self . data_attr = data_attr self . validation_key_attr = validation_key_attr super ( DynamoDBPersistenceLayer , self ) . __init__ () def _item_to_data_record ( self , item : Dict [ str , Any ]) -> DataRecord : \"\"\" Translate raw item records from DynamoDB to DataRecord Parameters ---------- item: Dict[str, Union[str, int]] Item format from dynamodb response Returns ------- DataRecord representation of item \"\"\" return DataRecord ( idempotency_key = item [ self . key_attr ], status = item [ self . status_attr ], expiry_timestamp = item [ self . expiry_attr ], response_data = item . get ( self . data_attr ), payload_hash = item . get ( self . validation_key_attr ), ) def _get_record ( self , idempotency_key ) -> DataRecord : response = self . table . get_item ( Key = { self . key_attr : idempotency_key }, ConsistentRead = True ) try : item = response [ \"Item\" ] except KeyError : raise IdempotencyItemNotFoundError return self . _item_to_data_record ( item ) def _put_record ( self , data_record : DataRecord ) -> None : item = { self . key_attr : data_record . idempotency_key , self . expiry_attr : data_record . expiry_timestamp , self . status_attr : data_record . status , } if self . payload_validation_enabled : item [ self . validation_key_attr ] = data_record . payload_hash now = datetime . datetime . now () try : logger . debug ( f \"Putting record for idempotency key: { data_record . idempotency_key } \" ) self . table . put_item ( Item = item , ConditionExpression = f \"attribute_not_exists( { self . key_attr } ) OR { self . expiry_attr } < :now\" , ExpressionAttributeValues = { \":now\" : int ( now . timestamp ())}, ) except self . _ddb_resource . meta . client . exceptions . ConditionalCheckFailedException : logger . debug ( f \"Failed to put record for already existing idempotency key: { data_record . idempotency_key } \" ) raise IdempotencyItemAlreadyExistsError def _update_record ( self , data_record : DataRecord ): logger . debug ( f \"Updating record for idempotency key: { data_record . idempotency_key } \" ) update_expression = \"SET #response_data = :response_data, #expiry = :expiry, #status = :status\" expression_attr_values = { \":expiry\" : data_record . expiry_timestamp , \":response_data\" : data_record . response_data , \":status\" : data_record . status , } expression_attr_names = { \"#response_data\" : self . data_attr , \"#expiry\" : self . expiry_attr , \"#status\" : self . status_attr , } if self . payload_validation_enabled : update_expression += \", #validation_key = :validation_key\" expression_attr_values [ \":validation_key\" ] = data_record . payload_hash expression_attr_names [ \"#validation_key\" ] = self . validation_key_attr kwargs = { \"Key\" : { self . key_attr : data_record . idempotency_key }, \"UpdateExpression\" : update_expression , \"ExpressionAttributeValues\" : expression_attr_values , \"ExpressionAttributeNames\" : expression_attr_names , } self . table . update_item ( ** kwargs ) def _delete_record ( self , data_record : DataRecord ) -> None : logger . debug ( f \"Deleting record for idempotency key: { data_record . idempotency_key } \" ) self . table . delete_item ( Key = { self . key_attr : data_record . idempotency_key },) Danger Pay attention to the documentation for each - you may need to perform additional checks inside these methods to ensure the idempotency guarantees remain intact. For example, the _put_record method needs to raise an exception if a non-expired record already exists in the data store with a matching key. Compatibility with other utilities \u00b6 Validation utility \u00b6 The idempotency utility can be used with the validator decorator. Ensure that idempotency is the innermost decorator. Warning If you use an envelope with the validator, the event received by the idempotency utility will be the unwrapped event - not the \"raw\" event Lambda was invoked with. Make sure to account for this behaviour, if you set the event_key_jmespath . Using Idempotency with JSONSchema Validation utility 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.validation import validator , envelopes from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"[message, username]\" ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @validator ( envelope = envelopes . API_GATEWAY_HTTP ) @idempotent ( config = config , persistence_store = persistence_layer ) def lambda_handler ( event , context ): cause_some_side_effects ( event [ 'username' ) return { \"message\" : event [ 'message' ], \"statusCode\" : 200 } Tip: JMESPath Powertools functions are also available Built-in functions known in the validation utility like powertools_json , powertools_base64 , powertools_base64_gzip are also available to use in this utility. Testing your code \u00b6 The idempotency utility provides several routes to test your code. Disabling the idempotency utility \u00b6 When testing your code, you may wish to disable the idempotency logic altogether and focus on testing your business logic. To do this, you can set the environment variable POWERTOOLS_IDEMPOTENCY_DISABLED with a truthy value. If you prefer setting this for specific tests, and are using Pytest, you can use monkeypatch fixture: tests.py 1 2 3 4 5 6 def test_idempotent_lambda_handler ( monkeypatch ): # Set POWERTOOLS_IDEMPOTENCY_DISABLED before calling decorated functions monkeypatch . setenv ( \"POWERTOOLS_IDEMPOTENCY_DISABLED\" , 1 ) result = handler () ... app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"idempotency\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): print ( 'expensive operation' ) return { \"payment_id\" : 12345 , \"message\" : \"success\" , \"statusCode\" : 200 , } Testing with DynamoDB Local \u00b6 To test with DynamoDB Local , you can replace the Table resource used by the persistence layer with one you create inside your tests. This allows you to set the endpoint_url. tests.py 1 2 3 4 5 6 7 8 9 10 11 12 import boto3 import app def test_idempotent_lambda (): # Create our own Table resource using the endpoint for our DynamoDB Local instance resource = boto3 . resource ( \"dynamodb\" , endpoint_url = 'http://localhost:8000' ) table = resource . Table ( app . persistence_layer . table_name ) app . persistence_layer . table = table result = app . handler ({ 'testkey' : 'testvalue' }, {}) assert result [ 'payment_id' ] == 12345 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"idempotency\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): print ( 'expensive operation' ) return { \"payment_id\" : 12345 , \"message\" : \"success\" , \"statusCode\" : 200 , } How do I mock all DynamoDB I/O operations \u00b6 The idempotency utility lazily creates the dynamodb Table which it uses to access DynamoDB. This means it is possible to pass a mocked Table resource, or stub various methods. tests.py 1 2 3 4 5 6 7 8 9 10 from unittest.mock import MagicMock import app def test_idempotent_lambda (): table = MagicMock () app . persistence_layer . table = table result = app . handler ({ 'testkey' : 'testvalue' }, {}) table . put_item . assert_called () ... app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"idempotency\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): print ( 'expensive operation' ) return { \"payment_id\" : 12345 , \"message\" : \"success\" , \"statusCode\" : 200 , } Extra resources \u00b6 If you're interested in a deep dive on how Amazon uses idempotency when building our APIs, check out this article .","title":"Idempotency"},{"location":"utilities/idempotency/#terminology","text":"The property of idempotency means that an operation does not cause additional side effects if it is called more than once with the same input parameters. Idempotent operations will return the same result when they are called multiple times with the same parameters . This makes idempotent operations safe to retry. Idempotency key is a hash representation of either the entire event or a specific configured subset of the event, and invocation results are JSON serialized and stored in your persistence storage layer.","title":"Terminology"},{"location":"utilities/idempotency/#key-features","text":"Prevent Lambda handler from executing more than once on the same event payload during a time window Ensure Lambda handler returns the same result when called with the same payload Select a subset of the event as the idempotency key using JMESPath expressions Set a time window in which records with the same payload should be considered duplicates","title":"Key features"},{"location":"utilities/idempotency/#getting-started","text":"","title":"Getting started"},{"location":"utilities/idempotency/#required-resources","text":"Before getting started, you need to create a persistent storage layer where the idempotency utility can store its state - your lambda functions will need read and write access to it. As of now, Amazon DynamoDB is the only supported persistent storage layer, so you'll need to create a table first. Default table configuration If you're not changing the default configuration for the DynamoDB persistence layer , this is the expected default configuration: Configuration Value Notes Partition key id TTL attribute name expiration This can only be configured after your table is created if you're using AWS Console Tip: You can share a single state table for all functions You can reuse the same DynamoDB table to store idempotency state. We add your function_name in addition to the idempotency key as a hash key. AWS Serverless Application Model (SAM) example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Resources : IdempotencyTable : Type : AWS::DynamoDB::Table Properties : AttributeDefinitions : - AttributeName : id AttributeType : S KeySchema : - AttributeName : id KeyType : HASH TimeToLiveSpecification : AttributeName : expiration Enabled : true BillingMode : PAY_PER_REQUEST HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 ... Policies : - DynamoDBCrudPolicy : TableName : !Ref IdempotencyTable Warning: Large responses with DynamoDB persistence layer When using this utility with DynamoDB, your function's responses must be smaller than 400KB . Larger items cannot be written to DynamoDB and will cause exceptions. Info: DynamoDB Each function invocation will generally make 2 requests to DynamoDB. If the result returned by your Lambda is less than 1kb, you can expect 2 WCUs per invocation. For retried invocations, you will see 1WCU and 1RCU. Review the DynamoDB pricing documentation to estimate the cost.","title":"Required resources"},{"location":"utilities/idempotency/#idempotent-decorator","text":"You can quickly start by initializing the DynamoDBPersistenceLayer class and using it with the idempotent decorator on your lambda handler. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): payment = create_subscription_payment ( user = event [ 'user' ], product = event [ 'product_id' ] ) ... return { \"payment_id\" : payment . id , \"message\" : \"success\" , \"statusCode\" : 200 , } Example event 1 2 3 4 { \"username\" : \"xyz\" , \"product_id\" : \"123456789\" }","title":"Idempotent decorator"},{"location":"utilities/idempotency/#idempotent_function-decorator","text":"Similar to idempotent decorator , you can use idempotent_function decorator for any synchronous Python function. When using idempotent_function , you must tell us which keyword parameter in your function signature has the data we should use via data_keyword_argument . We support JSON serializable data, Python Dataclasses , Parser/Pydantic Models , and our Event Source Data Classes . Warning Make sure to call your decorated function using keyword arguments batch_sample.py This example also demonstrates how you can integrate with Batch utility , so you can process each record in an idempotent manner. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from aws_lambda_powertools.utilities.batch import ( BatchProcessor , EventType , batch_processor ) from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , IdempotencyConfig , idempotent_function ) processor = BatchProcessor ( event_type = EventType . SQS ) dynamodb = DynamoDBPersistenceLayer ( table_name = \"idem\" ) config = IdempotencyConfig ( event_key_jmespath = \"messageId\" , # see Choosing a payload subset section use_local_cache = True , ) @idempotent_function ( data_keyword_argument = \"record\" , config = config , persistence_store = dynamodb ) def record_handler ( record : SQSRecord ): return { \"message\" : record [ \"body\" ]} @idempotent_function ( data_keyword_argument = \"data\" , config = config , persistence_store = dynamodb ) def dummy ( arg_one , arg_two , data : dict , ** kwargs ): return { \"data\" : data } @batch_processor ( record_handler = record_handler , processor = processor ) def lambda_handler ( event , context ): # `data` parameter must be called as a keyword argument to work dummy ( \"hello\" , \"universe\" , data = \"test\" ) return processor . response () Batch event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \"Records\" : [ { \"messageId\" : \"059f36b4-87a3-44ab-83d2-661975830a7d\" , \"receiptHandle\" : \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...\" , \"body\" : \"Test message.\" , \"attributes\" : { \"ApproximateReceiveCount\" : \"1\" , \"SentTimestamp\" : \"1545082649183\" , \"SenderId\" : \"AIDAIENQZJOLO23YVJ4VO\" , \"ApproximateFirstReceiveTimestamp\" : \"1545082649185\" }, \"messageAttributes\" : { \"testAttr\" : { \"stringValue\" : \"100\" , \"binaryValue\" : \"base64Str\" , \"dataType\" : \"Number\" } }, \"md5OfBody\" : \"e4e68fb7bd0e697a0ae8f1bb342846b3\" , \"eventSource\" : \"aws:sqs\" , \"eventSourceARN\" : \"arn:aws:sqs:us-east-2:123456789012:my-queue\" , \"awsRegion\" : \"us-east-2\" } ] } dataclass_sample.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from dataclasses import dataclass from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , IdempotencyConfig , idempotent_function ) dynamodb = DynamoDBPersistenceLayer ( table_name = \"idem\" ) config = IdempotencyConfig ( event_key_jmespath = \"order_id\" , # see Choosing a payload subset section use_local_cache = True , ) @dataclass class OrderItem : sku : str description : str @dataclass class Order : item : OrderItem order_id : int @idempotent_function ( data_keyword_argument = \"order\" , config = config , persistence_store = dynamodb ) def process_order ( order : Order ): return f \"processed order { order . order_id } \" order_item = OrderItem ( sku = \"fake\" , description = \"sample\" ) order = Order ( item = order_item , order_id = \"fake-id\" ) # `order` parameter must be called as a keyword argument to work process_order ( order = order ) parser_pydantic_sample.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , IdempotencyConfig , idempotent_function ) from aws_lambda_powertools.utilities.parser import BaseModel dynamodb = DynamoDBPersistenceLayer ( table_name = \"idem\" ) config = IdempotencyConfig ( event_key_jmespath = \"order_id\" , # see Choosing a payload subset section use_local_cache = True , ) class OrderItem ( BaseModel ): sku : str description : str class Order ( BaseModel ): item : OrderItem order_id : int @idempotent_function ( data_keyword_argument = \"order\" , config = config , persistence_store = dynamodb ) def process_order ( order : Order ): return f \"processed order { order . order_id } \" order_item = OrderItem ( sku = \"fake\" , description = \"sample\" ) order = Order ( item = order_item , order_id = \"fake-id\" ) # `order` parameter must be called as a keyword argument to work process_order ( order = order )","title":"Idempotent_function decorator"},{"location":"utilities/idempotency/#choosing-a-payload-subset-for-idempotency","text":"Tip: Dealing with always changing payloads When dealing with a more elaborate payload, where parts of the payload always change, you should use event_key_jmespath parameter. Use IdempotencyConfig to instruct the idempotent decorator to only use a portion of your payload to verify whether a request is idempotent, and therefore it should not be retried. Payment scenario In this example, we have a Lambda handler that creates a payment for a user subscribing to a product. We want to ensure that we don't accidentally charge our customer by subscribing them more than once. Imagine the function executes successfully, but the client never receives the response due to a connection issue. It is safe to retry in this instance, as the idempotent decorator will return a previously saved response. Warning: Idempotency for JSON payloads The payload extracted by the event_key_jmespath is treated as a string by default, so will be sensitive to differences in whitespace even when the JSON payload itself is identical. To alter this behaviour, we can use the JMESPath built-in function powertools_json() to treat the payload as a JSON object (dict) rather than a string. payment.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import json from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) # Treat everything under the \"body\" key # in the event json object as our payload config = IdempotencyConfig ( event_key_jmespath = \"powertools_json(body)\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): body = json . loads ( event [ 'body' ]) payment = create_subscription_payment ( user = body [ 'user' ], product = body [ 'product_id' ] ) ... return { \"payment_id\" : payment . id , \"message\" : \"success\" , \"statusCode\" : 200 } Example event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { \"version\" : \"2.0\" , \"routeKey\" : \"ANY /createpayment\" , \"rawPath\" : \"/createpayment\" , \"rawQueryString\" : \"\" , \"headers\" : { \"Header1\" : \"value1\" , \"Header2\" : \"value2\" }, \"requestContext\" :{ \"accountId\" : \"123456789012\" , \"apiId\" : \"api-id\" , \"domainName\" : \"id.execute-api.us-east-1.amazonaws.com\" , \"domainPrefix\" : \"id\" , \"http\" :{ \"method\" : \"POST\" , \"path\" : \"/createpayment\" , \"protocol\" : \"HTTP/1.1\" , \"sourceIp\" : \"ip\" , \"userAgent\" : \"agent\" }, \"requestId\" : \"id\" , \"routeKey\" : \"ANY /createpayment\" , \"stage\" : \"$default\" , \"time\" : \"10/Feb/2021:13:40:43 +0000\" , \"timeEpoch\" : 1612964443723 }, \"body\" : \"{\\\"user\\\":\\\"xyz\\\",\\\"product_id\\\":\\\"123456789\\\"}\" , \"isBase64Encoded\" : false }","title":"Choosing a payload subset for idempotency"},{"location":"utilities/idempotency/#idempotency-request-flow","text":"This sequence diagram shows an example flow of what happens in the payment scenario: The client was successful in receiving the result after the retry. Since the Lambda handler was only executed once, our customer hasn't been charged twice. Note Bear in mind that the entire Lambda handler is treated as a single idempotent operation. If your Lambda handler can cause multiple side effects, consider splitting it into separate functions.","title":"Idempotency request flow"},{"location":"utilities/idempotency/#handling-exceptions","text":"If you are using the idempotent decorator on your Lambda handler, any unhandled exceptions that are raised during the code execution will cause the record in the persistence layer to be deleted . This means that new invocations will execute your code again despite having the same payload. If you don't want the record to be deleted, you need to catch exceptions within the idempotent function and return a successful response. If you are using idempotent_function , any unhandled exceptions that are raised inside the decorated function will cause the record in the persistence layer to be deleted, and allow the function to be executed again if retried. If an Exception is raised outside the scope of the decorated function and after your function has been called, the persistent record will not be affected. In this case, idempotency will be maintained for your decorated function. Example: Exception not affecting idempotency record sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def lambda_handler ( event , context ): # If an exception is raised here, no idempotent record will ever get created as the # idempotent function does not get called do_some_stuff () result = call_external_service ( data = { \"user\" : \"user1\" , \"id\" : 5 }) # This exception will not cause the idempotent record to be deleted, since it # happens after the decorated function has been successfully called raise Exception @idempotent_function ( data_keyword_argument = \"data\" , config = config , persistence_store = dynamodb ) def call_external_service ( data : dict , ** kwargs ): result = requests . post ( 'http://example.com' , json = { \"user\" : data [ 'user' ], \"transaction_id\" : data [ 'id' ]} return result . json () Warning We will raise IdempotencyPersistenceLayerError if any of the calls to the persistence layer fail unexpectedly. As this happens outside the scope of your decorated function, you are not able to catch it if you're using the idempotent decorator on your Lambda handler.","title":"Handling exceptions"},{"location":"utilities/idempotency/#persistence-layers","text":"","title":"Persistence layers"},{"location":"utilities/idempotency/#dynamodbpersistencelayer","text":"This persistence layer is built-in, and you can either use an existing DynamoDB table or create a new one dedicated for idempotency state (recommended). Customizing DynamoDBPersistenceLayer to suit your table structure 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.idempotency import DynamoDBPersistenceLayer persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , key_attr = \"idempotency_key\" , expiry_attr = \"expires_at\" , status_attr = \"current_status\" , data_attr = \"result_data\" , validation_key_attr = \"validation_key\" , ) When using DynamoDB as a persistence layer, you can alter the attribute names by passing these parameters when initializing the persistence layer: Parameter Required Default Description table_name Table name to store state key_attr id Partition key of the table. Hashed representation of the payload (unless sort_key_attr is specified) expiry_attr expiration Unix timestamp of when record expires status_attr status Stores status of the lambda execution during and after invocation data_attr data Stores results of successfully executed Lambda handlers validation_key_attr validation Hashed representation of the parts of the event used for validation sort_key_attr Sort key of the table (if table is configured with a sort key). static_pk_value idempotency#{LAMBDA_FUNCTION_NAME} Static value to use as the partition key. Only used when sort_key_attr is set.","title":"DynamoDBPersistenceLayer"},{"location":"utilities/idempotency/#advanced","text":"","title":"Advanced"},{"location":"utilities/idempotency/#customizing-the-default-behavior","text":"Idempotent decorator can be further configured with IdempotencyConfig as seen in the previous example. These are the available options for further configuration Parameter Default Description event_key_jmespath \"\" JMESPath expression to extract the idempotency key from the event record using built-in functions payload_validation_jmespath \"\" JMESPath expression to validate whether certain parameters have changed in the event while the event payload raise_on_no_idempotency_key False Raise exception if no idempotency key was found in the request expires_after_seconds 3600 The number of seconds to wait before a record is expired use_local_cache False Whether to locally cache idempotency results local_cache_max_items 256 Max number of items to store in local cache hash_function md5 Function to use for calculating hashes, as provided by hashlib in the standard library.","title":"Customizing the default behavior"},{"location":"utilities/idempotency/#handling-concurrent-executions-with-the-same-payload","text":"This utility will raise an IdempotencyAlreadyInProgressError exception if you receive multiple invocations with the same payload while the first invocation hasn't completed yet . Info If you receive IdempotencyAlreadyInProgressError , you can safely retry the operation. This is a locking mechanism for correctness. Since we don't know the result from the first invocation yet, we can't safely allow another concurrent execution.","title":"Handling concurrent executions with the same payload"},{"location":"utilities/idempotency/#using-in-memory-cache","text":"By default, in-memory local caching is disabled , since we don't know how much memory you consume per invocation compared to the maximum configured in your Lambda function. Note: This in-memory cache is local to each Lambda execution environment This means it will be effective in cases where your function's concurrency is low in comparison to the number of \"retry\" invocations with the same payload, because cache might be empty. You can enable in-memory caching with the use_local_cache parameter: Caching idempotent transactions in-memory to prevent multiple calls to storage 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) config = IdempotencyConfig ( event_key_jmespath = \"body\" , use_local_cache = True , ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... When enabled, the default is to cache a maximum of 256 records in each Lambda execution environment - You can change it with the local_cache_max_items parameter.","title":"Using in-memory cache"},{"location":"utilities/idempotency/#expiring-idempotency-records","text":"Note By default, we expire idempotency records after an hour (3600 seconds). In most cases, it is not desirable to store the idempotency records forever. Rather, you want to guarantee that the same payload won't be executed within a period of time. You can change this window with the expires_after_seconds parameter: Adjusting cache TTL 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) config = IdempotencyConfig ( event_key_jmespath = \"body\" , expires_after_seconds = 5 * 60 , # 5 minutes ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... This will mark any records older than 5 minutes as expired, and the lambda handler will be executed as normal if it is invoked with a matching payload. Note: DynamoDB time-to-live field This utility uses expiration as the TTL field in DynamoDB, as demonstrated in the SAM example earlier .","title":"Expiring idempotency records"},{"location":"utilities/idempotency/#payload-validation","text":"Question: What if your function is invoked with the same payload except some outer parameters have changed? Example: A payment transaction for a given productID was requested twice for the same customer, however the amount to be paid has changed in the second transaction . By default, we will return the same result as it returned before, however in this instance it may be misleading; we provide a fail fast payload validation to address this edge case. With payload_validation_jmespath , you can provide an additional JMESPath expression to specify which part of the event body should be validated against previous idempotent invocations app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"[userDetail, productId]\" , payload_validation_jmespath = \"amount\" ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): # Creating a subscription payment is a side # effect of calling this function! payment = create_subscription_payment ( user = event [ 'userDetail' ][ 'username' ], product = event [ 'product_id' ], amount = event [ 'amount' ] ) ... return { \"message\" : \"success\" , \"statusCode\" : 200 , \"payment_id\" : payment . id , \"amount\" : payment . amount } Example Event 1 1 2 3 4 5 6 7 8 9 { \"userDetail\" : { \"username\" : \"User1\" , \"user_email\" : \"user@example.com\" }, \"productId\" : 1500 , \"charge_type\" : \"subscription\" , \"amount\" : 500 } Example Event 2 1 2 3 4 5 6 7 8 9 { \"userDetail\" : { \"username\" : \"User1\" , \"user_email\" : \"user@example.com\" }, \"productId\" : 1500 , \"charge_type\" : \"subscription\" , \"amount\" : 1 } In this example, the userDetail and productId keys are used as the payload to generate the idempotency key, as per event_key_jmespath parameter. Note If we try to send the same request but with a different amount, we will raise IdempotencyValidationError . Without payload validation, we would have returned the same result as we did for the initial request. Since we're also returning an amount in the response, this could be quite confusing for the client. By using payload_validation_jmespath=\"amount\" , we prevent this potentially confusing behavior and instead raise an Exception.","title":"Payload validation"},{"location":"utilities/idempotency/#making-idempotency-key-required","text":"If you want to enforce that an idempotency key is required, you can set raise_on_no_idempotency_key to True . This means that we will raise IdempotencyKeyError if the evaluation of event_key_jmespath is None . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) # Requires \"user\".\"uid\" and \"order_id\" to be present config = IdempotencyConfig ( event_key_jmespath = \"[user.uid, order_id]\" , raise_on_no_idempotency_key = True , ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): pass Success Event 1 2 3 4 5 6 7 { \"user\" : { \"uid\" : \"BB0D045C-8878-40C8-889E-38B3CB0A61B1\" , \"name\" : \"Foo\" }, \"order_id\" : 10000 } Failure Event Notice that order_id is now accidentally within user key 1 2 3 4 5 6 7 { \"user\" : { \"uid\" : \"DE0D000E-1234-10D1-991E-EAC1DD1D52C8\" , \"name\" : \"Joe Bloggs\" , \"order_id\" : 10000 }, }","title":"Making idempotency key required"},{"location":"utilities/idempotency/#customizing-boto-configuration","text":"The boto_config and boto3_session parameters enable you to pass in a custom botocore config object or a custom boto3 session when constructing the persistence store. Custom session 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import boto3 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) boto3_session = boto3 . session . Session () persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , boto3_session = boto3_session ) config = IdempotencyConfig ( event_key_jmespath = \"body\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... Custom config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from botocore.config import Config from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"body\" ) boto_config = Config () persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , boto_config = boto_config ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ...","title":"Customizing boto configuration"},{"location":"utilities/idempotency/#using-a-dynamodb-table-with-a-composite-primary-key","text":"When using a composite primary key table (hash+range key), use sort_key_attr parameter when initializing your persistence layer. With this setting, we will save the idempotency key in the sort key instead of the primary key. By default, the primary key will now be set to idempotency#{LAMBDA_FUNCTION_NAME} . You can optionally set a static value for the partition key using the static_pk_value parameter. Reusing a DynamoDB table that uses a composite primary key 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.idempotency import DynamoDBPersistenceLayer , idempotent persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , sort_key_attr = 'sort_key' ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): return { \"message\" : \"success\" : \"id\" : event [ 'body' ][ 'id]} The example function above would cause data to be stored in DynamoDB like this: id sort_key expiration status data idempotency#MyLambdaFunction 1e956ef7da78d0cb890be999aecc0c9e 1636549553 COMPLETED {\"id\": 12391, \"message\": \"success\"} idempotency#MyLambdaFunction 2b2cdb5f86361e97b4383087c1ffdf27 1636549571 COMPLETED {\"id\": 527212, \"message\": \"success\"} idempotency#MyLambdaFunction f091d2527ad1c78f05d54cc3f363be80 1636549585 IN_PROGRESS","title":"Using a DynamoDB table with a composite primary key"},{"location":"utilities/idempotency/#bring-your-own-persistent-store","text":"This utility provides an abstract base class (ABC), so that you can implement your choice of persistent storage layer. You can inherit from the BasePersistenceLayer class and implement the abstract methods _get_record , _put_record , _update_record and _delete_record . Excerpt DynamoDB Persisntence Layer implementation for reference 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 import datetime import logging from typing import Any , Dict , Optional import boto3 from botocore.config import Config from aws_lambda_powertools.utilities.idempotency import BasePersistenceLayer from aws_lambda_powertools.utilities.idempotency.exceptions import ( IdempotencyItemAlreadyExistsError , IdempotencyItemNotFoundError , ) from aws_lambda_powertools.utilities.idempotency.persistence.base import DataRecord logger = logging . getLogger ( __name__ ) class DynamoDBPersistenceLayer ( BasePersistenceLayer ): def __init__ ( self , table_name : str , key_attr : str = \"id\" , expiry_attr : str = \"expiration\" , status_attr : str = \"status\" , data_attr : str = \"data\" , validation_key_attr : str = \"validation\" , boto_config : Optional [ Config ] = None , boto3_session : Optional [ boto3 . session . Session ] = None , ): boto_config = boto_config or Config () session = boto3_session or boto3 . session . Session () self . _ddb_resource = session . resource ( \"dynamodb\" , config = boto_config ) self . table_name = table_name self . table = self . _ddb_resource . Table ( self . table_name ) self . key_attr = key_attr self . expiry_attr = expiry_attr self . status_attr = status_attr self . data_attr = data_attr self . validation_key_attr = validation_key_attr super ( DynamoDBPersistenceLayer , self ) . __init__ () def _item_to_data_record ( self , item : Dict [ str , Any ]) -> DataRecord : \"\"\" Translate raw item records from DynamoDB to DataRecord Parameters ---------- item: Dict[str, Union[str, int]] Item format from dynamodb response Returns ------- DataRecord representation of item \"\"\" return DataRecord ( idempotency_key = item [ self . key_attr ], status = item [ self . status_attr ], expiry_timestamp = item [ self . expiry_attr ], response_data = item . get ( self . data_attr ), payload_hash = item . get ( self . validation_key_attr ), ) def _get_record ( self , idempotency_key ) -> DataRecord : response = self . table . get_item ( Key = { self . key_attr : idempotency_key }, ConsistentRead = True ) try : item = response [ \"Item\" ] except KeyError : raise IdempotencyItemNotFoundError return self . _item_to_data_record ( item ) def _put_record ( self , data_record : DataRecord ) -> None : item = { self . key_attr : data_record . idempotency_key , self . expiry_attr : data_record . expiry_timestamp , self . status_attr : data_record . status , } if self . payload_validation_enabled : item [ self . validation_key_attr ] = data_record . payload_hash now = datetime . datetime . now () try : logger . debug ( f \"Putting record for idempotency key: { data_record . idempotency_key } \" ) self . table . put_item ( Item = item , ConditionExpression = f \"attribute_not_exists( { self . key_attr } ) OR { self . expiry_attr } < :now\" , ExpressionAttributeValues = { \":now\" : int ( now . timestamp ())}, ) except self . _ddb_resource . meta . client . exceptions . ConditionalCheckFailedException : logger . debug ( f \"Failed to put record for already existing idempotency key: { data_record . idempotency_key } \" ) raise IdempotencyItemAlreadyExistsError def _update_record ( self , data_record : DataRecord ): logger . debug ( f \"Updating record for idempotency key: { data_record . idempotency_key } \" ) update_expression = \"SET #response_data = :response_data, #expiry = :expiry, #status = :status\" expression_attr_values = { \":expiry\" : data_record . expiry_timestamp , \":response_data\" : data_record . response_data , \":status\" : data_record . status , } expression_attr_names = { \"#response_data\" : self . data_attr , \"#expiry\" : self . expiry_attr , \"#status\" : self . status_attr , } if self . payload_validation_enabled : update_expression += \", #validation_key = :validation_key\" expression_attr_values [ \":validation_key\" ] = data_record . payload_hash expression_attr_names [ \"#validation_key\" ] = self . validation_key_attr kwargs = { \"Key\" : { self . key_attr : data_record . idempotency_key }, \"UpdateExpression\" : update_expression , \"ExpressionAttributeValues\" : expression_attr_values , \"ExpressionAttributeNames\" : expression_attr_names , } self . table . update_item ( ** kwargs ) def _delete_record ( self , data_record : DataRecord ) -> None : logger . debug ( f \"Deleting record for idempotency key: { data_record . idempotency_key } \" ) self . table . delete_item ( Key = { self . key_attr : data_record . idempotency_key },) Danger Pay attention to the documentation for each - you may need to perform additional checks inside these methods to ensure the idempotency guarantees remain intact. For example, the _put_record method needs to raise an exception if a non-expired record already exists in the data store with a matching key.","title":"Bring your own persistent store"},{"location":"utilities/idempotency/#compatibility-with-other-utilities","text":"","title":"Compatibility with other utilities"},{"location":"utilities/idempotency/#validation-utility","text":"The idempotency utility can be used with the validator decorator. Ensure that idempotency is the innermost decorator. Warning If you use an envelope with the validator, the event received by the idempotency utility will be the unwrapped event - not the \"raw\" event Lambda was invoked with. Make sure to account for this behaviour, if you set the event_key_jmespath . Using Idempotency with JSONSchema Validation utility 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.validation import validator , envelopes from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"[message, username]\" ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @validator ( envelope = envelopes . API_GATEWAY_HTTP ) @idempotent ( config = config , persistence_store = persistence_layer ) def lambda_handler ( event , context ): cause_some_side_effects ( event [ 'username' ) return { \"message\" : event [ 'message' ], \"statusCode\" : 200 } Tip: JMESPath Powertools functions are also available Built-in functions known in the validation utility like powertools_json , powertools_base64 , powertools_base64_gzip are also available to use in this utility.","title":"Validation utility"},{"location":"utilities/idempotency/#testing-your-code","text":"The idempotency utility provides several routes to test your code.","title":"Testing your code"},{"location":"utilities/idempotency/#disabling-the-idempotency-utility","text":"When testing your code, you may wish to disable the idempotency logic altogether and focus on testing your business logic. To do this, you can set the environment variable POWERTOOLS_IDEMPOTENCY_DISABLED with a truthy value. If you prefer setting this for specific tests, and are using Pytest, you can use monkeypatch fixture: tests.py 1 2 3 4 5 6 def test_idempotent_lambda_handler ( monkeypatch ): # Set POWERTOOLS_IDEMPOTENCY_DISABLED before calling decorated functions monkeypatch . setenv ( \"POWERTOOLS_IDEMPOTENCY_DISABLED\" , 1 ) result = handler () ... app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"idempotency\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): print ( 'expensive operation' ) return { \"payment_id\" : 12345 , \"message\" : \"success\" , \"statusCode\" : 200 , }","title":"Disabling the idempotency utility"},{"location":"utilities/idempotency/#testing-with-dynamodb-local","text":"To test with DynamoDB Local , you can replace the Table resource used by the persistence layer with one you create inside your tests. This allows you to set the endpoint_url. tests.py 1 2 3 4 5 6 7 8 9 10 11 12 import boto3 import app def test_idempotent_lambda (): # Create our own Table resource using the endpoint for our DynamoDB Local instance resource = boto3 . resource ( \"dynamodb\" , endpoint_url = 'http://localhost:8000' ) table = resource . Table ( app . persistence_layer . table_name ) app . persistence_layer . table = table result = app . handler ({ 'testkey' : 'testvalue' }, {}) assert result [ 'payment_id' ] == 12345 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"idempotency\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): print ( 'expensive operation' ) return { \"payment_id\" : 12345 , \"message\" : \"success\" , \"statusCode\" : 200 , }","title":"Testing with DynamoDB Local"},{"location":"utilities/idempotency/#how-do-i-mock-all-dynamodb-io-operations","text":"The idempotency utility lazily creates the dynamodb Table which it uses to access DynamoDB. This means it is possible to pass a mocked Table resource, or stub various methods. tests.py 1 2 3 4 5 6 7 8 9 10 from unittest.mock import MagicMock import app def test_idempotent_lambda (): table = MagicMock () app . persistence_layer . table = table result = app . handler ({ 'testkey' : 'testvalue' }, {}) table . put_item . assert_called () ... app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"idempotency\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): print ( 'expensive operation' ) return { \"payment_id\" : 12345 , \"message\" : \"success\" , \"statusCode\" : 200 , }","title":"How do I mock all DynamoDB I/O operations"},{"location":"utilities/idempotency/#extra-resources","text":"If you're interested in a deep dive on how Amazon uses idempotency when building our APIs, check out this article .","title":"Extra resources"},{"location":"utilities/jmespath_functions/","text":"Tip JMESPath is a query language for JSON used by AWS CLI, AWS Python SDK, and AWS Lambda Powertools for Python. Built-in JMESPath Functions to easily deserialize common encoded JSON payloads in Lambda functions. Key features \u00b6 Deserialize JSON from JSON strings, base64, and compressed data Use JMESPath to extract and combine data recursively Getting started \u00b6 You might have events that contains encoded JSON payloads as string, base64, or even in compressed format. It is a common use case to decode and extract them partially or fully as part of your Lambda function invocation. Lambda Powertools also have utilities like validation , idempotency , or feature flags where you might need to extract a portion of your data before using them. Info Envelope is the terminology we use for the JMESPath expression to extract your JSON object from your data input. Extracting data \u00b6 You can use the extract_data_from_envelope function along with any JMESPath expression . app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.jmespath_utils import extract_data_from_envelope from aws_lambda_powertools.utilities.typing import LambdaContext def handler ( event : dict , context : LambdaContext ): payload = extract_data_from_envelope ( data = event , envelope = \"powertools_json(body)\" ) customer = payload . get ( \"customerId\" ) # now deserialized ... event.json 1 2 3 { \"body\" : \"{\\\"customerId\\\":\\\"dd4649e6-2484-4993-acb8-0f9123103394\\\"}\" } Built-in envelopes \u00b6 We provide built-in envelopes for popular JMESPath expressions used when looking to decode/deserialize JSON objects within AWS Lambda Event Sources. app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.jmespath_utils import extract_data_from_envelope , envelopes from aws_lambda_powertools.utilities.typing import LambdaContext def handler ( event : dict , context : LambdaContext ): payload = extract_data_from_envelope ( data = event , envelope = envelopes . SNS ) customer = payload . get ( \"customerId\" ) # now deserialized ... event.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"Records\" : [ { \"messageId\" : \"19dd0b57-b21e-4ac1-bd88-01bbb068cb78\" , \"receiptHandle\" : \"MessageReceiptHandle\" , \"body\" : \"{\\\"customerId\\\":\\\"dd4649e6-2484-4993-acb8-0f9123103394\\\",\\\"booking\\\":{\\\"id\\\":\\\"5b2c4803-330b-42b7-811a-c68689425de1\\\",\\\"reference\\\":\\\"ySz7oA\\\",\\\"outboundFlightId\\\":\\\"20c0d2f2-56a3-4068-bf20-ff7703db552d\\\"},\\\"payment\\\":{\\\"receipt\\\":\\\"https:\\/\\/pay.stripe.com\\/receipts\\/acct_1Dvn7pF4aIiftV70\\/ch_3JTC14F4aIiftV700iFq2CHB\\/rcpt_K7QsrFln9FgFnzUuBIiNdkkRYGxUL0X\\\",\\\"amount\\\":100}}\" , \"attributes\" : { \"ApproximateReceiveCount\" : \"1\" , \"SentTimestamp\" : \"1523232000000\" , \"SenderId\" : \"123456789012\" , \"ApproximateFirstReceiveTimestamp\" : \"1523232000001\" }, \"messageAttributes\" : {}, \"md5OfBody\" : \"7b270e59b47ff90a553787216d55d91d\" , \"eventSource\" : \"aws:sqs\" , \"eventSourceARN\" : \"arn:aws:sqs:us-east-1:123456789012:MyQueue\" , \"awsRegion\" : \"us-east-1\" } ] } These are all built-in envelopes you can use along with their expression as a reference: Envelope JMESPath expression API_GATEWAY_REST powertools_json(body) API_GATEWAY_HTTP API_GATEWAY_REST SQS Records[*].powertools_json(body) SNS Records[0].Sns.Message | powertools_json(@) EVENTBRIDGE detail CLOUDWATCH_EVENTS_SCHEDULED EVENTBRIDGE KINESIS_DATA_STREAM Records[*].kinesis.powertools_json(powertools_base64(data)) CLOUDWATCH_LOGS awslogs.powertools_base64_gzip(data) | powertools_json(@).logEvents[*] Advanced \u00b6 Built-in JMESPath functions \u00b6 You can use our built-in JMESPath functions within your expressions to do exactly that to decode JSON Strings, base64, and uncompress gzip data. Info We use these for built-in envelopes to easily decode and unwrap events from sources like API Gateway, Kinesis, CloudWatch Logs, etc. powertools_json function \u00b6 Use powertools_json function to decode any JSON String anywhere a JMESPath expression is allowed. Validation scenario This sample will decode the value within the data key into a valid JSON before we can validate it. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.validation import validate import schemas sample_event = { 'data' : '{\"payload\": {\"message\": \"hello hello\", \"username\": \"blah blah\"}}' } validate ( event = sample_event , schema = schemas . INPUT , envelope = \"powertools_json(data)\" ) schemas.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 INPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"message\" : \"hello world\" , \"username\" : \"lessa\" }], \"required\" : [ \"message\" , \"username\" ], \"properties\" : { \"message\" : { \"$id\" : \"#/properties/message\" , \"type\" : \"string\" , \"title\" : \"The message\" , \"examples\" : [ \"hello world\" ], \"maxLength\" : 100 , }, \"username\" : { \"$id\" : \"#/properties/username\" , \"type\" : \"string\" , \"title\" : \"The username\" , \"examples\" : [ \"lessa\" ], \"maxLength\" : 30 , }, }, } OUTPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample outgoing schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"statusCode\" : 200 , \"body\" : \"response\" }], \"required\" : [ \"statusCode\" , \"body\" ], \"properties\" : { \"statusCode\" : { \"$id\" : \"#/properties/statusCode\" , \"type\" : \"integer\" , \"title\" : \"The statusCode\" }, \"body\" : { \"$id\" : \"#/properties/body\" , \"type\" : \"string\" , \"title\" : \"The response\" }, }, } Idempotency scenario This sample will decode the value within the body key of an API Gateway event into a valid JSON object to ensure the Idempotency utility processes a JSON object instead of a string. Deserializing JSON before using as idempotency key 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import json from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) config = IdempotencyConfig ( event_key_jmespath = \"powertools_json(body)\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event : APIGatewayProxyEvent , context ): body = json . loads ( event [ 'body' ]) payment = create_subscription_payment ( user = body [ 'user' ], product = body [ 'product_id' ] ) ... return { \"payment_id\" : payment . id , \"message\" : \"success\" , \"statusCode\" : 200 } powertools_base64 function \u00b6 Use powertools_base64 function to decode any base64 data. This sample will decode the base64 value within the data key, and decode the JSON string into a valid JSON before we can validate it. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.validation import validate import schemas sample_event = { \"data\" : \"eyJtZXNzYWdlIjogImhlbGxvIGhlbGxvIiwgInVzZXJuYW1lIjogImJsYWggYmxhaCJ9=\" } validate ( event = sample_event , schema = schemas . INPUT , envelope = \"powertools_json(powertools_base64(data))\" ) schemas.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 INPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"message\" : \"hello world\" , \"username\" : \"lessa\" }], \"required\" : [ \"message\" , \"username\" ], \"properties\" : { \"message\" : { \"$id\" : \"#/properties/message\" , \"type\" : \"string\" , \"title\" : \"The message\" , \"examples\" : [ \"hello world\" ], \"maxLength\" : 100 , }, \"username\" : { \"$id\" : \"#/properties/username\" , \"type\" : \"string\" , \"title\" : \"The username\" , \"examples\" : [ \"lessa\" ], \"maxLength\" : 30 , }, }, } OUTPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample outgoing schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"statusCode\" : 200 , \"body\" : \"response\" }], \"required\" : [ \"statusCode\" , \"body\" ], \"properties\" : { \"statusCode\" : { \"$id\" : \"#/properties/statusCode\" , \"type\" : \"integer\" , \"title\" : \"The statusCode\" }, \"body\" : { \"$id\" : \"#/properties/body\" , \"type\" : \"string\" , \"title\" : \"The response\" }, }, } powertools_base64_gzip function \u00b6 Use powertools_base64_gzip function to decompress and decode base64 data. This sample will decompress and decode base64 data, then use JMESPath pipeline expression to pass the result for decoding its JSON string. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.validation import validate import schemas sample_event = { \"data\" : \"H4sIACZAXl8C/52PzUrEMBhFX2UILpX8tPbHXWHqIOiq3Q1F0ubrWEiakqTWofTdTYYB0YWL2d5zvnuTFellBIOedoiyKH5M0iwnlKH7HZL6dDB6ngLDfLFYctUKjie9gHFaS/sAX1xNEq525QxwFXRGGMEkx4Th491rUZdV3YiIZ6Ljfd+lfSyAtZloacQgAkqSJCGhxM6t7cwwuUGPz4N0YKyvO6I9WDeMPMSo8Z4Ca/kJ6vMEYW5f1MX7W1lVxaG8vqX8hNFdjlc0iCBBSF4ERT/3Pl7RbMGMXF2KZMh/C+gDpNS7RRsp0OaRGzx0/t8e0jgmcczyLCWEePhni/23JWalzjdu0a3ZvgEaNLXeugEAAA==\" } validate ( event = sample_event , schema = schemas . INPUT , envelope = \"powertools_base64_gzip(data) | powertools_json(@)\" ) schemas.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 INPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"message\" : \"hello world\" , \"username\" : \"lessa\" }], \"required\" : [ \"message\" , \"username\" ], \"properties\" : { \"message\" : { \"$id\" : \"#/properties/message\" , \"type\" : \"string\" , \"title\" : \"The message\" , \"examples\" : [ \"hello world\" ], \"maxLength\" : 100 , }, \"username\" : { \"$id\" : \"#/properties/username\" , \"type\" : \"string\" , \"title\" : \"The username\" , \"examples\" : [ \"lessa\" ], \"maxLength\" : 30 , }, }, } OUTPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample outgoing schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"statusCode\" : 200 , \"body\" : \"response\" }], \"required\" : [ \"statusCode\" , \"body\" ], \"properties\" : { \"statusCode\" : { \"$id\" : \"#/properties/statusCode\" , \"type\" : \"integer\" , \"title\" : \"The statusCode\" }, \"body\" : { \"$id\" : \"#/properties/body\" , \"type\" : \"string\" , \"title\" : \"The response\" }, }, } Bring your own JMESPath function \u00b6 Warning This should only be used for advanced use cases where you have special formats not covered by the built-in functions. For special binary formats that you want to decode before applying JSON Schema validation, you can bring your own JMESPath function and any additional option via jmespath_options param. In order to keep the built-in functions from Powertools, you can subclass from PowertoolsFunctions : custom_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools.utilities.jmespath_utils import ( PowertoolsFunctions , extract_data_from_envelope ) from jmespath.functions import signature class CustomFunctions ( PowertoolsFunctions ): @signature ({ 'types' : [ 'string' ]}) # Only decode if value is a string def _func_special_decoder ( self , s ): return my_custom_decoder_logic ( s ) custom_jmespath_options = { \"custom_functions\" : CustomFunctions ()} def handler ( event , context ): # use the custom name after `_func_` extract_data_from_envelope ( data = event , envelope = \"special_decoder(body)\" , jmespath_options =** custom_jmespath_options ) ... event.json 1 { \"body\" : \"custom_encoded_data\" }","title":"JMESPath Functions"},{"location":"utilities/jmespath_functions/#key-features","text":"Deserialize JSON from JSON strings, base64, and compressed data Use JMESPath to extract and combine data recursively","title":"Key features"},{"location":"utilities/jmespath_functions/#getting-started","text":"You might have events that contains encoded JSON payloads as string, base64, or even in compressed format. It is a common use case to decode and extract them partially or fully as part of your Lambda function invocation. Lambda Powertools also have utilities like validation , idempotency , or feature flags where you might need to extract a portion of your data before using them. Info Envelope is the terminology we use for the JMESPath expression to extract your JSON object from your data input.","title":"Getting started"},{"location":"utilities/jmespath_functions/#extracting-data","text":"You can use the extract_data_from_envelope function along with any JMESPath expression . app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.jmespath_utils import extract_data_from_envelope from aws_lambda_powertools.utilities.typing import LambdaContext def handler ( event : dict , context : LambdaContext ): payload = extract_data_from_envelope ( data = event , envelope = \"powertools_json(body)\" ) customer = payload . get ( \"customerId\" ) # now deserialized ... event.json 1 2 3 { \"body\" : \"{\\\"customerId\\\":\\\"dd4649e6-2484-4993-acb8-0f9123103394\\\"}\" }","title":"Extracting data"},{"location":"utilities/jmespath_functions/#built-in-envelopes","text":"We provide built-in envelopes for popular JMESPath expressions used when looking to decode/deserialize JSON objects within AWS Lambda Event Sources. app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.jmespath_utils import extract_data_from_envelope , envelopes from aws_lambda_powertools.utilities.typing import LambdaContext def handler ( event : dict , context : LambdaContext ): payload = extract_data_from_envelope ( data = event , envelope = envelopes . SNS ) customer = payload . get ( \"customerId\" ) # now deserialized ... event.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"Records\" : [ { \"messageId\" : \"19dd0b57-b21e-4ac1-bd88-01bbb068cb78\" , \"receiptHandle\" : \"MessageReceiptHandle\" , \"body\" : \"{\\\"customerId\\\":\\\"dd4649e6-2484-4993-acb8-0f9123103394\\\",\\\"booking\\\":{\\\"id\\\":\\\"5b2c4803-330b-42b7-811a-c68689425de1\\\",\\\"reference\\\":\\\"ySz7oA\\\",\\\"outboundFlightId\\\":\\\"20c0d2f2-56a3-4068-bf20-ff7703db552d\\\"},\\\"payment\\\":{\\\"receipt\\\":\\\"https:\\/\\/pay.stripe.com\\/receipts\\/acct_1Dvn7pF4aIiftV70\\/ch_3JTC14F4aIiftV700iFq2CHB\\/rcpt_K7QsrFln9FgFnzUuBIiNdkkRYGxUL0X\\\",\\\"amount\\\":100}}\" , \"attributes\" : { \"ApproximateReceiveCount\" : \"1\" , \"SentTimestamp\" : \"1523232000000\" , \"SenderId\" : \"123456789012\" , \"ApproximateFirstReceiveTimestamp\" : \"1523232000001\" }, \"messageAttributes\" : {}, \"md5OfBody\" : \"7b270e59b47ff90a553787216d55d91d\" , \"eventSource\" : \"aws:sqs\" , \"eventSourceARN\" : \"arn:aws:sqs:us-east-1:123456789012:MyQueue\" , \"awsRegion\" : \"us-east-1\" } ] } These are all built-in envelopes you can use along with their expression as a reference: Envelope JMESPath expression API_GATEWAY_REST powertools_json(body) API_GATEWAY_HTTP API_GATEWAY_REST SQS Records[*].powertools_json(body) SNS Records[0].Sns.Message | powertools_json(@) EVENTBRIDGE detail CLOUDWATCH_EVENTS_SCHEDULED EVENTBRIDGE KINESIS_DATA_STREAM Records[*].kinesis.powertools_json(powertools_base64(data)) CLOUDWATCH_LOGS awslogs.powertools_base64_gzip(data) | powertools_json(@).logEvents[*]","title":"Built-in envelopes"},{"location":"utilities/jmespath_functions/#advanced","text":"","title":"Advanced"},{"location":"utilities/jmespath_functions/#built-in-jmespath-functions","text":"You can use our built-in JMESPath functions within your expressions to do exactly that to decode JSON Strings, base64, and uncompress gzip data. Info We use these for built-in envelopes to easily decode and unwrap events from sources like API Gateway, Kinesis, CloudWatch Logs, etc.","title":"Built-in JMESPath functions"},{"location":"utilities/jmespath_functions/#powertools_json-function","text":"Use powertools_json function to decode any JSON String anywhere a JMESPath expression is allowed. Validation scenario This sample will decode the value within the data key into a valid JSON before we can validate it. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.validation import validate import schemas sample_event = { 'data' : '{\"payload\": {\"message\": \"hello hello\", \"username\": \"blah blah\"}}' } validate ( event = sample_event , schema = schemas . INPUT , envelope = \"powertools_json(data)\" ) schemas.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 INPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"message\" : \"hello world\" , \"username\" : \"lessa\" }], \"required\" : [ \"message\" , \"username\" ], \"properties\" : { \"message\" : { \"$id\" : \"#/properties/message\" , \"type\" : \"string\" , \"title\" : \"The message\" , \"examples\" : [ \"hello world\" ], \"maxLength\" : 100 , }, \"username\" : { \"$id\" : \"#/properties/username\" , \"type\" : \"string\" , \"title\" : \"The username\" , \"examples\" : [ \"lessa\" ], \"maxLength\" : 30 , }, }, } OUTPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample outgoing schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"statusCode\" : 200 , \"body\" : \"response\" }], \"required\" : [ \"statusCode\" , \"body\" ], \"properties\" : { \"statusCode\" : { \"$id\" : \"#/properties/statusCode\" , \"type\" : \"integer\" , \"title\" : \"The statusCode\" }, \"body\" : { \"$id\" : \"#/properties/body\" , \"type\" : \"string\" , \"title\" : \"The response\" }, }, } Idempotency scenario This sample will decode the value within the body key of an API Gateway event into a valid JSON object to ensure the Idempotency utility processes a JSON object instead of a string. Deserializing JSON before using as idempotency key 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import json from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) config = IdempotencyConfig ( event_key_jmespath = \"powertools_json(body)\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event : APIGatewayProxyEvent , context ): body = json . loads ( event [ 'body' ]) payment = create_subscription_payment ( user = body [ 'user' ], product = body [ 'product_id' ] ) ... return { \"payment_id\" : payment . id , \"message\" : \"success\" , \"statusCode\" : 200 }","title":"powertools_json function"},{"location":"utilities/jmespath_functions/#powertools_base64-function","text":"Use powertools_base64 function to decode any base64 data. This sample will decode the base64 value within the data key, and decode the JSON string into a valid JSON before we can validate it. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.validation import validate import schemas sample_event = { \"data\" : \"eyJtZXNzYWdlIjogImhlbGxvIGhlbGxvIiwgInVzZXJuYW1lIjogImJsYWggYmxhaCJ9=\" } validate ( event = sample_event , schema = schemas . INPUT , envelope = \"powertools_json(powertools_base64(data))\" ) schemas.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 INPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"message\" : \"hello world\" , \"username\" : \"lessa\" }], \"required\" : [ \"message\" , \"username\" ], \"properties\" : { \"message\" : { \"$id\" : \"#/properties/message\" , \"type\" : \"string\" , \"title\" : \"The message\" , \"examples\" : [ \"hello world\" ], \"maxLength\" : 100 , }, \"username\" : { \"$id\" : \"#/properties/username\" , \"type\" : \"string\" , \"title\" : \"The username\" , \"examples\" : [ \"lessa\" ], \"maxLength\" : 30 , }, }, } OUTPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample outgoing schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"statusCode\" : 200 , \"body\" : \"response\" }], \"required\" : [ \"statusCode\" , \"body\" ], \"properties\" : { \"statusCode\" : { \"$id\" : \"#/properties/statusCode\" , \"type\" : \"integer\" , \"title\" : \"The statusCode\" }, \"body\" : { \"$id\" : \"#/properties/body\" , \"type\" : \"string\" , \"title\" : \"The response\" }, }, }","title":"powertools_base64 function"},{"location":"utilities/jmespath_functions/#powertools_base64_gzip-function","text":"Use powertools_base64_gzip function to decompress and decode base64 data. This sample will decompress and decode base64 data, then use JMESPath pipeline expression to pass the result for decoding its JSON string. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.validation import validate import schemas sample_event = { \"data\" : \"H4sIACZAXl8C/52PzUrEMBhFX2UILpX8tPbHXWHqIOiq3Q1F0ubrWEiakqTWofTdTYYB0YWL2d5zvnuTFellBIOedoiyKH5M0iwnlKH7HZL6dDB6ngLDfLFYctUKjie9gHFaS/sAX1xNEq525QxwFXRGGMEkx4Th491rUZdV3YiIZ6Ljfd+lfSyAtZloacQgAkqSJCGhxM6t7cwwuUGPz4N0YKyvO6I9WDeMPMSo8Z4Ca/kJ6vMEYW5f1MX7W1lVxaG8vqX8hNFdjlc0iCBBSF4ERT/3Pl7RbMGMXF2KZMh/C+gDpNS7RRsp0OaRGzx0/t8e0jgmcczyLCWEePhni/23JWalzjdu0a3ZvgEaNLXeugEAAA==\" } validate ( event = sample_event , schema = schemas . INPUT , envelope = \"powertools_base64_gzip(data) | powertools_json(@)\" ) schemas.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 INPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"message\" : \"hello world\" , \"username\" : \"lessa\" }], \"required\" : [ \"message\" , \"username\" ], \"properties\" : { \"message\" : { \"$id\" : \"#/properties/message\" , \"type\" : \"string\" , \"title\" : \"The message\" , \"examples\" : [ \"hello world\" ], \"maxLength\" : 100 , }, \"username\" : { \"$id\" : \"#/properties/username\" , \"type\" : \"string\" , \"title\" : \"The username\" , \"examples\" : [ \"lessa\" ], \"maxLength\" : 30 , }, }, } OUTPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample outgoing schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"statusCode\" : 200 , \"body\" : \"response\" }], \"required\" : [ \"statusCode\" , \"body\" ], \"properties\" : { \"statusCode\" : { \"$id\" : \"#/properties/statusCode\" , \"type\" : \"integer\" , \"title\" : \"The statusCode\" }, \"body\" : { \"$id\" : \"#/properties/body\" , \"type\" : \"string\" , \"title\" : \"The response\" }, }, }","title":"powertools_base64_gzip function"},{"location":"utilities/jmespath_functions/#bring-your-own-jmespath-function","text":"Warning This should only be used for advanced use cases where you have special formats not covered by the built-in functions. For special binary formats that you want to decode before applying JSON Schema validation, you can bring your own JMESPath function and any additional option via jmespath_options param. In order to keep the built-in functions from Powertools, you can subclass from PowertoolsFunctions : custom_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools.utilities.jmespath_utils import ( PowertoolsFunctions , extract_data_from_envelope ) from jmespath.functions import signature class CustomFunctions ( PowertoolsFunctions ): @signature ({ 'types' : [ 'string' ]}) # Only decode if value is a string def _func_special_decoder ( self , s ): return my_custom_decoder_logic ( s ) custom_jmespath_options = { \"custom_functions\" : CustomFunctions ()} def handler ( event , context ): # use the custom name after `_func_` extract_data_from_envelope ( data = event , envelope = \"special_decoder(body)\" , jmespath_options =** custom_jmespath_options ) ... event.json 1 { \"body\" : \"custom_encoded_data\" }","title":"Bring your own JMESPath function"},{"location":"utilities/middleware_factory/","text":"Middleware factory provides a decorator factory to create your own middleware to run logic before, and after each Lambda invocation synchronously. Key features \u00b6 Run logic before, after, and handle exceptions Trace each middleware when requested Middleware with no params \u00b6 You can create your own middleware using lambda_handler_decorator . The decorator factory expects 3 arguments in your function signature: handler - Lambda function handler event - Lambda function invocation event context - Lambda function context object Creating your own middleware for before/after logic 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator @lambda_handler_decorator def middleware_before_after ( handler , event , context ): # logic_before_handler_execution() response = handler ( event , context ) # logic_after_handler_execution() return response @middleware_before_after def lambda_handler ( event , context ): ... Middleware with params \u00b6 You can also have your own keyword arguments after the mandatory arguments. Accepting arbitrary keyword arguments 1 2 3 4 5 6 7 8 9 10 11 12 13 @lambda_handler_decorator def obfuscate_sensitive_data ( handler , event , context , fields : List = None ): # Obfuscate email before calling Lambda handler if fields : for field in fields : if field in event : event [ field ] = obfuscate ( event [ field ]) return handler ( event , context ) @obfuscate_sensitive_data ( fields = [ \"email\" ]) def lambda_handler ( event , context ): ... Tracing middleware execution \u00b6 If you are making use of Tracer , you can trace the execution of your middleware to ease operations. This makes use of an existing Tracer instance that you may have initialized anywhere in your code. Tracing custom middlewares with Tracer 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator @lambda_handler_decorator ( trace_execution = True ) def my_middleware ( handler , event , context ): return handler ( event , context ) @my_middleware def lambda_handler ( event , context ): ... When executed, your middleware name will appear in AWS X-Ray Trace details as ## middleware_name . For advanced use cases, you can instantiate Tracer inside your middleware, and add annotations as well as metadata for additional operational insights. Add custom tracing insights before/after in your middlware 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator from aws_lambda_powertools import Tracer @lambda_handler_decorator ( trace_execution = True ) def middleware_name ( handler , event , context ): # tracer = Tracer() # Takes a copy of an existing tracer instance # tracer.add_annotation... # tracer.add_metadata... return handler ( event , context ) Tips \u00b6 Use trace_execution to quickly understand the performance impact of your middlewares, and reduce or merge tasks when necessary When nesting multiple middlewares, always return the handler with event and context, or response Keep in mind Python decorators execution order . Lambda handler is actually called once (top-down) Async middlewares are not supported","title":"Middleware factory"},{"location":"utilities/middleware_factory/#key-features","text":"Run logic before, after, and handle exceptions Trace each middleware when requested","title":"Key features"},{"location":"utilities/middleware_factory/#middleware-with-no-params","text":"You can create your own middleware using lambda_handler_decorator . The decorator factory expects 3 arguments in your function signature: handler - Lambda function handler event - Lambda function invocation event context - Lambda function context object Creating your own middleware for before/after logic 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator @lambda_handler_decorator def middleware_before_after ( handler , event , context ): # logic_before_handler_execution() response = handler ( event , context ) # logic_after_handler_execution() return response @middleware_before_after def lambda_handler ( event , context ): ...","title":"Middleware with no params"},{"location":"utilities/middleware_factory/#middleware-with-params","text":"You can also have your own keyword arguments after the mandatory arguments. Accepting arbitrary keyword arguments 1 2 3 4 5 6 7 8 9 10 11 12 13 @lambda_handler_decorator def obfuscate_sensitive_data ( handler , event , context , fields : List = None ): # Obfuscate email before calling Lambda handler if fields : for field in fields : if field in event : event [ field ] = obfuscate ( event [ field ]) return handler ( event , context ) @obfuscate_sensitive_data ( fields = [ \"email\" ]) def lambda_handler ( event , context ): ...","title":"Middleware with params"},{"location":"utilities/middleware_factory/#tracing-middleware-execution","text":"If you are making use of Tracer , you can trace the execution of your middleware to ease operations. This makes use of an existing Tracer instance that you may have initialized anywhere in your code. Tracing custom middlewares with Tracer 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator @lambda_handler_decorator ( trace_execution = True ) def my_middleware ( handler , event , context ): return handler ( event , context ) @my_middleware def lambda_handler ( event , context ): ... When executed, your middleware name will appear in AWS X-Ray Trace details as ## middleware_name . For advanced use cases, you can instantiate Tracer inside your middleware, and add annotations as well as metadata for additional operational insights. Add custom tracing insights before/after in your middlware 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator from aws_lambda_powertools import Tracer @lambda_handler_decorator ( trace_execution = True ) def middleware_name ( handler , event , context ): # tracer = Tracer() # Takes a copy of an existing tracer instance # tracer.add_annotation... # tracer.add_metadata... return handler ( event , context )","title":"Tracing middleware execution"},{"location":"utilities/middleware_factory/#tips","text":"Use trace_execution to quickly understand the performance impact of your middlewares, and reduce or merge tasks when necessary When nesting multiple middlewares, always return the handler with event and context, or response Keep in mind Python decorators execution order . Lambda handler is actually called once (top-down) Async middlewares are not supported","title":"Tips"},{"location":"utilities/parameters/","text":"The parameters utility provides high-level functions to retrieve one or multiple parameter values from AWS Systems Manager Parameter Store , AWS Secrets Manager , AWS AppConfig , Amazon DynamoDB , or bring your own. Key features \u00b6 Retrieve one or multiple parameters from the underlying provider Cache parameter values for a given amount of time (defaults to 5 seconds) Transform parameter values from JSON or base 64 encoded strings Bring Your Own Parameter Store Provider Getting started \u00b6 By default, we fetch parameters from System Manager Parameter Store, secrets from Secrets Manager, and application configuration from AppConfig. IAM Permissions \u00b6 This utility requires additional permissions to work as expected. Note Different parameter providers require different permissions. Provider Function/Method IAM Permission SSM Parameter Store get_parameter , SSMProvider.get ssm:GetParameter SSM Parameter Store get_parameters , SSMProvider.get_multiple ssm:GetParametersByPath Secrets Manager get_secret , SecretsManager.get secretsmanager:GetSecretValue DynamoDB DynamoDBProvider.get dynamodb:GetItem DynamoDB DynamoDBProvider.get_multiple dynamodb:Query App Config AppConfigProvider.get_app_config , get_app_config appconfig:GetConfiguration Fetching parameters \u00b6 You can retrieve a single parameter using get_parameter high-level function. For multiple parameters, you can use get_parameters and pass a path to retrieve them recursively. Fetching multiple parameters recursively 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single parameter value = parameters . get_parameter ( \"/my/parameter\" ) # Retrieve multiple parameters from a path prefix recursively # This returns a dict with the parameter name as key values = parameters . get_parameters ( \"/my/path/prefix\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) Fetching secrets \u00b6 You can fetch secrets stored in Secrets Manager using get_secrets . Fetching secrets 1 2 3 4 5 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single secret value = parameters . get_secret ( \"my-secret\" ) Fetching app configurations \u00b6 You can fetch application configurations in AWS AppConfig using get_app_config . The following will retrieve the latest version and store it in the cache. Fetching latest config from AppConfig 1 2 3 4 5 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single configuration, latest version value : bytes = parameters . get_app_config ( name = \"my_configuration\" , environment = \"my_env\" , application = \"my_app\" ) Advanced \u00b6 Adjusting cache TTL \u00b6 Tip max_age parameter is also available in high level functions like get_parameter , get_secret , etc. By default, we cache parameters retrieved in-memory for 5 seconds. You can adjust how long we should keep values in cache by using the param max_age , when using get() or get_multiple() methods across all providers. Caching parameter(s) value in memory for longer than 5 seconds 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) ssm_provider = parameters . SSMProvider ( config = config ) def handler ( event , context ): # Retrieve a single parameter value = ssm_provider . get ( \"/my/parameter\" , max_age = 60 ) # 1 minute # Retrieve multiple parameters from a path prefix values = ssm_provider . get_multiple ( \"/my/path/prefix\" , max_age = 60 ) for k , v in values . items (): print ( f \" { k } : { v } \" ) Always fetching the latest \u00b6 If you'd like to always ensure you fetch the latest parameter from the store regardless if already available in cache, use force_fetch param. Forcefully fetching the latest parameter whether TTL has expired or not 1 2 3 4 5 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single parameter value = parameters . get_parameter ( \"/my/parameter\" , force_fetch = True ) Built-in provider class \u00b6 For greater flexibility such as configuring the underlying SDK client used by built-in providers, you can use their respective Provider Classes directly. Tip This can be used to retrieve values from other regions, change the retry behavior, etc. SSMProvider \u00b6 Example with SSMProvider for further extensibility 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) ssm_provider = parameters . SSMProvider ( config = config ) # or boto3_session=boto3.Session() def handler ( event , context ): # Retrieve a single parameter value = ssm_provider . get ( \"/my/parameter\" ) # Retrieve multiple parameters from a path prefix values = ssm_provider . get_multiple ( \"/my/path/prefix\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) The AWS Systems Manager Parameter Store provider supports two additional arguments for the get() and get_multiple() methods: Parameter Default Description decrypt False Will automatically decrypt the parameter. recursive True For get_multiple() only, will fetch all parameter values recursively based on a path prefix. Example with get() and get_multiple() 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): decrypted_value = ssm_provider . get ( \"/my/encrypted/parameter\" , decrypt = True ) no_recursive_values = ssm_provider . get_multiple ( \"/my/path/prefix\" , recursive = False ) SecretsProvider \u00b6 Example with SecretsProvider for further extensibility 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) secrets_provider = parameters . SecretsProvider ( config = config ) def handler ( event , context ): # Retrieve a single secret value = secrets_provider . get ( \"my-secret\" ) DynamoDBProvider \u00b6 The DynamoDB Provider does not have any high-level functions, as it needs to know the name of the DynamoDB table containing the parameters. DynamoDB table structure for single parameters For single parameters, you must use id as the partition key for that table. Example DynamoDB table with id partition key and value as attribute id value my-parameter my-value With this table, dynamodb_provider.get(\"my-param\") will return my-value . app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" ) def handler ( event , context ): # Retrieve a value from DynamoDB value = dynamodb_provider . get ( \"my-parameter\" ) DynamoDB Local example You can initialize the DynamoDB provider pointing to DynamoDB Local using endpoint_url parameter: 1 2 3 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" , endpoint_url = \"http://localhost:8000\" ) DynamoDB table structure for multiple values parameters You can retrieve multiple parameters sharing the same id by having a sort key named sk . Example DynamoDB table with id primary key, sk as sort key and value` as attribute id sk value my-hash-key param-a my-value-a my-hash-key param-b my-value-b my-hash-key param-c my-value-c With this table, dynamodb_provider.get_multiple(\"my-hash-key\") will return a dictionary response in the shape of sk:value . app.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" ) def handler ( event , context ): # Retrieve multiple values by performing a Query on the DynamoDB table # This returns a dict with the sort key attribute as dict key. parameters = dynamodb_provider . get_multiple ( \"my-hash-key\" ) for k , v in parameters . items (): # k: param-a # v: \"my-value-a\" print ( f \" { k } : { v } \" ) parameters dict response 1 2 3 4 5 { \"param-a\" : \"my-value-a\" , \"param-b\" : \"my-value-b\" , \"param-c\" : \"my-value-c\" } Customizing DynamoDBProvider DynamoDB provider can be customized at initialization to match your table structure: Parameter Mandatory Default Description table_name Yes (N/A) Name of the DynamoDB table containing the parameter values. key_attr No id Hash key for the DynamoDB table. sort_attr No sk Range key for the DynamoDB table. You don't need to set this if you don't use the get_multiple() method. value_attr No value Name of the attribute containing the parameter value. Customizing DynamoDBProvider to suit your table design 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" , key_attr = \"MyKeyAttr\" , sort_attr = \"MySortAttr\" , value_attr = \"MyvalueAttr\" ) def handler ( event , context ): value = dynamodb_provider . get ( \"my-parameter\" ) AppConfigProvider \u00b6 Using AppConfigProvider 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) appconf_provider = parameters . AppConfigProvider ( environment = \"my_env\" , application = \"my_app\" , config = config ) def handler ( event , context ): # Retrieve a single secret value : bytes = appconf_provider . get ( \"my_conf\" ) Create your own provider \u00b6 You can create your own custom parameter store provider by inheriting the BaseProvider class, and implementing both _get() and _get_multiple() methods to retrieve a single, or multiple parameters from your custom store. All transformation and caching logic is handled by the get() and get_multiple() methods from the base provider class. Here is an example implementation using S3 as a custom parameter store: Creating a S3 Provider to fetch parameters 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 import copy from aws_lambda_powertools.utilities import BaseProvider import boto3 class S3Provider ( BaseProvider ): bucket_name = None client = None def __init__ ( self , bucket_name : str ): # Initialize the client to your custom parameter store # E.g.: self . bucket_name = bucket_name self . client = boto3 . client ( \"s3\" ) def _get ( self , name : str , ** sdk_options ) -> str : # Retrieve a single value # E.g.: sdk_options [ \"Bucket\" ] = self . bucket_name sdk_options [ \"Key\" ] = name response = self . client . get_object ( ** sdk_options ) return def _get_multiple ( self , path : str , ** sdk_options ) -> Dict [ str , str ]: # Retrieve multiple values # E.g.: list_sdk_options = copy . deepcopy ( sdk_options ) list_sdk_options [ \"Bucket\" ] = self . bucket_name list_sdk_options [ \"Prefix\" ] = path list_response = self . client . list_objects_v2 ( ** list_sdk_options ) parameters = {} for obj in list_response . get ( \"Contents\" , []): get_sdk_options = copy . deepcopy ( sdk_options ) get_sdk_options [ \"Bucket\" ] = self . bucket_name get_sdk_options [ \"Key\" ] = obj [ \"Key\" ] get_response = self . client . get_object ( ** get_sdk_options ) parameters [ obj [ \"Key\" ]] = get_response [ \"Body\" ] . read () . decode () return parameters Deserializing values with transform parameter \u00b6 For parameters stored in JSON or Base64 format, you can use the transform argument for deserialization. Info The transform argument is available across all providers, including the high level functions. High level functions 1 2 3 4 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): value_from_json = parameters . get_parameter ( \"/my/json/parameter\" , transform = \"json\" ) Providers 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): # Transform a JSON string value_from_json = ssm_provider . get ( \"/my/json/parameter\" , transform = \"json\" ) # Transform a Base64 encoded string value_from_binary = ssm_provider . get ( \"/my/binary/parameter\" , transform = \"binary\" ) Partial transform failures with get_multiple() \u00b6 If you use transform with get_multiple() , you can have a single malformed parameter value. To prevent failing the entire request, the method will return a None value for the parameters that failed to transform. You can override this by setting the raise_on_transform_error argument to True . If you do so, a single transform error will raise a TransformParameterError exception. For example, if you have three parameters, /param/a , /param/b and /param/c , but /param/c is malformed: Raising TransformParameterError at first malformed parameter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): # This will display: # /param/a: [some value] # /param/b: [some value] # /param/c: None values = ssm_provider . get_multiple ( \"/param\" , transform = \"json\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) try : # This will raise a TransformParameterError exception values = ssm_provider . get_multiple ( \"/param\" , transform = \"json\" , raise_on_transform_error = True ) except parameters . exceptions . TransformParameterError : ... Auto-transform values on suffix \u00b6 If you use transform with get_multiple() , you might want to retrieve and transform parameters encoded in different formats. You can do this with a single request by using transform=\"auto\" . This will instruct any Parameter to to infer its type based on the suffix and transform it accordingly. Info transform=\"auto\" feature is available across all providers, including the high level functions. Deserializing parameter values based on their suffix 1 2 3 4 5 6 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): values = ssm_provider . get_multiple ( \"/param\" , transform = \"auto\" ) For example, if you have two parameters with the following suffixes .json and .binary : Parameter name Parameter value /param/a.json [some encoded value] /param/a.binary [some encoded value] The return of ssm_provider.get_multiple(\"/param\", transform=\"auto\") call will be a dictionary like: 1 2 3 4 { \"a.json\" : [ some value ], \"b.binary\" : [ some value ] } Passing additional SDK arguments \u00b6 You can use arbitrary keyword arguments to pass it directly to the underlying SDK method. 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities import parameters secrets_provider = parameters . SecretsProvider () def handler ( event , context ): # The 'VersionId' argument will be passed to the underlying get_secret_value() call. value = secrets_provider . get ( \"my-secret\" , VersionId = \"e62ec170-6b01-48c7-94f3-d7497851a8d2\" ) Here is the mapping between this utility's functions and methods and the underlying SDK: Provider Function/Method Client name Function name SSM Parameter Store get_parameter ssm get_parameter SSM Parameter Store get_parameters ssm get_parameters_by_path SSM Parameter Store SSMProvider.get ssm get_parameter SSM Parameter Store SSMProvider.get_multiple ssm get_parameters_by_path Secrets Manager get_secret secretsmanager get_secret_value Secrets Manager SecretsManager.get secretsmanager get_secret_value DynamoDB DynamoDBProvider.get dynamodb ( Table resource ) DynamoDB DynamoDBProvider.get_multiple dynamodb ( Table resource ) App Config get_app_config appconfig get_configuration Customizing boto configuration \u00b6 The config and boto3_session parameters enable you to pass in a custom botocore config object or a custom boto3 session when constructing any of the built-in provider classes. Tip You can use a custom session for retrieving parameters cross-account/region and for snapshot testing. Custom session 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities import parameters import boto3 boto3_session = boto3 . session . Session () ssm_provider = parameters . SSMProvider ( boto3_session = boto3_session ) def handler ( event , context ): # Retrieve a single parameter value = ssm_provider . get ( \"/my/parameter\" ) ... Custom config 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities import parameters from botocore.config import Config boto_config = Config () ssm_provider = parameters . SSMProvider ( config = boto_config ) def handler ( event , context ): # Retrieve a single parameter value = ssm_provider . get ( \"/my/parameter\" ) ...","title":"Parameters"},{"location":"utilities/parameters/#key-features","text":"Retrieve one or multiple parameters from the underlying provider Cache parameter values for a given amount of time (defaults to 5 seconds) Transform parameter values from JSON or base 64 encoded strings Bring Your Own Parameter Store Provider","title":"Key features"},{"location":"utilities/parameters/#getting-started","text":"By default, we fetch parameters from System Manager Parameter Store, secrets from Secrets Manager, and application configuration from AppConfig.","title":"Getting started"},{"location":"utilities/parameters/#iam-permissions","text":"This utility requires additional permissions to work as expected. Note Different parameter providers require different permissions. Provider Function/Method IAM Permission SSM Parameter Store get_parameter , SSMProvider.get ssm:GetParameter SSM Parameter Store get_parameters , SSMProvider.get_multiple ssm:GetParametersByPath Secrets Manager get_secret , SecretsManager.get secretsmanager:GetSecretValue DynamoDB DynamoDBProvider.get dynamodb:GetItem DynamoDB DynamoDBProvider.get_multiple dynamodb:Query App Config AppConfigProvider.get_app_config , get_app_config appconfig:GetConfiguration","title":"IAM Permissions"},{"location":"utilities/parameters/#fetching-parameters","text":"You can retrieve a single parameter using get_parameter high-level function. For multiple parameters, you can use get_parameters and pass a path to retrieve them recursively. Fetching multiple parameters recursively 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single parameter value = parameters . get_parameter ( \"/my/parameter\" ) # Retrieve multiple parameters from a path prefix recursively # This returns a dict with the parameter name as key values = parameters . get_parameters ( \"/my/path/prefix\" ) for k , v in values . items (): print ( f \" { k } : { v } \" )","title":"Fetching parameters"},{"location":"utilities/parameters/#fetching-secrets","text":"You can fetch secrets stored in Secrets Manager using get_secrets . Fetching secrets 1 2 3 4 5 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single secret value = parameters . get_secret ( \"my-secret\" )","title":"Fetching secrets"},{"location":"utilities/parameters/#fetching-app-configurations","text":"You can fetch application configurations in AWS AppConfig using get_app_config . The following will retrieve the latest version and store it in the cache. Fetching latest config from AppConfig 1 2 3 4 5 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single configuration, latest version value : bytes = parameters . get_app_config ( name = \"my_configuration\" , environment = \"my_env\" , application = \"my_app\" )","title":"Fetching app configurations"},{"location":"utilities/parameters/#advanced","text":"","title":"Advanced"},{"location":"utilities/parameters/#adjusting-cache-ttl","text":"Tip max_age parameter is also available in high level functions like get_parameter , get_secret , etc. By default, we cache parameters retrieved in-memory for 5 seconds. You can adjust how long we should keep values in cache by using the param max_age , when using get() or get_multiple() methods across all providers. Caching parameter(s) value in memory for longer than 5 seconds 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) ssm_provider = parameters . SSMProvider ( config = config ) def handler ( event , context ): # Retrieve a single parameter value = ssm_provider . get ( \"/my/parameter\" , max_age = 60 ) # 1 minute # Retrieve multiple parameters from a path prefix values = ssm_provider . get_multiple ( \"/my/path/prefix\" , max_age = 60 ) for k , v in values . items (): print ( f \" { k } : { v } \" )","title":"Adjusting cache TTL"},{"location":"utilities/parameters/#always-fetching-the-latest","text":"If you'd like to always ensure you fetch the latest parameter from the store regardless if already available in cache, use force_fetch param. Forcefully fetching the latest parameter whether TTL has expired or not 1 2 3 4 5 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single parameter value = parameters . get_parameter ( \"/my/parameter\" , force_fetch = True )","title":"Always fetching the latest"},{"location":"utilities/parameters/#built-in-provider-class","text":"For greater flexibility such as configuring the underlying SDK client used by built-in providers, you can use their respective Provider Classes directly. Tip This can be used to retrieve values from other regions, change the retry behavior, etc.","title":"Built-in provider class"},{"location":"utilities/parameters/#ssmprovider","text":"Example with SSMProvider for further extensibility 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) ssm_provider = parameters . SSMProvider ( config = config ) # or boto3_session=boto3.Session() def handler ( event , context ): # Retrieve a single parameter value = ssm_provider . get ( \"/my/parameter\" ) # Retrieve multiple parameters from a path prefix values = ssm_provider . get_multiple ( \"/my/path/prefix\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) The AWS Systems Manager Parameter Store provider supports two additional arguments for the get() and get_multiple() methods: Parameter Default Description decrypt False Will automatically decrypt the parameter. recursive True For get_multiple() only, will fetch all parameter values recursively based on a path prefix. Example with get() and get_multiple() 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): decrypted_value = ssm_provider . get ( \"/my/encrypted/parameter\" , decrypt = True ) no_recursive_values = ssm_provider . get_multiple ( \"/my/path/prefix\" , recursive = False )","title":"SSMProvider"},{"location":"utilities/parameters/#secretsprovider","text":"Example with SecretsProvider for further extensibility 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) secrets_provider = parameters . SecretsProvider ( config = config ) def handler ( event , context ): # Retrieve a single secret value = secrets_provider . get ( \"my-secret\" )","title":"SecretsProvider"},{"location":"utilities/parameters/#dynamodbprovider","text":"The DynamoDB Provider does not have any high-level functions, as it needs to know the name of the DynamoDB table containing the parameters. DynamoDB table structure for single parameters For single parameters, you must use id as the partition key for that table. Example DynamoDB table with id partition key and value as attribute id value my-parameter my-value With this table, dynamodb_provider.get(\"my-param\") will return my-value . app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" ) def handler ( event , context ): # Retrieve a value from DynamoDB value = dynamodb_provider . get ( \"my-parameter\" ) DynamoDB Local example You can initialize the DynamoDB provider pointing to DynamoDB Local using endpoint_url parameter: 1 2 3 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" , endpoint_url = \"http://localhost:8000\" ) DynamoDB table structure for multiple values parameters You can retrieve multiple parameters sharing the same id by having a sort key named sk . Example DynamoDB table with id primary key, sk as sort key and value` as attribute id sk value my-hash-key param-a my-value-a my-hash-key param-b my-value-b my-hash-key param-c my-value-c With this table, dynamodb_provider.get_multiple(\"my-hash-key\") will return a dictionary response in the shape of sk:value . app.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" ) def handler ( event , context ): # Retrieve multiple values by performing a Query on the DynamoDB table # This returns a dict with the sort key attribute as dict key. parameters = dynamodb_provider . get_multiple ( \"my-hash-key\" ) for k , v in parameters . items (): # k: param-a # v: \"my-value-a\" print ( f \" { k } : { v } \" ) parameters dict response 1 2 3 4 5 { \"param-a\" : \"my-value-a\" , \"param-b\" : \"my-value-b\" , \"param-c\" : \"my-value-c\" } Customizing DynamoDBProvider DynamoDB provider can be customized at initialization to match your table structure: Parameter Mandatory Default Description table_name Yes (N/A) Name of the DynamoDB table containing the parameter values. key_attr No id Hash key for the DynamoDB table. sort_attr No sk Range key for the DynamoDB table. You don't need to set this if you don't use the get_multiple() method. value_attr No value Name of the attribute containing the parameter value. Customizing DynamoDBProvider to suit your table design 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" , key_attr = \"MyKeyAttr\" , sort_attr = \"MySortAttr\" , value_attr = \"MyvalueAttr\" ) def handler ( event , context ): value = dynamodb_provider . get ( \"my-parameter\" )","title":"DynamoDBProvider"},{"location":"utilities/parameters/#appconfigprovider","text":"Using AppConfigProvider 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) appconf_provider = parameters . AppConfigProvider ( environment = \"my_env\" , application = \"my_app\" , config = config ) def handler ( event , context ): # Retrieve a single secret value : bytes = appconf_provider . get ( \"my_conf\" )","title":"AppConfigProvider"},{"location":"utilities/parameters/#create-your-own-provider","text":"You can create your own custom parameter store provider by inheriting the BaseProvider class, and implementing both _get() and _get_multiple() methods to retrieve a single, or multiple parameters from your custom store. All transformation and caching logic is handled by the get() and get_multiple() methods from the base provider class. Here is an example implementation using S3 as a custom parameter store: Creating a S3 Provider to fetch parameters 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 import copy from aws_lambda_powertools.utilities import BaseProvider import boto3 class S3Provider ( BaseProvider ): bucket_name = None client = None def __init__ ( self , bucket_name : str ): # Initialize the client to your custom parameter store # E.g.: self . bucket_name = bucket_name self . client = boto3 . client ( \"s3\" ) def _get ( self , name : str , ** sdk_options ) -> str : # Retrieve a single value # E.g.: sdk_options [ \"Bucket\" ] = self . bucket_name sdk_options [ \"Key\" ] = name response = self . client . get_object ( ** sdk_options ) return def _get_multiple ( self , path : str , ** sdk_options ) -> Dict [ str , str ]: # Retrieve multiple values # E.g.: list_sdk_options = copy . deepcopy ( sdk_options ) list_sdk_options [ \"Bucket\" ] = self . bucket_name list_sdk_options [ \"Prefix\" ] = path list_response = self . client . list_objects_v2 ( ** list_sdk_options ) parameters = {} for obj in list_response . get ( \"Contents\" , []): get_sdk_options = copy . deepcopy ( sdk_options ) get_sdk_options [ \"Bucket\" ] = self . bucket_name get_sdk_options [ \"Key\" ] = obj [ \"Key\" ] get_response = self . client . get_object ( ** get_sdk_options ) parameters [ obj [ \"Key\" ]] = get_response [ \"Body\" ] . read () . decode () return parameters","title":"Create your own provider"},{"location":"utilities/parameters/#deserializing-values-with-transform-parameter","text":"For parameters stored in JSON or Base64 format, you can use the transform argument for deserialization. Info The transform argument is available across all providers, including the high level functions. High level functions 1 2 3 4 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): value_from_json = parameters . get_parameter ( \"/my/json/parameter\" , transform = \"json\" ) Providers 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): # Transform a JSON string value_from_json = ssm_provider . get ( \"/my/json/parameter\" , transform = \"json\" ) # Transform a Base64 encoded string value_from_binary = ssm_provider . get ( \"/my/binary/parameter\" , transform = \"binary\" )","title":"Deserializing values with transform parameter"},{"location":"utilities/parameters/#partial-transform-failures-with-get_multiple","text":"If you use transform with get_multiple() , you can have a single malformed parameter value. To prevent failing the entire request, the method will return a None value for the parameters that failed to transform. You can override this by setting the raise_on_transform_error argument to True . If you do so, a single transform error will raise a TransformParameterError exception. For example, if you have three parameters, /param/a , /param/b and /param/c , but /param/c is malformed: Raising TransformParameterError at first malformed parameter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): # This will display: # /param/a: [some value] # /param/b: [some value] # /param/c: None values = ssm_provider . get_multiple ( \"/param\" , transform = \"json\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) try : # This will raise a TransformParameterError exception values = ssm_provider . get_multiple ( \"/param\" , transform = \"json\" , raise_on_transform_error = True ) except parameters . exceptions . TransformParameterError : ...","title":"Partial transform failures with get_multiple()"},{"location":"utilities/parameters/#auto-transform-values-on-suffix","text":"If you use transform with get_multiple() , you might want to retrieve and transform parameters encoded in different formats. You can do this with a single request by using transform=\"auto\" . This will instruct any Parameter to to infer its type based on the suffix and transform it accordingly. Info transform=\"auto\" feature is available across all providers, including the high level functions. Deserializing parameter values based on their suffix 1 2 3 4 5 6 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): values = ssm_provider . get_multiple ( \"/param\" , transform = \"auto\" ) For example, if you have two parameters with the following suffixes .json and .binary : Parameter name Parameter value /param/a.json [some encoded value] /param/a.binary [some encoded value] The return of ssm_provider.get_multiple(\"/param\", transform=\"auto\") call will be a dictionary like: 1 2 3 4 { \"a.json\" : [ some value ], \"b.binary\" : [ some value ] }","title":"Auto-transform values on suffix"},{"location":"utilities/parameters/#passing-additional-sdk-arguments","text":"You can use arbitrary keyword arguments to pass it directly to the underlying SDK method. 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities import parameters secrets_provider = parameters . SecretsProvider () def handler ( event , context ): # The 'VersionId' argument will be passed to the underlying get_secret_value() call. value = secrets_provider . get ( \"my-secret\" , VersionId = \"e62ec170-6b01-48c7-94f3-d7497851a8d2\" ) Here is the mapping between this utility's functions and methods and the underlying SDK: Provider Function/Method Client name Function name SSM Parameter Store get_parameter ssm get_parameter SSM Parameter Store get_parameters ssm get_parameters_by_path SSM Parameter Store SSMProvider.get ssm get_parameter SSM Parameter Store SSMProvider.get_multiple ssm get_parameters_by_path Secrets Manager get_secret secretsmanager get_secret_value Secrets Manager SecretsManager.get secretsmanager get_secret_value DynamoDB DynamoDBProvider.get dynamodb ( Table resource ) DynamoDB DynamoDBProvider.get_multiple dynamodb ( Table resource ) App Config get_app_config appconfig get_configuration","title":"Passing additional SDK arguments"},{"location":"utilities/parameters/#customizing-boto-configuration","text":"The config and boto3_session parameters enable you to pass in a custom botocore config object or a custom boto3 session when constructing any of the built-in provider classes. Tip You can use a custom session for retrieving parameters cross-account/region and for snapshot testing. Custom session 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities import parameters import boto3 boto3_session = boto3 . session . Session () ssm_provider = parameters . SSMProvider ( boto3_session = boto3_session ) def handler ( event , context ): # Retrieve a single parameter value = ssm_provider . get ( \"/my/parameter\" ) ... Custom config 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities import parameters from botocore.config import Config boto_config = Config () ssm_provider = parameters . SSMProvider ( config = boto_config ) def handler ( event , context ): # Retrieve a single parameter value = ssm_provider . get ( \"/my/parameter\" ) ...","title":"Customizing boto configuration"},{"location":"utilities/parser/","text":"This utility provides data parsing and deep validation using Pydantic . Key features \u00b6 Defines data in pure Python classes, then parse, validate and extract only what you want Built-in envelopes to unwrap, extend, and validate popular event sources payloads Enforces type hints at runtime with user-friendly errors Extra dependency Warning This will increase the overall package size by approximately 75MB due to Pydantic dependency. Install parser's extra dependencies using pip install aws-lambda-powertools[pydantic] . Defining models \u00b6 You can define models to parse incoming events by inheriting from BaseModel . Defining an Order data model 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.parser import BaseModel from typing import List , Optional class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing These are simply Python classes that inherit from BaseModel. Parser enforces type hints declared in your model at runtime. Parsing events \u00b6 You can parse inbound events using event_parser decorator, or the standalone parse function. Both are also able to parse either dictionary or JSON string as an input. event_parser decorator \u00b6 Use the decorator for fail fast scenarios where you want your Lambda function to raise an exception in the event of a malformed payload. event_parser decorator will throw a ValidationError if your event cannot be parsed according to the model. Note This decorator will replace the event object with the parsed model if successful . This means you might be careful when nesting other decorators that expect event to be a dict . Parsing and validating upon invocation with event_parser decorator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from aws_lambda_powertools.utilities.parser import event_parser , BaseModel from aws_lambda_powertools.utilities.typing import LambdaContext from typing import List , Optional import json class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing @event_parser ( model = Order ) def handler ( event : Order , context : LambdaContext ): print ( event . id ) print ( event . description ) print ( event . items ) order_items = [ item for item in event . items ] ... payload = { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { \"id\" : 1015938732 , \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } handler ( event = payload , context = LambdaContext ()) handler ( event = json . dumps ( payload ), context = LambdaContext ()) # also works if event is a JSON string parse function \u00b6 Use this standalone function when you want more control over the data validation process, for example returning a 400 error for malformed payloads. Using standalone parse function for more flexibility 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 from aws_lambda_powertools.utilities.parser import parse , BaseModel , ValidationError from typing import List , Optional class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing payload = { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { # this will cause a validation error \"id\" : [ 1015938732 ], \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } def my_function (): try : parsed_payload : Order = parse ( event = payload , model = Order ) # payload dict is now parsed into our model return parsed_payload . items except ValidationError : return { \"status_code\" : 400 , \"message\" : \"Invalid order\" } Built-in models \u00b6 Parser comes with the following built-in models: Model name Description DynamoDBStreamModel Lambda Event Source payload for Amazon DynamoDB Streams EventBridgeModel Lambda Event Source payload for Amazon EventBridge SqsModel Lambda Event Source payload for Amazon SQS AlbModel Lambda Event Source payload for Amazon Application Load Balancer CloudwatchLogsModel Lambda Event Source payload for Amazon CloudWatch Logs S3Model Lambda Event Source payload for Amazon S3 S3ObjectLambdaEvent Lambda Event Source payload for Amazon S3 Object Lambda KinesisDataStreamModel Lambda Event Source payload for Amazon Kinesis Data Streams SesModel Lambda Event Source payload for Amazon Simple Email Service SnsModel Lambda Event Source payload for Amazon Simple Notification Service APIGatewayProxyEvent Lambda Event Source payload for Amazon API Gateway APIGatewayProxyEventV2Model Lambda Event Source payload for Amazon API Gateway v2 payload extending built-in models \u00b6 You can extend them to include your own models, and yet have all other known fields parsed along the way. Tip For Mypy users, we only allow type override for fields where payload is injected e.g. detail , body , etc. Extending EventBridge model as an example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 from aws_lambda_powertools.utilities.parser import parse , BaseModel from aws_lambda_powertools.utilities.parser.models import EventBridgeModel from typing import List , Optional class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] class OrderEventModel ( EventBridgeModel ): detail : Order payload = { \"version\" : \"0\" , \"id\" : \"6a7e8feb-b491-4cf7-a9f1-bf3703467718\" , \"detail-type\" : \"OrderPurchased\" , \"source\" : \"OrderService\" , \"account\" : \"111122223333\" , \"time\" : \"2020-10-22T18:43:48Z\" , \"region\" : \"us-west-1\" , \"resources\" : [ \"some_additional\" ], \"detail\" : { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { \"id\" : 1015938732 , \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } } ret = parse ( model = OrderEventModel , event = payload ) assert ret . source == \"OrderService\" assert ret . detail . description == \"My order\" assert ret . detail_type == \"OrderPurchased\" # we rename it to snake_case since detail-type is an invalid name for order_item in ret . detail . items : ... What's going on here, you might ask : We imported our built-in model EventBridgeModel from the parser utility Defined how our Order should look like Defined how part of our EventBridge event should look like by overriding detail key within our OrderEventModel Parser parsed the original event against OrderEventModel Envelopes \u00b6 When trying to parse your payloads wrapped in a known structure, you might encounter the following situations: Your actual payload is wrapped around a known structure, for example Lambda Event Sources like EventBridge You're only interested in a portion of the payload, for example parsing the detail of custom events in EventBridge, or body of SQS records You can either solve these situations by creating a model of these known structures, parsing them, then extracting and parsing a key where your payload is. This can become difficult quite quickly. Parser makes this problem easier through a feature named Envelope . Envelopes can be used via envelope parameter available in both parse function and event_parser decorator. Here's an example of parsing a model found in an event coming from EventBridge, where all you want is what's inside the detail key. Parsing payload in a given key only using envelope feature 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from aws_lambda_powertools.utilities.parser import event_parser , parse , BaseModel , envelopes from aws_lambda_powertools.utilities.typing import LambdaContext class UserModel ( BaseModel ): username : str password1 : str password2 : str payload = { \"version\" : \"0\" , \"id\" : \"6a7e8feb-b491-4cf7-a9f1-bf3703467718\" , \"detail-type\" : \"CustomerSignedUp\" , \"source\" : \"CustomerService\" , \"account\" : \"111122223333\" , \"time\" : \"2020-10-22T18:43:48Z\" , \"region\" : \"us-west-1\" , \"resources\" : [ \"some_additional_\" ], \"detail\" : { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } } ret = parse ( model = UserModel , envelope = envelopes . EventBridgeEnvelope , event = payload ) # Parsed model only contains our actual model, not the entire EventBridge + Payload parsed assert ret . password1 == ret . password2 # Same behaviour but using our decorator @event_parser ( model = UserModel , envelope = envelopes . EventBridgeEnvelope ) def handler ( event : UserModel , context : LambdaContext ): assert event . password1 == event . password2 What's going on here, you might ask : We imported built-in envelopes from the parser utility Used envelopes.EventBridgeEnvelope as the envelope for our UserModel model Parser parsed the original event against the EventBridge model Parser then parsed the detail key using UserModel Built-in envelopes \u00b6 Parser comes with the following built-in envelopes, where Model in the return section is your given model. Envelope name Behaviour Return DynamoDBStreamEnvelope 1. Parses data using DynamoDBStreamModel . 2. Parses records in NewImage and OldImage keys using your model. 3. Returns a list with a dictionary containing NewImage and OldImage keys List[Dict[str, Optional[Model]]] EventBridgeEnvelope 1. Parses data using EventBridgeModel . 2. Parses detail key using your model and returns it. Model SqsEnvelope 1. Parses data using SqsModel . 2. Parses records in body key using your model and return them in a list. List[Model] CloudWatchLogsEnvelope 1. Parses data using CloudwatchLogsModel which will base64 decode and decompress it. 2. Parses records in message key using your model and return them in a list. List[Model] KinesisDataStreamEnvelope 1. Parses data using KinesisDataStreamModel which will base64 decode it. 2. Parses records in in Records key using your model and returns them in a list. List[Model] SnsEnvelope 1. Parses data using SnsModel . 2. Parses records in body key using your model and return them in a list. List[Model] SnsSqsEnvelope 1. Parses data using SqsModel . 2. Parses SNS records in body key using SnsNotificationModel . 3. Parses data in Message key using your model and return them in a list. List[Model] ApiGatewayEnvelope 1. Parses data using APIGatewayProxyEventModel . 2. Parses body key using your model and returns it. Model ApiGatewayV2Envelope 1. Parses data using APIGatewayProxyEventV2Model . 2. Parses body key using your model and returns it. Model Bringing your own envelope \u00b6 You can create your own Envelope model and logic by inheriting from BaseEnvelope , and implementing the parse method. Here's a snippet of how the EventBridge envelope we demonstrated previously is implemented. EventBridge Model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from datetime import datetime from typing import Any , Dict , List from aws_lambda_powertools.utilities.parser import BaseModel , Field class EventBridgeModel ( BaseModel ): version : str id : str # noqa: A003,VNE003 source : str account : str time : datetime region : str resources : List [ str ] detail_type : str = Field ( None , alias = \"detail-type\" ) detail : Dict [ str , Any ] EventBridge Envelope 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from aws_lambda_powertools.utilities.parser import BaseEnvelope , models from aws_lambda_powertools.utilities.parser.models import EventBridgeModel from typing import Any , Dict , Optional , TypeVar Model = TypeVar ( \"Model\" , bound = BaseModel ) class EventBridgeEnvelope ( BaseEnvelope ): def parse ( self , data : Optional [ Union [ Dict [ str , Any ], Any ]], model : Model ) -> Optional [ Model ]: \"\"\"Parses data found with model provided Parameters ---------- data : Dict Lambda event to be parsed model : Model Data model provided to parse after extracting data using envelope Returns ------- Any Parsed detail payload with model provided \"\"\" parsed_envelope = EventBridgeModel . parse_obj ( data ) return self . _parse ( data = parsed_envelope . detail , model = model ) What's going on here, you might ask : We defined an envelope named EventBridgeEnvelope inheriting from BaseEnvelope Implemented the parse abstract method taking data and model as parameters Then, we parsed the incoming data with our envelope to confirm it matches EventBridge's structure defined in EventBridgeModel Lastly, we call _parse from BaseEnvelope to parse the data in our envelope (.detail) using the customer model Data model validation \u00b6 Warning This is radically different from the Validator utility which validates events against JSON Schema. You can use parser's validator for deep inspection of object values and complex relationships. There are two types of class method decorators you can use: validator - Useful to quickly validate an individual field and its value root_validator - Useful to validate the entire model's data Keep the following in mind regardless of which decorator you end up using it: You must raise either ValueError , TypeError , or AssertionError when value is not compliant You must return the value(s) itself if compliant validating fields \u00b6 Quick validation to verify whether the field message has the value of hello world . Data field validation with validator 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.parser import parse , BaseModel , validator class HelloWorldModel ( BaseModel ): message : str @validator ( 'message' ) def is_hello_world ( cls , v ): if v != \"hello world\" : raise ValueError ( \"Message must be hello world!\" ) return v parse ( model = HelloWorldModel , event = { \"message\" : \"hello universe\" }) If you run as-is, you should expect the following error with the message we provided in our exception: Sample validation error message 1 2 message Message must be hello world ! ( type = value_error ) Alternatively, you can pass '*' as an argument for the decorator so that you can validate every value available. Validating all data fields with custom logic 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.parser import parse , BaseModel , validator class HelloWorldModel ( BaseModel ): message : str sender : str @validator ( '*' ) def has_whitespace ( cls , v ): if ' ' not in v : raise ValueError ( \"Must have whitespace...\" ) return v parse ( model = HelloWorldModel , event = { \"message\" : \"hello universe\" , \"sender\" : \"universe\" }) validating entire model \u00b6 root_validator can help when you have a complex validation mechanism. For example finding whether data has been omitted, comparing field values, etc. Comparing and validating multiple fields at once with root_validator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from aws_lambda_powertools.utilities.parser import parse , BaseModel , root_validator class UserModel ( BaseModel ): username : str password1 : str password2 : str @root_validator def check_passwords_match ( cls , values ): pw1 , pw2 = values . get ( 'password1' ), values . get ( 'password2' ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( 'passwords do not match' ) return values payload = { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } parse ( model = UserModel , event = payload ) Info You can read more about validating list items, reusing validators, validating raw inputs, and a lot more in Pydantic's documentation . Advanced use cases \u00b6 Tip: Looking to auto-generate models from JSON, YAML, JSON Schemas, OpenApi, etc? Use Koudai Aono's data model code generation tool for Pydantic There are number of advanced use cases well documented in Pydantic's doc such as creating immutable models , declaring fields with dynamic values ) e.g. UUID, and helper functions to parse models from files, str , etc. Two possible unknown use cases are Models and exception' serialization. Models have methods to export them as dict , JSON , JSON Schema , and Validation exceptions can be exported as JSON. Converting data models in various formats 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools.utilities import Logger from aws_lambda_powertools.utilities.parser import parse , BaseModel , ValidationError , validator logger = Logger ( service = \"user\" ) class UserModel ( BaseModel ): username : str password1 : str password2 : str payload = { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } def my_function (): try : return parse ( model = UserModel , event = payload ) except ValidationError as e : logger . exception ( e . json ()) return { \"status_code\" : 400 , \"message\" : \"Invalid username\" } User : UserModel = my_function () user_dict = User . dict () user_json = User . json () user_json_schema_as_dict = User . schema () user_json_schema_as_json = User . schema_json ( indent = 2 ) These can be quite useful when manipulating models that later need to be serialized as inputs for services like DynamoDB, EventBridge, etc. FAQ \u00b6 When should I use parser vs data_classes utility? Use data classes utility when you're after autocomplete, self-documented attributes and helpers to extract data from common event sources. Parser is best suited for those looking for a trade-off between defining their models for deep validation, parsing and autocomplete for an additional dependency to be brought in. How do I import X from Pydantic? We export most common classes, exceptions, and utilities from Pydantic as part of parser e.g. from aws_lambda_powertools.utilities.parser import BaseModel . If what's your trying to use isn't available as part of the high level import system, use the following escape hatch mechanism: Pydantic import escape hatch 1 from aws_lambda_powertools.utilities.parser.pydantic import < what you 'd like to import' > What is the cold start impact in bringing this additional dependency? No significant cold start impact. It does increase the final uncompressed package by 71M , when you bring the additional dependency that parser requires. Artillery load test sample against a hello world sample using Tracer, Metrics, and Logger with and without parser. No parser Info Uncompressed package size : 55M, p99 : 180.3ms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Summary report @ 14:36:07(+0200) 2020-10-23 Scenarios launched: 10 Scenarios completed: 10 Requests completed: 2000 Mean response/sec: 114.81 Response time (msec): min: 54.9 max: 1684.9 median: 68 p95: 109.1 p99: 180.3 Scenario counts: 0: 10 (100%) Codes: 200: 2000 With parser Info Uncompressed package size : 128M, p99 : 193.1ms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Summary report @ 14:29:23(+0200) 2020-10-23 Scenarios launched: 10 Scenarios completed: 10 Requests completed: 2000 Mean response/sec: 111.67 Response time (msec): min: 54.3 max: 1887.2 median: 66.1 p95: 113.3 p99: 193.1 Scenario counts: 0: 10 (100%) Codes: 200: 2000","title":"Parser"},{"location":"utilities/parser/#key-features","text":"Defines data in pure Python classes, then parse, validate and extract only what you want Built-in envelopes to unwrap, extend, and validate popular event sources payloads Enforces type hints at runtime with user-friendly errors Extra dependency Warning This will increase the overall package size by approximately 75MB due to Pydantic dependency. Install parser's extra dependencies using pip install aws-lambda-powertools[pydantic] .","title":"Key features"},{"location":"utilities/parser/#defining-models","text":"You can define models to parse incoming events by inheriting from BaseModel . Defining an Order data model 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.parser import BaseModel from typing import List , Optional class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing These are simply Python classes that inherit from BaseModel. Parser enforces type hints declared in your model at runtime.","title":"Defining models"},{"location":"utilities/parser/#parsing-events","text":"You can parse inbound events using event_parser decorator, or the standalone parse function. Both are also able to parse either dictionary or JSON string as an input.","title":"Parsing events"},{"location":"utilities/parser/#event_parser-decorator","text":"Use the decorator for fail fast scenarios where you want your Lambda function to raise an exception in the event of a malformed payload. event_parser decorator will throw a ValidationError if your event cannot be parsed according to the model. Note This decorator will replace the event object with the parsed model if successful . This means you might be careful when nesting other decorators that expect event to be a dict . Parsing and validating upon invocation with event_parser decorator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from aws_lambda_powertools.utilities.parser import event_parser , BaseModel from aws_lambda_powertools.utilities.typing import LambdaContext from typing import List , Optional import json class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing @event_parser ( model = Order ) def handler ( event : Order , context : LambdaContext ): print ( event . id ) print ( event . description ) print ( event . items ) order_items = [ item for item in event . items ] ... payload = { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { \"id\" : 1015938732 , \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } handler ( event = payload , context = LambdaContext ()) handler ( event = json . dumps ( payload ), context = LambdaContext ()) # also works if event is a JSON string","title":"event_parser decorator"},{"location":"utilities/parser/#parse-function","text":"Use this standalone function when you want more control over the data validation process, for example returning a 400 error for malformed payloads. Using standalone parse function for more flexibility 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 from aws_lambda_powertools.utilities.parser import parse , BaseModel , ValidationError from typing import List , Optional class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing payload = { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { # this will cause a validation error \"id\" : [ 1015938732 ], \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } def my_function (): try : parsed_payload : Order = parse ( event = payload , model = Order ) # payload dict is now parsed into our model return parsed_payload . items except ValidationError : return { \"status_code\" : 400 , \"message\" : \"Invalid order\" }","title":"parse function"},{"location":"utilities/parser/#built-in-models","text":"Parser comes with the following built-in models: Model name Description DynamoDBStreamModel Lambda Event Source payload for Amazon DynamoDB Streams EventBridgeModel Lambda Event Source payload for Amazon EventBridge SqsModel Lambda Event Source payload for Amazon SQS AlbModel Lambda Event Source payload for Amazon Application Load Balancer CloudwatchLogsModel Lambda Event Source payload for Amazon CloudWatch Logs S3Model Lambda Event Source payload for Amazon S3 S3ObjectLambdaEvent Lambda Event Source payload for Amazon S3 Object Lambda KinesisDataStreamModel Lambda Event Source payload for Amazon Kinesis Data Streams SesModel Lambda Event Source payload for Amazon Simple Email Service SnsModel Lambda Event Source payload for Amazon Simple Notification Service APIGatewayProxyEvent Lambda Event Source payload for Amazon API Gateway APIGatewayProxyEventV2Model Lambda Event Source payload for Amazon API Gateway v2 payload","title":"Built-in models"},{"location":"utilities/parser/#extending-built-in-models","text":"You can extend them to include your own models, and yet have all other known fields parsed along the way. Tip For Mypy users, we only allow type override for fields where payload is injected e.g. detail , body , etc. Extending EventBridge model as an example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 from aws_lambda_powertools.utilities.parser import parse , BaseModel from aws_lambda_powertools.utilities.parser.models import EventBridgeModel from typing import List , Optional class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] class OrderEventModel ( EventBridgeModel ): detail : Order payload = { \"version\" : \"0\" , \"id\" : \"6a7e8feb-b491-4cf7-a9f1-bf3703467718\" , \"detail-type\" : \"OrderPurchased\" , \"source\" : \"OrderService\" , \"account\" : \"111122223333\" , \"time\" : \"2020-10-22T18:43:48Z\" , \"region\" : \"us-west-1\" , \"resources\" : [ \"some_additional\" ], \"detail\" : { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { \"id\" : 1015938732 , \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } } ret = parse ( model = OrderEventModel , event = payload ) assert ret . source == \"OrderService\" assert ret . detail . description == \"My order\" assert ret . detail_type == \"OrderPurchased\" # we rename it to snake_case since detail-type is an invalid name for order_item in ret . detail . items : ... What's going on here, you might ask : We imported our built-in model EventBridgeModel from the parser utility Defined how our Order should look like Defined how part of our EventBridge event should look like by overriding detail key within our OrderEventModel Parser parsed the original event against OrderEventModel","title":"extending built-in models"},{"location":"utilities/parser/#envelopes","text":"When trying to parse your payloads wrapped in a known structure, you might encounter the following situations: Your actual payload is wrapped around a known structure, for example Lambda Event Sources like EventBridge You're only interested in a portion of the payload, for example parsing the detail of custom events in EventBridge, or body of SQS records You can either solve these situations by creating a model of these known structures, parsing them, then extracting and parsing a key where your payload is. This can become difficult quite quickly. Parser makes this problem easier through a feature named Envelope . Envelopes can be used via envelope parameter available in both parse function and event_parser decorator. Here's an example of parsing a model found in an event coming from EventBridge, where all you want is what's inside the detail key. Parsing payload in a given key only using envelope feature 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from aws_lambda_powertools.utilities.parser import event_parser , parse , BaseModel , envelopes from aws_lambda_powertools.utilities.typing import LambdaContext class UserModel ( BaseModel ): username : str password1 : str password2 : str payload = { \"version\" : \"0\" , \"id\" : \"6a7e8feb-b491-4cf7-a9f1-bf3703467718\" , \"detail-type\" : \"CustomerSignedUp\" , \"source\" : \"CustomerService\" , \"account\" : \"111122223333\" , \"time\" : \"2020-10-22T18:43:48Z\" , \"region\" : \"us-west-1\" , \"resources\" : [ \"some_additional_\" ], \"detail\" : { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } } ret = parse ( model = UserModel , envelope = envelopes . EventBridgeEnvelope , event = payload ) # Parsed model only contains our actual model, not the entire EventBridge + Payload parsed assert ret . password1 == ret . password2 # Same behaviour but using our decorator @event_parser ( model = UserModel , envelope = envelopes . EventBridgeEnvelope ) def handler ( event : UserModel , context : LambdaContext ): assert event . password1 == event . password2 What's going on here, you might ask : We imported built-in envelopes from the parser utility Used envelopes.EventBridgeEnvelope as the envelope for our UserModel model Parser parsed the original event against the EventBridge model Parser then parsed the detail key using UserModel","title":"Envelopes"},{"location":"utilities/parser/#built-in-envelopes","text":"Parser comes with the following built-in envelopes, where Model in the return section is your given model. Envelope name Behaviour Return DynamoDBStreamEnvelope 1. Parses data using DynamoDBStreamModel . 2. Parses records in NewImage and OldImage keys using your model. 3. Returns a list with a dictionary containing NewImage and OldImage keys List[Dict[str, Optional[Model]]] EventBridgeEnvelope 1. Parses data using EventBridgeModel . 2. Parses detail key using your model and returns it. Model SqsEnvelope 1. Parses data using SqsModel . 2. Parses records in body key using your model and return them in a list. List[Model] CloudWatchLogsEnvelope 1. Parses data using CloudwatchLogsModel which will base64 decode and decompress it. 2. Parses records in message key using your model and return them in a list. List[Model] KinesisDataStreamEnvelope 1. Parses data using KinesisDataStreamModel which will base64 decode it. 2. Parses records in in Records key using your model and returns them in a list. List[Model] SnsEnvelope 1. Parses data using SnsModel . 2. Parses records in body key using your model and return them in a list. List[Model] SnsSqsEnvelope 1. Parses data using SqsModel . 2. Parses SNS records in body key using SnsNotificationModel . 3. Parses data in Message key using your model and return them in a list. List[Model] ApiGatewayEnvelope 1. Parses data using APIGatewayProxyEventModel . 2. Parses body key using your model and returns it. Model ApiGatewayV2Envelope 1. Parses data using APIGatewayProxyEventV2Model . 2. Parses body key using your model and returns it. Model","title":"Built-in envelopes"},{"location":"utilities/parser/#bringing-your-own-envelope","text":"You can create your own Envelope model and logic by inheriting from BaseEnvelope , and implementing the parse method. Here's a snippet of how the EventBridge envelope we demonstrated previously is implemented. EventBridge Model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from datetime import datetime from typing import Any , Dict , List from aws_lambda_powertools.utilities.parser import BaseModel , Field class EventBridgeModel ( BaseModel ): version : str id : str # noqa: A003,VNE003 source : str account : str time : datetime region : str resources : List [ str ] detail_type : str = Field ( None , alias = \"detail-type\" ) detail : Dict [ str , Any ] EventBridge Envelope 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from aws_lambda_powertools.utilities.parser import BaseEnvelope , models from aws_lambda_powertools.utilities.parser.models import EventBridgeModel from typing import Any , Dict , Optional , TypeVar Model = TypeVar ( \"Model\" , bound = BaseModel ) class EventBridgeEnvelope ( BaseEnvelope ): def parse ( self , data : Optional [ Union [ Dict [ str , Any ], Any ]], model : Model ) -> Optional [ Model ]: \"\"\"Parses data found with model provided Parameters ---------- data : Dict Lambda event to be parsed model : Model Data model provided to parse after extracting data using envelope Returns ------- Any Parsed detail payload with model provided \"\"\" parsed_envelope = EventBridgeModel . parse_obj ( data ) return self . _parse ( data = parsed_envelope . detail , model = model ) What's going on here, you might ask : We defined an envelope named EventBridgeEnvelope inheriting from BaseEnvelope Implemented the parse abstract method taking data and model as parameters Then, we parsed the incoming data with our envelope to confirm it matches EventBridge's structure defined in EventBridgeModel Lastly, we call _parse from BaseEnvelope to parse the data in our envelope (.detail) using the customer model","title":"Bringing your own envelope"},{"location":"utilities/parser/#data-model-validation","text":"Warning This is radically different from the Validator utility which validates events against JSON Schema. You can use parser's validator for deep inspection of object values and complex relationships. There are two types of class method decorators you can use: validator - Useful to quickly validate an individual field and its value root_validator - Useful to validate the entire model's data Keep the following in mind regardless of which decorator you end up using it: You must raise either ValueError , TypeError , or AssertionError when value is not compliant You must return the value(s) itself if compliant","title":"Data model validation"},{"location":"utilities/parser/#validating-fields","text":"Quick validation to verify whether the field message has the value of hello world . Data field validation with validator 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.parser import parse , BaseModel , validator class HelloWorldModel ( BaseModel ): message : str @validator ( 'message' ) def is_hello_world ( cls , v ): if v != \"hello world\" : raise ValueError ( \"Message must be hello world!\" ) return v parse ( model = HelloWorldModel , event = { \"message\" : \"hello universe\" }) If you run as-is, you should expect the following error with the message we provided in our exception: Sample validation error message 1 2 message Message must be hello world ! ( type = value_error ) Alternatively, you can pass '*' as an argument for the decorator so that you can validate every value available. Validating all data fields with custom logic 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.parser import parse , BaseModel , validator class HelloWorldModel ( BaseModel ): message : str sender : str @validator ( '*' ) def has_whitespace ( cls , v ): if ' ' not in v : raise ValueError ( \"Must have whitespace...\" ) return v parse ( model = HelloWorldModel , event = { \"message\" : \"hello universe\" , \"sender\" : \"universe\" })","title":"validating fields"},{"location":"utilities/parser/#validating-entire-model","text":"root_validator can help when you have a complex validation mechanism. For example finding whether data has been omitted, comparing field values, etc. Comparing and validating multiple fields at once with root_validator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from aws_lambda_powertools.utilities.parser import parse , BaseModel , root_validator class UserModel ( BaseModel ): username : str password1 : str password2 : str @root_validator def check_passwords_match ( cls , values ): pw1 , pw2 = values . get ( 'password1' ), values . get ( 'password2' ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( 'passwords do not match' ) return values payload = { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } parse ( model = UserModel , event = payload ) Info You can read more about validating list items, reusing validators, validating raw inputs, and a lot more in Pydantic's documentation .","title":"validating entire model"},{"location":"utilities/parser/#advanced-use-cases","text":"Tip: Looking to auto-generate models from JSON, YAML, JSON Schemas, OpenApi, etc? Use Koudai Aono's data model code generation tool for Pydantic There are number of advanced use cases well documented in Pydantic's doc such as creating immutable models , declaring fields with dynamic values ) e.g. UUID, and helper functions to parse models from files, str , etc. Two possible unknown use cases are Models and exception' serialization. Models have methods to export them as dict , JSON , JSON Schema , and Validation exceptions can be exported as JSON. Converting data models in various formats 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools.utilities import Logger from aws_lambda_powertools.utilities.parser import parse , BaseModel , ValidationError , validator logger = Logger ( service = \"user\" ) class UserModel ( BaseModel ): username : str password1 : str password2 : str payload = { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } def my_function (): try : return parse ( model = UserModel , event = payload ) except ValidationError as e : logger . exception ( e . json ()) return { \"status_code\" : 400 , \"message\" : \"Invalid username\" } User : UserModel = my_function () user_dict = User . dict () user_json = User . json () user_json_schema_as_dict = User . schema () user_json_schema_as_json = User . schema_json ( indent = 2 ) These can be quite useful when manipulating models that later need to be serialized as inputs for services like DynamoDB, EventBridge, etc.","title":"Advanced use cases"},{"location":"utilities/parser/#faq","text":"When should I use parser vs data_classes utility? Use data classes utility when you're after autocomplete, self-documented attributes and helpers to extract data from common event sources. Parser is best suited for those looking for a trade-off between defining their models for deep validation, parsing and autocomplete for an additional dependency to be brought in. How do I import X from Pydantic? We export most common classes, exceptions, and utilities from Pydantic as part of parser e.g. from aws_lambda_powertools.utilities.parser import BaseModel . If what's your trying to use isn't available as part of the high level import system, use the following escape hatch mechanism: Pydantic import escape hatch 1 from aws_lambda_powertools.utilities.parser.pydantic import < what you 'd like to import' > What is the cold start impact in bringing this additional dependency? No significant cold start impact. It does increase the final uncompressed package by 71M , when you bring the additional dependency that parser requires. Artillery load test sample against a hello world sample using Tracer, Metrics, and Logger with and without parser. No parser Info Uncompressed package size : 55M, p99 : 180.3ms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Summary report @ 14:36:07(+0200) 2020-10-23 Scenarios launched: 10 Scenarios completed: 10 Requests completed: 2000 Mean response/sec: 114.81 Response time (msec): min: 54.9 max: 1684.9 median: 68 p95: 109.1 p99: 180.3 Scenario counts: 0: 10 (100%) Codes: 200: 2000 With parser Info Uncompressed package size : 128M, p99 : 193.1ms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Summary report @ 14:29:23(+0200) 2020-10-23 Scenarios launched: 10 Scenarios completed: 10 Requests completed: 2000 Mean response/sec: 111.67 Response time (msec): min: 54.3 max: 1887.2 median: 66.1 p95: 113.3 p99: 193.1 Scenario counts: 0: 10 (100%) Codes: 200: 2000","title":"FAQ"},{"location":"utilities/typing/","text":"This typing utility provides static typing classes that can be used to ease the development by providing the IDE type hints. LambdaContext \u00b6 The LambdaContext typing is typically used in the handler method for the Lambda function. Annotating Lambda context type 1 2 3 4 5 6 from typing import Any , Dict from aws_lambda_powertools.utilities.typing import LambdaContext def handler ( event : Dict [ str , Any ], context : LambdaContext ) -> Dict [ str , Any ]: # Insert business logic return event","title":"Typing"},{"location":"utilities/typing/#lambdacontext","text":"The LambdaContext typing is typically used in the handler method for the Lambda function. Annotating Lambda context type 1 2 3 4 5 6 from typing import Any , Dict from aws_lambda_powertools.utilities.typing import LambdaContext def handler ( event : Dict [ str , Any ], context : LambdaContext ) -> Dict [ str , Any ]: # Insert business logic return event","title":"LambdaContext"},{"location":"utilities/validation/","text":"This utility provides JSON Schema validation for events and responses, including JMESPath support to unwrap events before validation. Key features \u00b6 Validate incoming event and response JMESPath support to unwrap events before validation applies Built-in envelopes to unwrap popular event sources payloads Getting started \u00b6 Tip: Using JSON Schemas for the first time? Check this step-by-step tour in the official JSON Schema website . You can validate inbound and outbound events using validator decorator . You can also use the standalone validate function, if you want more control over the validation process such as handling a validation error. We support any JSONSchema draft supported by fastjsonschema library. Warning Both validator decorator and validate standalone function expects your JSON Schema to be a dictionary , not a filename. Validator decorator \u00b6 Validator decorator is typically used to validate either inbound or functions' response. It will fail fast with SchemaValidationError exception if event or response doesn't conform with given JSON Schema. validator_decorator.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities.validation import validator import schemas @validator ( inbound_schema = schemas . INPUT , outbound_schema = schemas . OUTPUT ) def handler ( event , context ): return event event.json 1 2 3 4 { \"message\" : \"hello world\" , \"username\" : \"lessa\" } schemas.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 INPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"message\" : \"hello world\" , \"username\" : \"lessa\" }], \"required\" : [ \"message\" , \"username\" ], \"properties\" : { \"message\" : { \"$id\" : \"#/properties/message\" , \"type\" : \"string\" , \"title\" : \"The message\" , \"examples\" : [ \"hello world\" ], \"maxLength\" : 100 , }, \"username\" : { \"$id\" : \"#/properties/username\" , \"type\" : \"string\" , \"title\" : \"The username\" , \"examples\" : [ \"lessa\" ], \"maxLength\" : 30 , }, }, } OUTPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample outgoing schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"statusCode\" : 200 , \"body\" : \"response\" }], \"required\" : [ \"statusCode\" , \"body\" ], \"properties\" : { \"statusCode\" : { \"$id\" : \"#/properties/statusCode\" , \"type\" : \"integer\" , \"title\" : \"The statusCode\" }, \"body\" : { \"$id\" : \"#/properties/body\" , \"type\" : \"string\" , \"title\" : \"The response\" }, }, } Note It's not a requirement to validate both inbound and outbound schemas - You can either use one, or both. Validate function \u00b6 Validate standalone function is typically used within the Lambda handler, or any other methods that perform data validation. You can also gracefully handle schema validation errors by catching SchemaValidationError exception. validator_decorator.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.validation import validate from aws_lambda_powertools.utilities.validation.exceptions import SchemaValidationError import schemas def handler ( event , context ): try : validate ( event = event , schema = schemas . INPUT ) except SchemaValidationError as e : # do something before re-raising raise return event event.json 1 2 3 4 { \"data\" : \"hello world\" , \"username\" : \"lessa\" } schemas.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 INPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"message\" : \"hello world\" , \"username\" : \"lessa\" }], \"required\" : [ \"message\" , \"username\" ], \"properties\" : { \"message\" : { \"$id\" : \"#/properties/message\" , \"type\" : \"string\" , \"title\" : \"The message\" , \"examples\" : [ \"hello world\" ], \"maxLength\" : 100 , }, \"username\" : { \"$id\" : \"#/properties/username\" , \"type\" : \"string\" , \"title\" : \"The username\" , \"examples\" : [ \"lessa\" ], \"maxLength\" : 30 , }, }, } OUTPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample outgoing schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"statusCode\" : 200 , \"body\" : \"response\" }], \"required\" : [ \"statusCode\" , \"body\" ], \"properties\" : { \"statusCode\" : { \"$id\" : \"#/properties/statusCode\" , \"type\" : \"integer\" , \"title\" : \"The statusCode\" }, \"body\" : { \"$id\" : \"#/properties/body\" , \"type\" : \"string\" , \"title\" : \"The response\" }, }, } Unwrapping events prior to validation \u00b6 You might want to validate only a portion of your event - This is where the envelope parameter is for. Envelopes are JMESPath expressions to extract a portion of JSON you want before applying JSON Schema validation. Here is a sample custom EventBridge event, where we only validate what's inside the detail key: unwrapping_events.py We use the envelope parameter to extract the payload inside the detail key before validating. 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities.validation import validator import schemas @validator ( inbound_schema = schemas . INPUT , envelope = \"detail\" ) def handler ( event , context ): return event sample_wrapped_event.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"id\" : \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\" , \"detail-type\" : \"Scheduled Event\" , \"source\" : \"aws.events\" , \"account\" : \"123456789012\" , \"time\" : \"1970-01-01T00:00:00Z\" , \"region\" : \"us-east-1\" , \"resources\" : [ \"arn:aws:events:us-east-1:123456789012:rule/ExampleRule\" ], \"detail\" : { \"message\" : \"hello hello\" , \"username\" : \"blah blah\" } } schemas.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 INPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"message\" : \"hello world\" , \"username\" : \"lessa\" }], \"required\" : [ \"message\" , \"username\" ], \"properties\" : { \"message\" : { \"$id\" : \"#/properties/message\" , \"type\" : \"string\" , \"title\" : \"The message\" , \"examples\" : [ \"hello world\" ], \"maxLength\" : 100 , }, \"username\" : { \"$id\" : \"#/properties/username\" , \"type\" : \"string\" , \"title\" : \"The username\" , \"examples\" : [ \"lessa\" ], \"maxLength\" : 30 , }, }, } OUTPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample outgoing schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"statusCode\" : 200 , \"body\" : \"response\" }], \"required\" : [ \"statusCode\" , \"body\" ], \"properties\" : { \"statusCode\" : { \"$id\" : \"#/properties/statusCode\" , \"type\" : \"integer\" , \"title\" : \"The statusCode\" }, \"body\" : { \"$id\" : \"#/properties/body\" , \"type\" : \"string\" , \"title\" : \"The response\" }, }, } This is quite powerful because you can use JMESPath Query language to extract records from arrays, slice and dice , to pipe expressions and function expressions , where you'd extract what you need before validating the actual payload. Built-in envelopes \u00b6 This utility comes with built-in envelopes to easily extract the payload from popular event sources. unwrapping_popular_event_sources.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities.validation import envelopes , validator import schemas @validator ( inbound_schema = schemas . INPUT , envelope = envelopes . EVENTBRIDGE ) def handler ( event , context ): return event sample_wrapped_event.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"id\" : \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\" , \"detail-type\" : \"Scheduled Event\" , \"source\" : \"aws.events\" , \"account\" : \"123456789012\" , \"time\" : \"1970-01-01T00:00:00Z\" , \"region\" : \"us-east-1\" , \"resources\" : [ \"arn:aws:events:us-east-1:123456789012:rule/ExampleRule\" ], \"detail\" : { \"message\" : \"hello hello\" , \"username\" : \"blah blah\" } } schemas.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 INPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"message\" : \"hello world\" , \"username\" : \"lessa\" }], \"required\" : [ \"message\" , \"username\" ], \"properties\" : { \"message\" : { \"$id\" : \"#/properties/message\" , \"type\" : \"string\" , \"title\" : \"The message\" , \"examples\" : [ \"hello world\" ], \"maxLength\" : 100 , }, \"username\" : { \"$id\" : \"#/properties/username\" , \"type\" : \"string\" , \"title\" : \"The username\" , \"examples\" : [ \"lessa\" ], \"maxLength\" : 30 , }, }, } OUTPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample outgoing schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"statusCode\" : 200 , \"body\" : \"response\" }], \"required\" : [ \"statusCode\" , \"body\" ], \"properties\" : { \"statusCode\" : { \"$id\" : \"#/properties/statusCode\" , \"type\" : \"integer\" , \"title\" : \"The statusCode\" }, \"body\" : { \"$id\" : \"#/properties/body\" , \"type\" : \"string\" , \"title\" : \"The response\" }, }, } Here is a handy table with built-in envelopes along with their JMESPath expressions in case you want to build your own. Envelope name JMESPath expression API_GATEWAY_REST \"powertools_json(body)\" API_GATEWAY_HTTP \"powertools_json(body)\" SQS \"Records[*].powertools_json(body)\" SNS \"Records[0].Sns.Message EVENTBRIDGE \"detail\" CLOUDWATCH_EVENTS_SCHEDULED \"detail\" KINESIS_DATA_STREAM \"Records[*].kinesis.powertools_json(powertools_base64(data))\" CLOUDWATCH_LOGS \"awslogs.powertools_base64_gzip(data) Advanced \u00b6 Validating custom formats \u00b6 Note JSON Schema DRAFT 7 has many new built-in formats such as date, time, and specifically a regex format which might be a better replacement for a custom format, if you do have control over the schema. JSON Schemas with custom formats like int64 will fail validation. If you have these, you can pass them using formats parameter: custom_json_schema_type_format.json 1 2 3 4 5 6 { \"lastModifiedTime\" : { \"format\" : \"int64\" , \"type\" : \"integer\" } } For each format defined in a dictionary key, you must use a regex, or a function that returns a boolean to instruct the validator on how to proceed when encountering that type. validate_custom_format.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.validation import validate import schema custom_format = { \"int64\" : True , # simply ignore it, \"positive\" : lambda x : False if x < 0 else True } validate ( event = event , schema = schemas . INPUT , formats = custom_format ) schemas.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 INPUT = { \"$schema\" : \"http://json-schema.org/draft-04/schema#\" , \"definitions\" : { \"AWSAPICallViaCloudTrail\" : { \"properties\" : { \"additionalEventData\" : { \"$ref\" : \"#/definitions/AdditionalEventData\" }, \"awsRegion\" : { \"type\" : \"string\" }, \"errorCode\" : { \"type\" : \"string\" }, \"errorMessage\" : { \"type\" : \"string\" }, \"eventID\" : { \"type\" : \"string\" }, \"eventName\" : { \"type\" : \"string\" }, \"eventSource\" : { \"type\" : \"string\" }, \"eventTime\" : { \"format\" : \"date-time\" , \"type\" : \"string\" }, \"eventType\" : { \"type\" : \"string\" }, \"eventVersion\" : { \"type\" : \"string\" }, \"recipientAccountId\" : { \"type\" : \"string\" }, \"requestID\" : { \"type\" : \"string\" }, \"requestParameters\" : { \"$ref\" : \"#/definitions/RequestParameters\" }, \"resources\" : { \"items\" : { \"type\" : \"object\" }, \"type\" : \"array\" }, \"responseElements\" : { \"type\" : [ \"object\" , \"null\" ]}, \"sourceIPAddress\" : { \"type\" : \"string\" }, \"userAgent\" : { \"type\" : \"string\" }, \"userIdentity\" : { \"$ref\" : \"#/definitions/UserIdentity\" }, \"vpcEndpointId\" : { \"type\" : \"string\" }, \"x-amazon-open-api-schema-readOnly\" : { \"type\" : \"boolean\" }, }, \"required\" : [ \"eventID\" , \"awsRegion\" , \"eventVersion\" , \"responseElements\" , \"sourceIPAddress\" , \"eventSource\" , \"requestParameters\" , \"resources\" , \"userAgent\" , \"readOnly\" , \"userIdentity\" , \"eventType\" , \"additionalEventData\" , \"vpcEndpointId\" , \"requestID\" , \"eventTime\" , \"eventName\" , \"recipientAccountId\" , ], \"type\" : \"object\" , }, \"AdditionalEventData\" : { \"properties\" : { \"objectRetentionInfo\" : { \"$ref\" : \"#/definitions/ObjectRetentionInfo\" }, \"x-amz-id-2\" : { \"type\" : \"string\" }, }, \"required\" : [ \"x-amz-id-2\" ], \"type\" : \"object\" , }, \"Attributes\" : { \"properties\" : { \"creationDate\" : { \"format\" : \"date-time\" , \"type\" : \"string\" }, \"mfaAuthenticated\" : { \"type\" : \"string\" }, }, \"required\" : [ \"mfaAuthenticated\" , \"creationDate\" ], \"type\" : \"object\" , }, \"LegalHoldInfo\" : { \"properties\" : { \"isUnderLegalHold\" : { \"type\" : \"boolean\" }, \"lastModifiedTime\" : { \"format\" : \"int64\" , \"type\" : \"integer\" }, }, \"type\" : \"object\" , }, \"ObjectRetentionInfo\" : { \"properties\" : { \"legalHoldInfo\" : { \"$ref\" : \"#/definitions/LegalHoldInfo\" }, \"retentionInfo\" : { \"$ref\" : \"#/definitions/RetentionInfo\" }, }, \"type\" : \"object\" , }, \"RequestParameters\" : { \"properties\" : { \"bucketName\" : { \"type\" : \"string\" }, \"key\" : { \"type\" : \"string\" }, \"legal-hold\" : { \"type\" : \"string\" }, \"retention\" : { \"type\" : \"string\" }, }, \"required\" : [ \"bucketName\" , \"key\" ], \"type\" : \"object\" , }, \"RetentionInfo\" : { \"properties\" : { \"lastModifiedTime\" : { \"format\" : \"int64\" , \"type\" : \"integer\" }, \"retainUntilMode\" : { \"type\" : \"string\" }, \"retainUntilTime\" : { \"format\" : \"int64\" , \"type\" : \"integer\" }, }, \"type\" : \"object\" , }, \"SessionContext\" : { \"properties\" : { \"attributes\" : { \"$ref\" : \"#/definitions/Attributes\" }}, \"required\" : [ \"attributes\" ], \"type\" : \"object\" , }, \"UserIdentity\" : { \"properties\" : { \"accessKeyId\" : { \"type\" : \"string\" }, \"accountId\" : { \"type\" : \"string\" }, \"arn\" : { \"type\" : \"string\" }, \"principalId\" : { \"type\" : \"string\" }, \"sessionContext\" : { \"$ref\" : \"#/definitions/SessionContext\" }, \"type\" : { \"type\" : \"string\" }, }, \"required\" : [ \"accessKeyId\" , \"sessionContext\" , \"accountId\" , \"principalId\" , \"type\" , \"arn\" ], \"type\" : \"object\" , }, }, \"properties\" : { \"account\" : { \"type\" : \"string\" }, \"detail\" : { \"$ref\" : \"#/definitions/AWSAPICallViaCloudTrail\" }, \"detail-type\" : { \"type\" : \"string\" }, \"id\" : { \"type\" : \"string\" }, \"region\" : { \"type\" : \"string\" }, \"resources\" : { \"items\" : { \"type\" : \"string\" }, \"type\" : \"array\" }, \"source\" : { \"type\" : \"string\" }, \"time\" : { \"format\" : \"date-time\" , \"type\" : \"string\" }, \"version\" : { \"type\" : \"string\" }, }, \"required\" : [ \"detail-type\" , \"resources\" , \"id\" , \"source\" , \"time\" , \"detail\" , \"region\" , \"version\" , \"account\" ], \"title\" : \"AWSAPICallViaCloudTrail\" , \"type\" : \"object\" , \"x-amazon-events-detail-type\" : \"AWS API Call via CloudTrail\" , \"x-amazon-events-source\" : \"aws.s3\" , } event.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 { \"account\" : \"123456789012\" , \"detail\" : { \"additionalEventData\" : { \"AuthenticationMethod\" : \"AuthHeader\" , \"CipherSuite\" : \"ECDHE-RSA-AES128-GCM-SHA256\" , \"SignatureVersion\" : \"SigV4\" , \"bytesTransferredIn\" : 0 , \"bytesTransferredOut\" : 0 , \"x-amz-id-2\" : \"ejUr9Nd/4IO1juF/a6GOcu+PKrVX6dOH6jDjQOeCJvtARUqzxrhHGrhEt04cqYtAZVqcSEXYqo0=\" , }, \"awsRegion\" : \"us-west-1\" , \"eventCategory\" : \"Data\" , \"eventID\" : \"be4fdb30-9508-4984-b071-7692221899ae\" , \"eventName\" : \"HeadObject\" , \"eventSource\" : \"s3.amazonaws.com\" , \"eventTime\" : \"2020-12-22T10:05:29Z\" , \"eventType\" : \"AwsApiCall\" , \"eventVersion\" : \"1.07\" , \"managementEvent\" : False , \"readOnly\" : True , \"recipientAccountId\" : \"123456789012\" , \"requestID\" : \"A123B1C123D1E123\" , \"requestParameters\" : { \"Host\" : \"lambda-artifacts-deafc19498e3f2df.s3.us-west-1.amazonaws.com\" , \"bucketName\" : \"lambda-artifacts-deafc19498e3f2df\" , \"key\" : \"path1/path2/path3/file.zip\" , }, \"resources\" : [ { \"ARN\" : \"arn:aws:s3:::lambda-artifacts-deafc19498e3f2df/path1/path2/path3/file.zip\" , \"type\" : \"AWS::S3::Object\" , }, { \"ARN\" : \"arn:aws:s3:::lambda-artifacts-deafc19498e3f2df\" , \"accountId\" : \"123456789012\" , \"type\" : \"AWS::S3::Bucket\" , }, ], \"responseElements\" : No ne , \"sourceIPAddress\" : \"AWS Internal\" , \"userAgent\" : \"AWS Internal\" , \"userIdentity\" : { \"accessKeyId\" : \"ABCDEFGHIJKLMNOPQR12\" , \"accountId\" : \"123456789012\" , \"arn\" : \"arn:aws:sts::123456789012:assumed-role/role-name1/1234567890123\" , \"invokedBy\" : \"AWS Internal\" , \"principalId\" : \"ABCDEFGHIJKLMN1OPQRST:1234567890123\" , \"sessionContext\" : { \"attributes\" : { \"creationDate\" : \"2020-12-09T09:58:24Z\" , \"mfaAuthenticated\" : \"false\" }, \"sessionIssuer\" : { \"accountId\" : \"123456789012\" , \"arn\" : \"arn:aws:iam::123456789012:role/role-name1\" , \"principalId\" : \"ABCDEFGHIJKLMN1OPQRST\" , \"type\" : \"Role\" , \"userName\" : \"role-name1\" , }, }, \"type\" : \"AssumedRole\" , }, \"vpcEndpointId\" : \"vpce-a123cdef\" , }, \"detail-type\" : \"AWS API Call via CloudTrail\" , \"id\" : \"e0bad426-0a70-4424-b53a-eb902ebf5786\" , \"region\" : \"us-west-1\" , \"resources\" : [], \"source\" : \"aws.s3\" , \"time\" : \"2020-12-22T10:05:29Z\" , \"version\" : \"0\" , } Built-in JMESPath functions \u00b6 You might have events or responses that contain non-encoded JSON, where you need to decode before validating them. You can use our built-in JMESPath functions within your expressions to do exactly that to decode JSON Strings, base64, and uncompress gzip data. Info We use these for built-in envelopes to easily to decode and unwrap events from sources like Kinesis, CloudWatch Logs, etc.","title":"Validation"},{"location":"utilities/validation/#key-features","text":"Validate incoming event and response JMESPath support to unwrap events before validation applies Built-in envelopes to unwrap popular event sources payloads","title":"Key features"},{"location":"utilities/validation/#getting-started","text":"Tip: Using JSON Schemas for the first time? Check this step-by-step tour in the official JSON Schema website . You can validate inbound and outbound events using validator decorator . You can also use the standalone validate function, if you want more control over the validation process such as handling a validation error. We support any JSONSchema draft supported by fastjsonschema library. Warning Both validator decorator and validate standalone function expects your JSON Schema to be a dictionary , not a filename.","title":"Getting started"},{"location":"utilities/validation/#validator-decorator","text":"Validator decorator is typically used to validate either inbound or functions' response. It will fail fast with SchemaValidationError exception if event or response doesn't conform with given JSON Schema. validator_decorator.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities.validation import validator import schemas @validator ( inbound_schema = schemas . INPUT , outbound_schema = schemas . OUTPUT ) def handler ( event , context ): return event event.json 1 2 3 4 { \"message\" : \"hello world\" , \"username\" : \"lessa\" } schemas.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 INPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"message\" : \"hello world\" , \"username\" : \"lessa\" }], \"required\" : [ \"message\" , \"username\" ], \"properties\" : { \"message\" : { \"$id\" : \"#/properties/message\" , \"type\" : \"string\" , \"title\" : \"The message\" , \"examples\" : [ \"hello world\" ], \"maxLength\" : 100 , }, \"username\" : { \"$id\" : \"#/properties/username\" , \"type\" : \"string\" , \"title\" : \"The username\" , \"examples\" : [ \"lessa\" ], \"maxLength\" : 30 , }, }, } OUTPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample outgoing schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"statusCode\" : 200 , \"body\" : \"response\" }], \"required\" : [ \"statusCode\" , \"body\" ], \"properties\" : { \"statusCode\" : { \"$id\" : \"#/properties/statusCode\" , \"type\" : \"integer\" , \"title\" : \"The statusCode\" }, \"body\" : { \"$id\" : \"#/properties/body\" , \"type\" : \"string\" , \"title\" : \"The response\" }, }, } Note It's not a requirement to validate both inbound and outbound schemas - You can either use one, or both.","title":"Validator decorator"},{"location":"utilities/validation/#validate-function","text":"Validate standalone function is typically used within the Lambda handler, or any other methods that perform data validation. You can also gracefully handle schema validation errors by catching SchemaValidationError exception. validator_decorator.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.validation import validate from aws_lambda_powertools.utilities.validation.exceptions import SchemaValidationError import schemas def handler ( event , context ): try : validate ( event = event , schema = schemas . INPUT ) except SchemaValidationError as e : # do something before re-raising raise return event event.json 1 2 3 4 { \"data\" : \"hello world\" , \"username\" : \"lessa\" } schemas.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 INPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"message\" : \"hello world\" , \"username\" : \"lessa\" }], \"required\" : [ \"message\" , \"username\" ], \"properties\" : { \"message\" : { \"$id\" : \"#/properties/message\" , \"type\" : \"string\" , \"title\" : \"The message\" , \"examples\" : [ \"hello world\" ], \"maxLength\" : 100 , }, \"username\" : { \"$id\" : \"#/properties/username\" , \"type\" : \"string\" , \"title\" : \"The username\" , \"examples\" : [ \"lessa\" ], \"maxLength\" : 30 , }, }, } OUTPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample outgoing schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"statusCode\" : 200 , \"body\" : \"response\" }], \"required\" : [ \"statusCode\" , \"body\" ], \"properties\" : { \"statusCode\" : { \"$id\" : \"#/properties/statusCode\" , \"type\" : \"integer\" , \"title\" : \"The statusCode\" }, \"body\" : { \"$id\" : \"#/properties/body\" , \"type\" : \"string\" , \"title\" : \"The response\" }, }, }","title":"Validate function"},{"location":"utilities/validation/#unwrapping-events-prior-to-validation","text":"You might want to validate only a portion of your event - This is where the envelope parameter is for. Envelopes are JMESPath expressions to extract a portion of JSON you want before applying JSON Schema validation. Here is a sample custom EventBridge event, where we only validate what's inside the detail key: unwrapping_events.py We use the envelope parameter to extract the payload inside the detail key before validating. 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities.validation import validator import schemas @validator ( inbound_schema = schemas . INPUT , envelope = \"detail\" ) def handler ( event , context ): return event sample_wrapped_event.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"id\" : \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\" , \"detail-type\" : \"Scheduled Event\" , \"source\" : \"aws.events\" , \"account\" : \"123456789012\" , \"time\" : \"1970-01-01T00:00:00Z\" , \"region\" : \"us-east-1\" , \"resources\" : [ \"arn:aws:events:us-east-1:123456789012:rule/ExampleRule\" ], \"detail\" : { \"message\" : \"hello hello\" , \"username\" : \"blah blah\" } } schemas.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 INPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"message\" : \"hello world\" , \"username\" : \"lessa\" }], \"required\" : [ \"message\" , \"username\" ], \"properties\" : { \"message\" : { \"$id\" : \"#/properties/message\" , \"type\" : \"string\" , \"title\" : \"The message\" , \"examples\" : [ \"hello world\" ], \"maxLength\" : 100 , }, \"username\" : { \"$id\" : \"#/properties/username\" , \"type\" : \"string\" , \"title\" : \"The username\" , \"examples\" : [ \"lessa\" ], \"maxLength\" : 30 , }, }, } OUTPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample outgoing schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"statusCode\" : 200 , \"body\" : \"response\" }], \"required\" : [ \"statusCode\" , \"body\" ], \"properties\" : { \"statusCode\" : { \"$id\" : \"#/properties/statusCode\" , \"type\" : \"integer\" , \"title\" : \"The statusCode\" }, \"body\" : { \"$id\" : \"#/properties/body\" , \"type\" : \"string\" , \"title\" : \"The response\" }, }, } This is quite powerful because you can use JMESPath Query language to extract records from arrays, slice and dice , to pipe expressions and function expressions , where you'd extract what you need before validating the actual payload.","title":"Unwrapping events prior to validation"},{"location":"utilities/validation/#built-in-envelopes","text":"This utility comes with built-in envelopes to easily extract the payload from popular event sources. unwrapping_popular_event_sources.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities.validation import envelopes , validator import schemas @validator ( inbound_schema = schemas . INPUT , envelope = envelopes . EVENTBRIDGE ) def handler ( event , context ): return event sample_wrapped_event.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"id\" : \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\" , \"detail-type\" : \"Scheduled Event\" , \"source\" : \"aws.events\" , \"account\" : \"123456789012\" , \"time\" : \"1970-01-01T00:00:00Z\" , \"region\" : \"us-east-1\" , \"resources\" : [ \"arn:aws:events:us-east-1:123456789012:rule/ExampleRule\" ], \"detail\" : { \"message\" : \"hello hello\" , \"username\" : \"blah blah\" } } schemas.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 INPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"message\" : \"hello world\" , \"username\" : \"lessa\" }], \"required\" : [ \"message\" , \"username\" ], \"properties\" : { \"message\" : { \"$id\" : \"#/properties/message\" , \"type\" : \"string\" , \"title\" : \"The message\" , \"examples\" : [ \"hello world\" ], \"maxLength\" : 100 , }, \"username\" : { \"$id\" : \"#/properties/username\" , \"type\" : \"string\" , \"title\" : \"The username\" , \"examples\" : [ \"lessa\" ], \"maxLength\" : 30 , }, }, } OUTPUT = { \"$schema\" : \"http://json-schema.org/draft-07/schema\" , \"$id\" : \"http://example.com/example.json\" , \"type\" : \"object\" , \"title\" : \"Sample outgoing schema\" , \"description\" : \"The root schema comprises the entire JSON document.\" , \"examples\" : [{ \"statusCode\" : 200 , \"body\" : \"response\" }], \"required\" : [ \"statusCode\" , \"body\" ], \"properties\" : { \"statusCode\" : { \"$id\" : \"#/properties/statusCode\" , \"type\" : \"integer\" , \"title\" : \"The statusCode\" }, \"body\" : { \"$id\" : \"#/properties/body\" , \"type\" : \"string\" , \"title\" : \"The response\" }, }, } Here is a handy table with built-in envelopes along with their JMESPath expressions in case you want to build your own. Envelope name JMESPath expression API_GATEWAY_REST \"powertools_json(body)\" API_GATEWAY_HTTP \"powertools_json(body)\" SQS \"Records[*].powertools_json(body)\" SNS \"Records[0].Sns.Message EVENTBRIDGE \"detail\" CLOUDWATCH_EVENTS_SCHEDULED \"detail\" KINESIS_DATA_STREAM \"Records[*].kinesis.powertools_json(powertools_base64(data))\" CLOUDWATCH_LOGS \"awslogs.powertools_base64_gzip(data)","title":"Built-in envelopes"},{"location":"utilities/validation/#advanced","text":"","title":"Advanced"},{"location":"utilities/validation/#validating-custom-formats","text":"Note JSON Schema DRAFT 7 has many new built-in formats such as date, time, and specifically a regex format which might be a better replacement for a custom format, if you do have control over the schema. JSON Schemas with custom formats like int64 will fail validation. If you have these, you can pass them using formats parameter: custom_json_schema_type_format.json 1 2 3 4 5 6 { \"lastModifiedTime\" : { \"format\" : \"int64\" , \"type\" : \"integer\" } } For each format defined in a dictionary key, you must use a regex, or a function that returns a boolean to instruct the validator on how to proceed when encountering that type. validate_custom_format.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.validation import validate import schema custom_format = { \"int64\" : True , # simply ignore it, \"positive\" : lambda x : False if x < 0 else True } validate ( event = event , schema = schemas . INPUT , formats = custom_format ) schemas.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 INPUT = { \"$schema\" : \"http://json-schema.org/draft-04/schema#\" , \"definitions\" : { \"AWSAPICallViaCloudTrail\" : { \"properties\" : { \"additionalEventData\" : { \"$ref\" : \"#/definitions/AdditionalEventData\" }, \"awsRegion\" : { \"type\" : \"string\" }, \"errorCode\" : { \"type\" : \"string\" }, \"errorMessage\" : { \"type\" : \"string\" }, \"eventID\" : { \"type\" : \"string\" }, \"eventName\" : { \"type\" : \"string\" }, \"eventSource\" : { \"type\" : \"string\" }, \"eventTime\" : { \"format\" : \"date-time\" , \"type\" : \"string\" }, \"eventType\" : { \"type\" : \"string\" }, \"eventVersion\" : { \"type\" : \"string\" }, \"recipientAccountId\" : { \"type\" : \"string\" }, \"requestID\" : { \"type\" : \"string\" }, \"requestParameters\" : { \"$ref\" : \"#/definitions/RequestParameters\" }, \"resources\" : { \"items\" : { \"type\" : \"object\" }, \"type\" : \"array\" }, \"responseElements\" : { \"type\" : [ \"object\" , \"null\" ]}, \"sourceIPAddress\" : { \"type\" : \"string\" }, \"userAgent\" : { \"type\" : \"string\" }, \"userIdentity\" : { \"$ref\" : \"#/definitions/UserIdentity\" }, \"vpcEndpointId\" : { \"type\" : \"string\" }, \"x-amazon-open-api-schema-readOnly\" : { \"type\" : \"boolean\" }, }, \"required\" : [ \"eventID\" , \"awsRegion\" , \"eventVersion\" , \"responseElements\" , \"sourceIPAddress\" , \"eventSource\" , \"requestParameters\" , \"resources\" , \"userAgent\" , \"readOnly\" , \"userIdentity\" , \"eventType\" , \"additionalEventData\" , \"vpcEndpointId\" , \"requestID\" , \"eventTime\" , \"eventName\" , \"recipientAccountId\" , ], \"type\" : \"object\" , }, \"AdditionalEventData\" : { \"properties\" : { \"objectRetentionInfo\" : { \"$ref\" : \"#/definitions/ObjectRetentionInfo\" }, \"x-amz-id-2\" : { \"type\" : \"string\" }, }, \"required\" : [ \"x-amz-id-2\" ], \"type\" : \"object\" , }, \"Attributes\" : { \"properties\" : { \"creationDate\" : { \"format\" : \"date-time\" , \"type\" : \"string\" }, \"mfaAuthenticated\" : { \"type\" : \"string\" }, }, \"required\" : [ \"mfaAuthenticated\" , \"creationDate\" ], \"type\" : \"object\" , }, \"LegalHoldInfo\" : { \"properties\" : { \"isUnderLegalHold\" : { \"type\" : \"boolean\" }, \"lastModifiedTime\" : { \"format\" : \"int64\" , \"type\" : \"integer\" }, }, \"type\" : \"object\" , }, \"ObjectRetentionInfo\" : { \"properties\" : { \"legalHoldInfo\" : { \"$ref\" : \"#/definitions/LegalHoldInfo\" }, \"retentionInfo\" : { \"$ref\" : \"#/definitions/RetentionInfo\" }, }, \"type\" : \"object\" , }, \"RequestParameters\" : { \"properties\" : { \"bucketName\" : { \"type\" : \"string\" }, \"key\" : { \"type\" : \"string\" }, \"legal-hold\" : { \"type\" : \"string\" }, \"retention\" : { \"type\" : \"string\" }, }, \"required\" : [ \"bucketName\" , \"key\" ], \"type\" : \"object\" , }, \"RetentionInfo\" : { \"properties\" : { \"lastModifiedTime\" : { \"format\" : \"int64\" , \"type\" : \"integer\" }, \"retainUntilMode\" : { \"type\" : \"string\" }, \"retainUntilTime\" : { \"format\" : \"int64\" , \"type\" : \"integer\" }, }, \"type\" : \"object\" , }, \"SessionContext\" : { \"properties\" : { \"attributes\" : { \"$ref\" : \"#/definitions/Attributes\" }}, \"required\" : [ \"attributes\" ], \"type\" : \"object\" , }, \"UserIdentity\" : { \"properties\" : { \"accessKeyId\" : { \"type\" : \"string\" }, \"accountId\" : { \"type\" : \"string\" }, \"arn\" : { \"type\" : \"string\" }, \"principalId\" : { \"type\" : \"string\" }, \"sessionContext\" : { \"$ref\" : \"#/definitions/SessionContext\" }, \"type\" : { \"type\" : \"string\" }, }, \"required\" : [ \"accessKeyId\" , \"sessionContext\" , \"accountId\" , \"principalId\" , \"type\" , \"arn\" ], \"type\" : \"object\" , }, }, \"properties\" : { \"account\" : { \"type\" : \"string\" }, \"detail\" : { \"$ref\" : \"#/definitions/AWSAPICallViaCloudTrail\" }, \"detail-type\" : { \"type\" : \"string\" }, \"id\" : { \"type\" : \"string\" }, \"region\" : { \"type\" : \"string\" }, \"resources\" : { \"items\" : { \"type\" : \"string\" }, \"type\" : \"array\" }, \"source\" : { \"type\" : \"string\" }, \"time\" : { \"format\" : \"date-time\" , \"type\" : \"string\" }, \"version\" : { \"type\" : \"string\" }, }, \"required\" : [ \"detail-type\" , \"resources\" , \"id\" , \"source\" , \"time\" , \"detail\" , \"region\" , \"version\" , \"account\" ], \"title\" : \"AWSAPICallViaCloudTrail\" , \"type\" : \"object\" , \"x-amazon-events-detail-type\" : \"AWS API Call via CloudTrail\" , \"x-amazon-events-source\" : \"aws.s3\" , } event.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 { \"account\" : \"123456789012\" , \"detail\" : { \"additionalEventData\" : { \"AuthenticationMethod\" : \"AuthHeader\" , \"CipherSuite\" : \"ECDHE-RSA-AES128-GCM-SHA256\" , \"SignatureVersion\" : \"SigV4\" , \"bytesTransferredIn\" : 0 , \"bytesTransferredOut\" : 0 , \"x-amz-id-2\" : \"ejUr9Nd/4IO1juF/a6GOcu+PKrVX6dOH6jDjQOeCJvtARUqzxrhHGrhEt04cqYtAZVqcSEXYqo0=\" , }, \"awsRegion\" : \"us-west-1\" , \"eventCategory\" : \"Data\" , \"eventID\" : \"be4fdb30-9508-4984-b071-7692221899ae\" , \"eventName\" : \"HeadObject\" , \"eventSource\" : \"s3.amazonaws.com\" , \"eventTime\" : \"2020-12-22T10:05:29Z\" , \"eventType\" : \"AwsApiCall\" , \"eventVersion\" : \"1.07\" , \"managementEvent\" : False , \"readOnly\" : True , \"recipientAccountId\" : \"123456789012\" , \"requestID\" : \"A123B1C123D1E123\" , \"requestParameters\" : { \"Host\" : \"lambda-artifacts-deafc19498e3f2df.s3.us-west-1.amazonaws.com\" , \"bucketName\" : \"lambda-artifacts-deafc19498e3f2df\" , \"key\" : \"path1/path2/path3/file.zip\" , }, \"resources\" : [ { \"ARN\" : \"arn:aws:s3:::lambda-artifacts-deafc19498e3f2df/path1/path2/path3/file.zip\" , \"type\" : \"AWS::S3::Object\" , }, { \"ARN\" : \"arn:aws:s3:::lambda-artifacts-deafc19498e3f2df\" , \"accountId\" : \"123456789012\" , \"type\" : \"AWS::S3::Bucket\" , }, ], \"responseElements\" : No ne , \"sourceIPAddress\" : \"AWS Internal\" , \"userAgent\" : \"AWS Internal\" , \"userIdentity\" : { \"accessKeyId\" : \"ABCDEFGHIJKLMNOPQR12\" , \"accountId\" : \"123456789012\" , \"arn\" : \"arn:aws:sts::123456789012:assumed-role/role-name1/1234567890123\" , \"invokedBy\" : \"AWS Internal\" , \"principalId\" : \"ABCDEFGHIJKLMN1OPQRST:1234567890123\" , \"sessionContext\" : { \"attributes\" : { \"creationDate\" : \"2020-12-09T09:58:24Z\" , \"mfaAuthenticated\" : \"false\" }, \"sessionIssuer\" : { \"accountId\" : \"123456789012\" , \"arn\" : \"arn:aws:iam::123456789012:role/role-name1\" , \"principalId\" : \"ABCDEFGHIJKLMN1OPQRST\" , \"type\" : \"Role\" , \"userName\" : \"role-name1\" , }, }, \"type\" : \"AssumedRole\" , }, \"vpcEndpointId\" : \"vpce-a123cdef\" , }, \"detail-type\" : \"AWS API Call via CloudTrail\" , \"id\" : \"e0bad426-0a70-4424-b53a-eb902ebf5786\" , \"region\" : \"us-west-1\" , \"resources\" : [], \"source\" : \"aws.s3\" , \"time\" : \"2020-12-22T10:05:29Z\" , \"version\" : \"0\" , }","title":"Validating custom formats"},{"location":"utilities/validation/#built-in-jmespath-functions","text":"You might have events or responses that contain non-encoded JSON, where you need to decode before validating them. You can use our built-in JMESPath functions within your expressions to do exactly that to decode JSON Strings, base64, and uncompress gzip data. Info We use these for built-in envelopes to easily to decode and unwrap events from sources like Kinesis, CloudWatch Logs, etc.","title":"Built-in JMESPath functions"}]}